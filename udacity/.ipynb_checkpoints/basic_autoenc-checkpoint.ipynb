{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = 10\n",
    "train_x = np.random.normal(size=(10000, n))\n",
    "val_x = np.random.normal(size=(1000, n))\n",
    "test_x = np.random.normal(size=(1000, n))\n",
    "\n",
    "train_x = train_x.astype(np.float32)\n",
    "val_x = val_x.astype(np.float32)\n",
    "test_x = test_x.astype(np.float32)\n",
    "\n",
    "mults = np.arange(n) + 1\n",
    "\n",
    "train_y = np.random.normal(size=(train_x.shape[0], 1)) + np.expand_dims(np.sum(train_x * mults, axis=1) + 5, 1)\n",
    "val_y = np.random.normal(size=(val_x.shape[0], 1)) + np.expand_dims(np.sum(val_x * mults, axis=1) + 5, 1)\n",
    "test_y = np.random.normal(size=(test_x.shape[0], 1)) + np.expand_dims(np.sum(test_x * mults, axis=1) + 5, 1)\n",
    "\n",
    "train_y = train_y.astype(np.float32)\n",
    "val_y = val_y.astype(np.float32)\n",
    "test_y = test_y.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  6.1142869 ],\n",
       "       [ -3.29616547],\n",
       "       [ -0.38745737],\n",
       "       ..., \n",
       "       [-22.88450813],\n",
       "       [ -5.97206831],\n",
       "       [ 23.34959602]], dtype=float32)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print train_y.shape\n",
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rmse(predictions, actual):\n",
    "    sse = np.sum(np.square(predictions - actual))\n",
    "    return np.sqrt(sse / float(actual.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 11753.597656\n",
      "Minibatch RMSE: 19.16507\n",
      "Validation RMSE: 19.87259\n",
      "Minibatch loss at step 500: 5980.211426\n",
      "Minibatch RMSE: 13.67046\n",
      "Validation RMSE: 13.97473\n",
      "Minibatch loss at step 1000: 3667.376953\n",
      "Minibatch RMSE: 10.70540\n",
      "Validation RMSE: 9.46711\n",
      "Minibatch loss at step 1500: 1292.009277\n",
      "Minibatch RMSE: 6.35416\n",
      "Validation RMSE: 6.14222\n",
      "Minibatch loss at step 2000: 662.995239\n",
      "Minibatch RMSE: 4.55177\n",
      "Validation RMSE: 3.83010\n",
      "Minibatch loss at step 2500: 151.446045\n",
      "Minibatch RMSE: 2.17547\n",
      "Validation RMSE: 2.31364\n",
      "Minibatch loss at step 3000: 58.142159\n",
      "Minibatch RMSE: 1.34794\n",
      "Validation RMSE: 1.45080\n",
      "Minibatch loss at step 3500: 45.994278\n",
      "Minibatch RMSE: 1.19888\n",
      "Validation RMSE: 1.09211\n",
      "Minibatch loss at step 4000: 28.234392\n",
      "Minibatch RMSE: 0.93932\n",
      "Validation RMSE: 1.00379\n",
      "Minibatch loss at step 4500: 23.549706\n",
      "Minibatch RMSE: 0.85786\n",
      "Validation RMSE: 0.99184\n",
      "Minibatch loss at step 5000: 29.924740\n",
      "Minibatch RMSE: 0.96703\n",
      "Validation RMSE: 0.99193\n",
      "Minibatch loss at step 5500: 30.733768\n",
      "Minibatch RMSE: 0.98002\n",
      "Validation RMSE: 0.99203\n",
      "Minibatch loss at step 6000: 40.032600\n",
      "Minibatch RMSE: 1.11849\n",
      "Validation RMSE: 0.99168\n",
      "Minibatch loss at step 6500: 33.109058\n",
      "Minibatch RMSE: 1.01718\n",
      "Validation RMSE: 0.99149\n",
      "Minibatch loss at step 7000: 32.725403\n",
      "Minibatch RMSE: 1.01127\n",
      "Validation RMSE: 0.99110\n",
      "Minibatch loss at step 7500: 30.455271\n",
      "Minibatch RMSE: 0.97557\n",
      "Validation RMSE: 0.99358\n",
      "Minibatch loss at step 8000: 39.553780\n",
      "Minibatch RMSE: 1.11178\n",
      "Validation RMSE: 0.99288\n",
      "Minibatch loss at step 8500: 25.869177\n",
      "Minibatch RMSE: 0.89912\n",
      "Validation RMSE: 0.99362\n",
      "Minibatch loss at step 9000: 39.195961\n",
      "Minibatch RMSE: 1.10674\n",
      "Validation RMSE: 0.99057\n",
      "Minibatch loss at step 9500: 34.364952\n",
      "Minibatch RMSE: 1.03629\n",
      "Validation RMSE: 0.99369\n",
      "Minibatch loss at step 10000: 23.859804\n",
      "Minibatch RMSE: 0.86349\n",
      "Validation RMSE: 0.99275\n",
      "Minibatch loss at step 10500: 21.124638\n",
      "Minibatch RMSE: 0.81249\n",
      "Validation RMSE: 0.99216\n",
      "Minibatch loss at step 11000: 31.429714\n",
      "Minibatch RMSE: 0.99105\n",
      "Validation RMSE: 0.99141\n",
      "Minibatch loss at step 11500: 28.385277\n",
      "Minibatch RMSE: 0.94183\n",
      "Validation RMSE: 0.99265\n",
      "Minibatch loss at step 12000: 36.087128\n",
      "Minibatch RMSE: 1.06194\n",
      "Validation RMSE: 0.99532\n",
      "Minibatch loss at step 12500: 27.188454\n",
      "Minibatch RMSE: 0.92176\n",
      "Validation RMSE: 0.99372\n",
      "Minibatch loss at step 13000: 38.026855\n",
      "Minibatch RMSE: 1.09011\n",
      "Validation RMSE: 0.99384\n",
      "Minibatch loss at step 13500: 32.227215\n",
      "Minibatch RMSE: 1.00354\n",
      "Validation RMSE: 0.99083\n",
      "Minibatch loss at step 14000: 27.536985\n",
      "Minibatch RMSE: 0.92765\n",
      "Validation RMSE: 0.99387\n",
      "Minibatch loss at step 14500: 42.624485\n",
      "Minibatch RMSE: 1.15413\n",
      "Validation RMSE: 0.99255\n",
      "Minibatch loss at step 15000: 37.085629\n",
      "Minibatch RMSE: 1.07653\n",
      "Validation RMSE: 0.99203\n",
      "Minibatch loss at step 15500: 28.827366\n",
      "Minibatch RMSE: 0.94913\n",
      "Validation RMSE: 0.99187\n",
      "Minibatch loss at step 16000: 37.905098\n",
      "Minibatch RMSE: 1.08836\n",
      "Validation RMSE: 0.99198\n",
      "Minibatch loss at step 16500: 31.114456\n",
      "Minibatch RMSE: 0.98607\n",
      "Validation RMSE: 0.99629\n",
      "Minibatch loss at step 17000: 30.296930\n",
      "Minibatch RMSE: 0.97303\n",
      "Validation RMSE: 0.99368\n",
      "Minibatch loss at step 17500: 32.342564\n",
      "Minibatch RMSE: 1.00534\n",
      "Validation RMSE: 0.99365\n",
      "Minibatch loss at step 18000: 38.302189\n",
      "Minibatch RMSE: 1.09405\n",
      "Validation RMSE: 0.99052\n",
      "Minibatch loss at step 18500: 34.993752\n",
      "Minibatch RMSE: 1.04573\n",
      "Validation RMSE: 0.99312\n",
      "Minibatch loss at step 19000: 37.223579\n",
      "Minibatch RMSE: 1.07853\n",
      "Validation RMSE: 0.99252\n",
      "Minibatch loss at step 19500: 27.961376\n",
      "Minibatch RMSE: 0.93477\n",
      "Validation RMSE: 0.99196\n",
      "Minibatch loss at step 20000: 36.320961\n",
      "Minibatch RMSE: 1.06538\n",
      "Validation RMSE: 0.99260\n",
      "Test RMSE: 1.03141\n",
      "\n",
      "[[  1.02200902]\n",
      " [  1.99050641]\n",
      " [  3.01760626]\n",
      " [  4.02608633]\n",
      " [  4.99301147]\n",
      " [  5.99364805]\n",
      " [  7.00433302]\n",
      " [  7.94844532]\n",
      " [  8.99708462]\n",
      " [ 10.0092001 ]]\n",
      "[ 5.02767134]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "num_labels = train_y.shape[1]\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,shape=(batch_size, n))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(val_x)\n",
    "    tf_test_dataset = tf.constant(test_x)\n",
    "    tf_tr_dataset = tf.constant(train_x)\n",
    "  \n",
    "    # Variables.\n",
    "    weights = tf.Variable(tf.truncated_normal([n, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "    # Training computation.\n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    # loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    loss = tf.nn.l2_loss(logits - tf_train_labels)\n",
    "  \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.AdamOptimizer(0.005).minimize(loss)\n",
    "  \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    # train_prediction = tf.nn.softmax(logits)\n",
    "    train_prediction = logits\n",
    "    valid_prediction = tf.matmul(tf_valid_dataset, weights) + biases\n",
    "    test_prediction = tf.matmul(tf_test_dataset, weights) + biases\n",
    "    tr_preds = tf.matmul(tf_tr_dataset, weights) + biases\n",
    "    \n",
    "num_steps = 20001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_y.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_x[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_y[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch RMSE: %.5f\" % rmse(predictions, batch_labels))\n",
    "            print(\"Validation RMSE: %.5f\" % rmse(valid_prediction.eval(), val_y))\n",
    "        \n",
    "    print(\"Test RMSE: %.5f\" % rmse(test_prediction.eval(), test_y))\n",
    "    print(\"\")\n",
    "    print weights.eval()\n",
    "    print biases.eval()\n",
    "    print(\"\")\n",
    "    preds = tr_preds.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         actual       pred\n",
      "0      6.114287   7.591687\n",
      "1     -3.296165  -4.201170\n",
      "2     -0.387457   0.065979\n",
      "3    -31.533215 -31.105721\n",
      "4     -6.089595  -4.314531\n",
      "5     24.203949  25.157784\n",
      "6     34.981419  34.143700\n",
      "7     11.803017  13.194857\n",
      "8      3.807801   5.496472\n",
      "9     13.454775  15.859959\n",
      "10    13.207471  13.294289\n",
      "11   -13.326806 -12.384052\n",
      "12    24.561901  24.954006\n",
      "13    -4.188128  -2.832715\n",
      "14    19.238928  20.350555\n",
      "15   -19.454119 -19.309490\n",
      "16    -1.853180  -2.129075\n",
      "17    -3.220191  -2.002963\n",
      "18     1.965698   1.663415\n",
      "19    -9.647721 -10.108244\n",
      "20    -7.458369  -7.067541\n",
      "21    23.078756  22.586800\n",
      "22     6.698447   5.623510\n",
      "23    41.753529  41.292187\n",
      "24    15.295608  14.619745\n",
      "25    29.777733  29.023819\n",
      "26     5.174055   4.213811\n",
      "27     4.853136   4.272203\n",
      "28    32.243752  31.258030\n",
      "29   -14.729937 -15.265156\n",
      "...         ...        ...\n",
      "9970   1.221900  -0.234058\n",
      "9971  23.677076  23.249630\n",
      "9972  25.399446  26.382950\n",
      "9973   8.740164   8.938507\n",
      "9974  58.265644  58.180939\n",
      "9975 -23.672373 -22.358450\n",
      "9976 -49.129791 -50.880280\n",
      "9977  39.597763  40.055649\n",
      "9978  15.376143  15.837513\n",
      "9979  30.468021  30.209806\n",
      "9980  25.510002  26.057634\n",
      "9981  -7.921922  -7.434231\n",
      "9982  -8.656141  -9.265396\n",
      "9983  13.358600  11.868402\n",
      "9984  13.619740  14.819359\n",
      "9985 -14.163846 -13.767994\n",
      "9986  28.446774  29.719570\n",
      "9987  -0.968054  -0.053507\n",
      "9988  -3.204789  -3.740489\n",
      "9989   1.327242   2.388481\n",
      "9990 -12.372934 -12.843386\n",
      "9991  21.989294  21.729141\n",
      "9992  18.453920  16.593029\n",
      "9993  18.326572  18.579937\n",
      "9994  29.624071  28.906488\n",
      "9995   4.795215   2.646193\n",
      "9996  22.384312  21.743416\n",
      "9997 -22.884508 -22.596138\n",
      "9998  -5.972068  -5.392167\n",
      "9999  23.349596  22.636765\n",
      "\n",
      "[10000 rows x 2 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1279c5ad0>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEACAYAAAC3adEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12deef390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pdf = pd.DataFrame(train_y, columns=['actual'])\n",
    "pdf['pred'] = preds\n",
    "print(pdf)\n",
    "\n",
    "plt.scatter(pdf.actual, pdf.pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mults = np.array([np.arange(10)+1, 10-np.arange(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.dot(train_x, mults.T) + [5, -1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3931.530029\n",
      "Minibatch RMSE: 10.23918\n",
      "Validation RMSE: 7.22335\n",
      "Minibatch loss at step 1000: 4632.592285\n",
      "Minibatch RMSE: 11.11467\n",
      "Validation RMSE: 7.07820\n",
      "Minibatch loss at step 2000: 3733.845947\n",
      "Minibatch RMSE: 9.97844\n",
      "Validation RMSE: 6.93736\n",
      "Minibatch loss at step 3000: 3122.553467\n",
      "Minibatch RMSE: 9.12513\n",
      "Validation RMSE: 6.79893\n",
      "Minibatch loss at step 4000: 3599.522461\n",
      "Minibatch RMSE: 9.79731\n",
      "Validation RMSE: 6.66352\n",
      "Minibatch loss at step 5000: 3213.379150\n",
      "Minibatch RMSE: 9.25689\n",
      "Validation RMSE: 6.53092\n",
      "Minibatch loss at step 6000: 2909.605713\n",
      "Minibatch RMSE: 8.80849\n",
      "Validation RMSE: 6.39973\n",
      "Minibatch loss at step 7000: 2078.756836\n",
      "Minibatch RMSE: 7.44537\n",
      "Validation RMSE: 6.27122\n",
      "Minibatch loss at step 8000: 2426.938721\n",
      "Minibatch RMSE: 8.04477\n",
      "Validation RMSE: 6.14474\n",
      "Minibatch loss at step 9000: 2814.239502\n",
      "Minibatch RMSE: 8.66293\n",
      "Validation RMSE: 6.02028\n",
      "Minibatch loss at step 10000: 2525.492432\n",
      "Minibatch RMSE: 8.20649\n",
      "Validation RMSE: 5.89803\n",
      "Minibatch loss at step 11000: 2005.421021\n",
      "Minibatch RMSE: 7.31286\n",
      "Validation RMSE: 5.77793\n",
      "Minibatch loss at step 12000: 2781.343506\n",
      "Minibatch RMSE: 8.61215\n",
      "Validation RMSE: 5.66035\n",
      "Minibatch loss at step 13000: 2257.710693\n",
      "Minibatch RMSE: 7.75923\n",
      "Validation RMSE: 5.54451\n",
      "Minibatch loss at step 14000: 1803.304321\n",
      "Minibatch RMSE: 6.93456\n",
      "Validation RMSE: 5.43090\n",
      "Minibatch loss at step 15000: 1669.255127\n",
      "Minibatch RMSE: 6.67184\n",
      "Validation RMSE: 5.32013\n",
      "Minibatch loss at step 16000: 1684.057739\n",
      "Minibatch RMSE: 6.70136\n",
      "Validation RMSE: 5.21064\n",
      "Minibatch loss at step 17000: 1443.012573\n",
      "Minibatch RMSE: 6.20325\n",
      "Validation RMSE: 5.10427\n",
      "Minibatch loss at step 18000: 1837.325439\n",
      "Minibatch RMSE: 6.99967\n",
      "Validation RMSE: 4.99995\n",
      "Minibatch loss at step 19000: 1370.038940\n",
      "Minibatch RMSE: 6.04437\n",
      "Validation RMSE: 4.89738\n",
      "Minibatch loss at step 20000: 1696.534180\n",
      "Minibatch RMSE: 6.72614\n",
      "Validation RMSE: 4.79747\n",
      "Minibatch loss at step 21000: 1679.579834\n",
      "Minibatch RMSE: 6.69244\n",
      "Validation RMSE: 4.69915\n",
      "Minibatch loss at step 22000: 1861.823975\n",
      "Minibatch RMSE: 7.04618\n",
      "Validation RMSE: 4.60335\n",
      "Minibatch loss at step 23000: 1276.038696\n",
      "Minibatch RMSE: 5.83333\n",
      "Validation RMSE: 4.50855\n",
      "Minibatch loss at step 24000: 1580.027222\n",
      "Minibatch RMSE: 6.49108\n",
      "Validation RMSE: 4.41671\n",
      "Minibatch loss at step 25000: 1410.301025\n",
      "Minibatch RMSE: 6.13254\n",
      "Validation RMSE: 4.32640\n",
      "Minibatch loss at step 26000: 1094.719238\n",
      "Minibatch RMSE: 5.40301\n",
      "Validation RMSE: 4.23877\n",
      "Minibatch loss at step 27000: 1257.165405\n",
      "Minibatch RMSE: 5.79003\n",
      "Validation RMSE: 4.15302\n",
      "Minibatch loss at step 28000: 983.771912\n",
      "Minibatch RMSE: 5.12191\n",
      "Validation RMSE: 4.06880\n",
      "Minibatch loss at step 29000: 1163.827271\n",
      "Minibatch RMSE: 5.57094\n",
      "Validation RMSE: 3.98631\n",
      "Minibatch loss at step 30000: 1463.275391\n",
      "Minibatch RMSE: 6.24665\n",
      "Validation RMSE: 3.90630\n",
      "Minibatch loss at step 31000: 1096.560303\n",
      "Minibatch RMSE: 5.40755\n",
      "Validation RMSE: 3.82798\n",
      "Minibatch loss at step 32000: 879.477600\n",
      "Minibatch RMSE: 4.84280\n",
      "Validation RMSE: 3.75165\n",
      "Minibatch loss at step 33000: 707.131836\n",
      "Minibatch RMSE: 4.34245\n",
      "Validation RMSE: 3.67773\n",
      "Minibatch loss at step 34000: 1003.219604\n",
      "Minibatch RMSE: 5.17228\n",
      "Validation RMSE: 3.60604\n",
      "Minibatch loss at step 35000: 1002.486206\n",
      "Minibatch RMSE: 5.17039\n",
      "Validation RMSE: 3.53631\n",
      "Minibatch loss at step 36000: 663.228638\n",
      "Minibatch RMSE: 4.20548\n",
      "Validation RMSE: 3.46879\n",
      "Minibatch loss at step 37000: 616.410095\n",
      "Minibatch RMSE: 4.05433\n",
      "Validation RMSE: 3.40287\n",
      "Minibatch loss at step 38000: 794.954224\n",
      "Minibatch RMSE: 4.60421\n",
      "Validation RMSE: 3.33920\n",
      "Minibatch loss at step 39000: 688.158813\n",
      "Minibatch RMSE: 4.28380\n",
      "Validation RMSE: 3.27770\n",
      "Minibatch loss at step 40000: 679.160095\n",
      "Minibatch RMSE: 4.25569\n",
      "Validation RMSE: 3.21773\n",
      "Minibatch loss at step 41000: 555.275146\n",
      "Minibatch RMSE: 3.84803\n",
      "Validation RMSE: 3.15956\n",
      "Minibatch loss at step 42000: 756.645447\n",
      "Minibatch RMSE: 4.49190\n",
      "Validation RMSE: 3.10366\n",
      "Minibatch loss at step 43000: 646.680176\n",
      "Minibatch RMSE: 4.15269\n",
      "Validation RMSE: 3.04945\n",
      "Minibatch loss at step 44000: 536.888000\n",
      "Minibatch RMSE: 3.78378\n",
      "Validation RMSE: 2.99717\n",
      "Minibatch loss at step 45000: 604.185730\n",
      "Minibatch RMSE: 4.01393\n",
      "Validation RMSE: 2.94678\n",
      "Minibatch loss at step 46000: 472.504486\n",
      "Minibatch RMSE: 3.54966\n",
      "Validation RMSE: 2.89827\n",
      "Minibatch loss at step 47000: 441.626221\n",
      "Minibatch RMSE: 3.43172\n",
      "Validation RMSE: 2.85161\n",
      "Minibatch loss at step 48000: 443.761566\n",
      "Minibatch RMSE: 3.44001\n",
      "Validation RMSE: 2.80636\n",
      "Minibatch loss at step 49000: 481.020508\n",
      "Minibatch RMSE: 3.58151\n",
      "Validation RMSE: 2.76280\n",
      "Minibatch loss at step 50000: 551.392639\n",
      "Minibatch RMSE: 3.83455\n",
      "Validation RMSE: 2.72098\n",
      "Minibatch loss at step 51000: 450.155884\n",
      "Minibatch RMSE: 3.46470\n",
      "Validation RMSE: 2.68052\n",
      "Minibatch loss at step 52000: 382.429077\n",
      "Minibatch RMSE: 3.19345\n",
      "Validation RMSE: 2.64168\n",
      "Minibatch loss at step 53000: 405.406342\n",
      "Minibatch RMSE: 3.28798\n",
      "Validation RMSE: 2.60476\n",
      "Minibatch loss at step 54000: 401.169495\n",
      "Minibatch RMSE: 3.27076\n",
      "Validation RMSE: 2.56886\n",
      "Minibatch loss at step 55000: 368.686066\n",
      "Minibatch RMSE: 3.13554\n",
      "Validation RMSE: 2.53410\n",
      "Minibatch loss at step 56000: 330.007202\n",
      "Minibatch RMSE: 2.96651\n",
      "Validation RMSE: 2.50099\n",
      "Minibatch loss at step 57000: 352.159698\n",
      "Minibatch RMSE: 3.06446\n",
      "Validation RMSE: 2.46940\n",
      "Minibatch loss at step 58000: 326.949219\n",
      "Minibatch RMSE: 2.95274\n",
      "Validation RMSE: 2.43896\n",
      "Minibatch loss at step 59000: 245.628494\n",
      "Minibatch RMSE: 2.55931\n",
      "Validation RMSE: 2.40951\n",
      "Minibatch loss at step 60000: 361.977905\n",
      "Minibatch RMSE: 3.10689\n",
      "Validation RMSE: 2.38206\n",
      "Minibatch loss at step 61000: 370.220184\n",
      "Minibatch RMSE: 3.14206\n",
      "Validation RMSE: 2.35534\n",
      "Minibatch loss at step 62000: 467.042572\n",
      "Minibatch RMSE: 3.52909\n",
      "Validation RMSE: 2.32981\n",
      "Minibatch loss at step 63000: 251.364624\n",
      "Minibatch RMSE: 2.58903\n",
      "Validation RMSE: 2.30534\n",
      "Minibatch loss at step 64000: 381.024811\n",
      "Minibatch RMSE: 3.18758\n",
      "Validation RMSE: 2.28183\n",
      "Minibatch loss at step 65000: 256.779358\n",
      "Minibatch RMSE: 2.61676\n",
      "Validation RMSE: 2.25949\n",
      "Minibatch loss at step 66000: 213.866821\n",
      "Minibatch RMSE: 2.38812\n",
      "Validation RMSE: 2.23793\n",
      "Minibatch loss at step 67000: 239.889191\n",
      "Minibatch RMSE: 2.52924\n",
      "Validation RMSE: 2.21737\n",
      "Minibatch loss at step 68000: 340.557312\n",
      "Minibatch RMSE: 3.01356\n",
      "Validation RMSE: 2.19758\n",
      "Minibatch loss at step 69000: 202.185822\n",
      "Minibatch RMSE: 2.32199\n",
      "Validation RMSE: 2.17906\n",
      "Minibatch loss at step 70000: 261.389923\n",
      "Minibatch RMSE: 2.64015\n",
      "Validation RMSE: 2.16099\n",
      "Minibatch loss at step 71000: 208.181030\n",
      "Minibatch RMSE: 2.35616\n",
      "Validation RMSE: 2.14397\n",
      "Minibatch loss at step 72000: 214.750107\n",
      "Minibatch RMSE: 2.39305\n",
      "Validation RMSE: 2.12758\n",
      "Minibatch loss at step 73000: 247.349991\n",
      "Minibatch RMSE: 2.56827\n",
      "Validation RMSE: 2.11206\n",
      "Minibatch loss at step 74000: 212.224579\n",
      "Minibatch RMSE: 2.37893\n",
      "Validation RMSE: 2.09733\n",
      "Minibatch loss at step 75000: 199.577576\n",
      "Minibatch RMSE: 2.30696\n",
      "Validation RMSE: 2.08312\n",
      "Minibatch loss at step 76000: 227.726120\n",
      "Minibatch RMSE: 2.46428\n",
      "Validation RMSE: 2.06961\n",
      "Minibatch loss at step 77000: 187.136169\n",
      "Minibatch RMSE: 2.23390\n",
      "Validation RMSE: 2.05664\n",
      "Minibatch loss at step 78000: 208.698761\n",
      "Minibatch RMSE: 2.35909\n",
      "Validation RMSE: 2.04403\n",
      "Minibatch loss at step 79000: 167.562576\n",
      "Minibatch RMSE: 2.11384\n",
      "Validation RMSE: 2.03235\n",
      "Minibatch loss at step 80000: 183.103012\n",
      "Minibatch RMSE: 2.20969\n",
      "Validation RMSE: 2.02088\n",
      "Minibatch loss at step 81000: 214.018219\n",
      "Minibatch RMSE: 2.38896\n",
      "Validation RMSE: 2.01005\n",
      "Minibatch loss at step 82000: 179.747299\n",
      "Minibatch RMSE: 2.18935\n",
      "Validation RMSE: 1.99954\n",
      "Minibatch loss at step 83000: 166.666382\n",
      "Minibatch RMSE: 2.10818\n",
      "Validation RMSE: 1.98941\n",
      "Minibatch loss at step 84000: 147.040817\n",
      "Minibatch RMSE: 1.98017\n",
      "Validation RMSE: 1.97988\n",
      "Minibatch loss at step 85000: 213.639618\n",
      "Minibatch RMSE: 2.38685\n",
      "Validation RMSE: 1.97040\n",
      "Minibatch loss at step 86000: 185.230988\n",
      "Minibatch RMSE: 2.22250\n",
      "Validation RMSE: 1.96138\n",
      "Minibatch loss at step 87000: 172.959320\n",
      "Minibatch RMSE: 2.14761\n",
      "Validation RMSE: 1.95259\n",
      "Minibatch loss at step 88000: 170.436798\n",
      "Minibatch RMSE: 2.13190\n",
      "Validation RMSE: 1.94407\n",
      "Minibatch loss at step 89000: 167.903534\n",
      "Minibatch RMSE: 2.11599\n",
      "Validation RMSE: 1.93565\n",
      "Minibatch loss at step 90000: 144.774689\n",
      "Minibatch RMSE: 1.96486\n",
      "Validation RMSE: 1.92756\n",
      "Minibatch loss at step 91000: 185.177094\n",
      "Minibatch RMSE: 2.22217\n",
      "Validation RMSE: 1.92046\n",
      "Minibatch loss at step 92000: 164.258575\n",
      "Minibatch RMSE: 2.09290\n",
      "Validation RMSE: 1.91308\n",
      "Minibatch loss at step 93000: 161.216919\n",
      "Minibatch RMSE: 2.07343\n",
      "Validation RMSE: 1.90610\n",
      "Minibatch loss at step 94000: 167.215240\n",
      "Minibatch RMSE: 2.11165\n",
      "Validation RMSE: 1.89943\n",
      "Minibatch loss at step 95000: 145.852005\n",
      "Minibatch RMSE: 1.97215\n",
      "Validation RMSE: 1.89288\n",
      "Minibatch loss at step 96000: 178.036469\n",
      "Minibatch RMSE: 2.17891\n",
      "Validation RMSE: 1.88679\n",
      "Minibatch loss at step 97000: 185.358551\n",
      "Minibatch RMSE: 2.22326\n",
      "Validation RMSE: 1.88050\n",
      "Minibatch loss at step 98000: 195.621353\n",
      "Minibatch RMSE: 2.28398\n",
      "Validation RMSE: 1.87445\n",
      "Minibatch loss at step 99000: 169.071396\n",
      "Minibatch RMSE: 2.12334\n",
      "Validation RMSE: 1.86887\n",
      "Minibatch loss at step 100000: 167.560638\n",
      "Minibatch RMSE: 2.11383\n",
      "Validation RMSE: 1.86378\n",
      "Minibatch loss at step 101000: 196.191101\n",
      "Minibatch RMSE: 2.28730\n",
      "Validation RMSE: 1.85874\n",
      "Minibatch loss at step 102000: 165.223633\n",
      "Minibatch RMSE: 2.09904\n",
      "Validation RMSE: 1.85398\n",
      "Minibatch loss at step 103000: 169.708527\n",
      "Minibatch RMSE: 2.12734\n",
      "Validation RMSE: 1.84924\n",
      "Minibatch loss at step 104000: 148.696686\n",
      "Minibatch RMSE: 1.99129\n",
      "Validation RMSE: 1.84459\n",
      "Minibatch loss at step 105000: 135.805618\n",
      "Minibatch RMSE: 1.90302\n",
      "Validation RMSE: 1.84026\n",
      "Minibatch loss at step 106000: 163.146790\n",
      "Minibatch RMSE: 2.08580\n",
      "Validation RMSE: 1.83617\n",
      "Minibatch loss at step 107000: 146.448029\n",
      "Minibatch RMSE: 1.97618\n",
      "Validation RMSE: 1.83197\n",
      "Minibatch loss at step 108000: 142.066742\n",
      "Minibatch RMSE: 1.94639\n",
      "Validation RMSE: 1.82799\n",
      "Minibatch loss at step 109000: 138.408798\n",
      "Minibatch RMSE: 1.92117\n",
      "Validation RMSE: 1.82427\n",
      "Minibatch loss at step 110000: 163.020081\n",
      "Minibatch RMSE: 2.08499\n",
      "Validation RMSE: 1.82083\n",
      "Minibatch loss at step 111000: 156.976669\n",
      "Minibatch RMSE: 2.04598\n",
      "Validation RMSE: 1.81767\n",
      "Minibatch loss at step 112000: 144.265533\n",
      "Minibatch RMSE: 1.96140\n",
      "Validation RMSE: 1.81455\n",
      "Minibatch loss at step 113000: 144.619431\n",
      "Minibatch RMSE: 1.96380\n",
      "Validation RMSE: 1.81144\n",
      "Minibatch loss at step 114000: 141.847733\n",
      "Minibatch RMSE: 1.94489\n",
      "Validation RMSE: 1.80841\n",
      "Minibatch loss at step 115000: 137.986435\n",
      "Minibatch RMSE: 1.91824\n",
      "Validation RMSE: 1.80571\n",
      "Minibatch loss at step 116000: 134.174484\n",
      "Minibatch RMSE: 1.89156\n",
      "Validation RMSE: 1.80314\n",
      "Minibatch loss at step 117000: 131.115524\n",
      "Minibatch RMSE: 1.86987\n",
      "Validation RMSE: 1.80062\n",
      "Minibatch loss at step 118000: 173.339661\n",
      "Minibatch RMSE: 2.14997\n",
      "Validation RMSE: 1.79799\n",
      "Minibatch loss at step 119000: 133.275269\n",
      "Minibatch RMSE: 1.88521\n",
      "Validation RMSE: 1.79524\n",
      "Minibatch loss at step 120000: 131.810013\n",
      "Minibatch RMSE: 1.87482\n",
      "Validation RMSE: 1.79281\n",
      "Minibatch loss at step 121000: 144.147400\n",
      "Minibatch RMSE: 1.96059\n",
      "Validation RMSE: 1.79045\n",
      "Minibatch loss at step 122000: 137.865646\n",
      "Minibatch RMSE: 1.91740\n",
      "Validation RMSE: 1.78812\n",
      "Minibatch loss at step 123000: 170.007278\n",
      "Minibatch RMSE: 2.12921\n",
      "Validation RMSE: 1.78578\n",
      "Minibatch loss at step 124000: 146.779663\n",
      "Minibatch RMSE: 1.97841\n",
      "Validation RMSE: 1.78367\n",
      "Minibatch loss at step 125000: 151.613022\n",
      "Minibatch RMSE: 2.01072\n",
      "Validation RMSE: 1.78173\n",
      "Minibatch loss at step 126000: 139.742142\n",
      "Minibatch RMSE: 1.93040\n",
      "Validation RMSE: 1.77979\n",
      "Minibatch loss at step 127000: 162.718292\n",
      "Minibatch RMSE: 2.08306\n",
      "Validation RMSE: 1.77769\n",
      "Minibatch loss at step 128000: 142.626984\n",
      "Minibatch RMSE: 1.95023\n",
      "Validation RMSE: 1.77542\n",
      "Minibatch loss at step 129000: 136.766922\n",
      "Minibatch RMSE: 1.90974\n",
      "Validation RMSE: 1.77346\n",
      "Minibatch loss at step 130000: 129.935043\n",
      "Minibatch RMSE: 1.86143\n",
      "Validation RMSE: 1.77158\n",
      "Minibatch loss at step 131000: 127.074646\n",
      "Minibatch RMSE: 1.84083\n",
      "Validation RMSE: 1.76969\n",
      "Minibatch loss at step 132000: 159.054016\n",
      "Minibatch RMSE: 2.05948\n",
      "Validation RMSE: 1.76752\n",
      "Minibatch loss at step 133000: 138.893753\n",
      "Minibatch RMSE: 1.92453\n",
      "Validation RMSE: 1.76546\n",
      "Minibatch loss at step 134000: 133.543686\n",
      "Minibatch RMSE: 1.88710\n",
      "Validation RMSE: 1.76351\n",
      "Minibatch loss at step 135000: 134.827744\n",
      "Minibatch RMSE: 1.89616\n",
      "Validation RMSE: 1.76166\n",
      "Minibatch loss at step 136000: 124.598495\n",
      "Minibatch RMSE: 1.82281\n",
      "Validation RMSE: 1.75966\n",
      "Minibatch loss at step 137000: 120.472168\n",
      "Minibatch RMSE: 1.79237\n",
      "Validation RMSE: 1.75749\n",
      "Minibatch loss at step 138000: 147.930176\n",
      "Minibatch RMSE: 1.98615\n",
      "Validation RMSE: 1.75536\n",
      "Minibatch loss at step 139000: 144.486877\n",
      "Minibatch RMSE: 1.96290\n",
      "Validation RMSE: 1.75354\n",
      "Minibatch loss at step 140000: 137.749466\n",
      "Minibatch RMSE: 1.91659\n",
      "Validation RMSE: 1.75160\n",
      "Minibatch loss at step 141000: 112.169838\n",
      "Minibatch RMSE: 1.72951\n",
      "Validation RMSE: 1.74938\n",
      "Minibatch loss at step 142000: 129.305618\n",
      "Minibatch RMSE: 1.85692\n",
      "Validation RMSE: 1.74739\n",
      "Minibatch loss at step 143000: 127.037231\n",
      "Minibatch RMSE: 1.84056\n",
      "Validation RMSE: 1.74521\n",
      "Minibatch loss at step 144000: 129.214325\n",
      "Minibatch RMSE: 1.85626\n",
      "Validation RMSE: 1.74325\n",
      "Minibatch loss at step 145000: 136.458435\n",
      "Minibatch RMSE: 1.90759\n",
      "Validation RMSE: 1.74143\n",
      "Minibatch loss at step 146000: 140.636490\n",
      "Minibatch RMSE: 1.93657\n",
      "Validation RMSE: 1.73953\n",
      "Minibatch loss at step 147000: 134.172882\n",
      "Minibatch RMSE: 1.89155\n",
      "Validation RMSE: 1.73753\n",
      "Minibatch loss at step 148000: 122.820869\n",
      "Minibatch RMSE: 1.80976\n",
      "Validation RMSE: 1.73544\n",
      "Minibatch loss at step 149000: 132.597412\n",
      "Minibatch RMSE: 1.88041\n",
      "Validation RMSE: 1.73336\n",
      "Minibatch loss at step 150000: 110.234138\n",
      "Minibatch RMSE: 1.71452\n",
      "Validation RMSE: 1.73135\n",
      "Minibatch loss at step 151000: 124.448662\n",
      "Minibatch RMSE: 1.82171\n",
      "Validation RMSE: 1.72937\n",
      "Minibatch loss at step 152000: 137.789856\n",
      "Minibatch RMSE: 1.91687\n",
      "Validation RMSE: 1.72730\n",
      "Minibatch loss at step 153000: 131.408539\n",
      "Minibatch RMSE: 1.87196\n",
      "Validation RMSE: 1.72537\n",
      "Minibatch loss at step 154000: 137.825439\n",
      "Minibatch RMSE: 1.91712\n",
      "Validation RMSE: 1.72355\n",
      "Minibatch loss at step 155000: 142.517059\n",
      "Minibatch RMSE: 1.94948\n",
      "Validation RMSE: 1.72152\n",
      "Minibatch loss at step 156000: 155.511887\n",
      "Minibatch RMSE: 2.03641\n",
      "Validation RMSE: 1.71946\n",
      "Minibatch loss at step 157000: 112.654541\n",
      "Minibatch RMSE: 1.73324\n",
      "Validation RMSE: 1.71760\n",
      "Minibatch loss at step 158000: 130.764297\n",
      "Minibatch RMSE: 1.86736\n",
      "Validation RMSE: 1.71509\n",
      "Minibatch loss at step 159000: 118.017532\n",
      "Minibatch RMSE: 1.77402\n",
      "Validation RMSE: 1.71311\n",
      "Minibatch loss at step 160000: 127.553535\n",
      "Minibatch RMSE: 1.84430\n",
      "Validation RMSE: 1.71156\n",
      "Minibatch loss at step 161000: 121.987564\n",
      "Minibatch RMSE: 1.80361\n",
      "Validation RMSE: 1.70973\n",
      "Minibatch loss at step 162000: 119.072800\n",
      "Minibatch RMSE: 1.78193\n",
      "Validation RMSE: 1.70805\n",
      "Minibatch loss at step 163000: 129.422440\n",
      "Minibatch RMSE: 1.85776\n",
      "Validation RMSE: 1.70597\n",
      "Minibatch loss at step 164000: 143.063187\n",
      "Minibatch RMSE: 1.95321\n",
      "Validation RMSE: 1.70396\n",
      "Minibatch loss at step 165000: 124.469765\n",
      "Minibatch RMSE: 1.82187\n",
      "Validation RMSE: 1.70209\n",
      "Minibatch loss at step 166000: 148.367065\n",
      "Minibatch RMSE: 1.98908\n",
      "Validation RMSE: 1.70001\n",
      "Minibatch loss at step 167000: 149.186066\n",
      "Minibatch RMSE: 1.99457\n",
      "Validation RMSE: 1.69806\n",
      "Minibatch loss at step 168000: 130.759476\n",
      "Minibatch RMSE: 1.86733\n",
      "Validation RMSE: 1.69613\n",
      "Minibatch loss at step 169000: 141.397858\n",
      "Minibatch RMSE: 1.94181\n",
      "Validation RMSE: 1.69447\n",
      "Minibatch loss at step 170000: 136.310669\n",
      "Minibatch RMSE: 1.90655\n",
      "Validation RMSE: 1.69296\n",
      "Minibatch loss at step 171000: 124.879478\n",
      "Minibatch RMSE: 1.82486\n",
      "Validation RMSE: 1.69071\n",
      "Minibatch loss at step 172000: 138.582291\n",
      "Minibatch RMSE: 1.92238\n",
      "Validation RMSE: 1.68901\n",
      "Minibatch loss at step 173000: 133.332855\n",
      "Minibatch RMSE: 1.88561\n",
      "Validation RMSE: 1.68712\n",
      "Minibatch loss at step 174000: 127.134636\n",
      "Minibatch RMSE: 1.84126\n",
      "Validation RMSE: 1.68526\n",
      "Minibatch loss at step 175000: 137.957596\n",
      "Minibatch RMSE: 1.91804\n",
      "Validation RMSE: 1.68338\n",
      "Minibatch loss at step 176000: 130.652161\n",
      "Minibatch RMSE: 1.86656\n",
      "Validation RMSE: 1.68170\n",
      "Minibatch loss at step 177000: 127.952782\n",
      "Minibatch RMSE: 1.84718\n",
      "Validation RMSE: 1.67965\n",
      "Minibatch loss at step 178000: 143.536667\n",
      "Minibatch RMSE: 1.95644\n",
      "Validation RMSE: 1.67807\n",
      "Minibatch loss at step 179000: 131.013748\n",
      "Minibatch RMSE: 1.86914\n",
      "Validation RMSE: 1.67611\n",
      "Minibatch loss at step 180000: 126.493713\n",
      "Minibatch RMSE: 1.83662\n",
      "Validation RMSE: 1.67432\n",
      "Minibatch loss at step 181000: 104.235870\n",
      "Minibatch RMSE: 1.66722\n",
      "Validation RMSE: 1.67222\n",
      "Minibatch loss at step 182000: 153.985519\n",
      "Minibatch RMSE: 2.02640\n",
      "Validation RMSE: 1.67031\n",
      "Minibatch loss at step 183000: 138.354187\n",
      "Minibatch RMSE: 1.92079\n",
      "Validation RMSE: 1.66832\n",
      "Minibatch loss at step 184000: 120.283546\n",
      "Minibatch RMSE: 1.79097\n",
      "Validation RMSE: 1.66665\n",
      "Minibatch loss at step 185000: 124.144974\n",
      "Minibatch RMSE: 1.81949\n",
      "Validation RMSE: 1.66472\n",
      "Minibatch loss at step 186000: 123.264862\n",
      "Minibatch RMSE: 1.81303\n",
      "Validation RMSE: 1.66289\n",
      "Minibatch loss at step 187000: 130.331482\n",
      "Minibatch RMSE: 1.86427\n",
      "Validation RMSE: 1.66093\n",
      "Minibatch loss at step 188000: 126.173157\n",
      "Minibatch RMSE: 1.83429\n",
      "Validation RMSE: 1.65891\n",
      "Minibatch loss at step 189000: 124.162949\n",
      "Minibatch RMSE: 1.81962\n",
      "Validation RMSE: 1.65706\n",
      "Minibatch loss at step 190000: 130.846054\n",
      "Minibatch RMSE: 1.86795\n",
      "Validation RMSE: 1.65500\n",
      "Minibatch loss at step 191000: 133.576523\n",
      "Minibatch RMSE: 1.88734\n",
      "Validation RMSE: 1.65341\n",
      "Minibatch loss at step 192000: 125.609070\n",
      "Minibatch RMSE: 1.83018\n",
      "Validation RMSE: 1.65157\n",
      "Minibatch loss at step 193000: 132.102509\n",
      "Minibatch RMSE: 1.87689\n",
      "Validation RMSE: 1.64964\n",
      "Minibatch loss at step 194000: 143.685028\n",
      "Minibatch RMSE: 1.95745\n",
      "Validation RMSE: 1.64778\n",
      "Minibatch loss at step 195000: 129.584763\n",
      "Minibatch RMSE: 1.85892\n",
      "Validation RMSE: 1.64638\n",
      "Minibatch loss at step 196000: 134.015717\n",
      "Minibatch RMSE: 1.89044\n",
      "Validation RMSE: 1.64459\n",
      "Minibatch loss at step 197000: 129.220993\n",
      "Minibatch RMSE: 1.85631\n",
      "Validation RMSE: 1.64255\n",
      "Minibatch loss at step 198000: 134.902908\n",
      "Minibatch RMSE: 1.89668\n",
      "Validation RMSE: 1.64080\n",
      "Minibatch loss at step 199000: 108.163452\n",
      "Minibatch RMSE: 1.69834\n",
      "Validation RMSE: 1.63929\n",
      "Minibatch loss at step 200000: 129.711945\n",
      "Minibatch RMSE: 1.85983\n",
      "Validation RMSE: 1.63740\n",
      "Minibatch loss at step 201000: 121.524551\n",
      "Minibatch RMSE: 1.80018\n",
      "Validation RMSE: 1.63552\n",
      "Minibatch loss at step 202000: 107.518532\n",
      "Minibatch RMSE: 1.69327\n",
      "Validation RMSE: 1.63386\n",
      "Minibatch loss at step 203000: 125.104919\n",
      "Minibatch RMSE: 1.82651\n",
      "Validation RMSE: 1.63230\n",
      "Minibatch loss at step 204000: 126.218582\n",
      "Minibatch RMSE: 1.83462\n",
      "Validation RMSE: 1.63054\n",
      "Minibatch loss at step 205000: 121.975853\n",
      "Minibatch RMSE: 1.80352\n",
      "Validation RMSE: 1.62857\n",
      "Minibatch loss at step 206000: 139.241104\n",
      "Minibatch RMSE: 1.92694\n",
      "Validation RMSE: 1.62684\n",
      "Minibatch loss at step 207000: 142.031631\n",
      "Minibatch RMSE: 1.94615\n",
      "Validation RMSE: 1.62515\n",
      "Minibatch loss at step 208000: 132.275604\n",
      "Minibatch RMSE: 1.87812\n",
      "Validation RMSE: 1.62325\n",
      "Minibatch loss at step 209000: 125.726425\n",
      "Minibatch RMSE: 1.83104\n",
      "Validation RMSE: 1.62147\n",
      "Minibatch loss at step 210000: 142.270264\n",
      "Minibatch RMSE: 1.94779\n",
      "Validation RMSE: 1.61974\n",
      "Minibatch loss at step 211000: 105.450287\n",
      "Minibatch RMSE: 1.67690\n",
      "Validation RMSE: 1.61771\n",
      "Minibatch loss at step 212000: 116.659973\n",
      "Minibatch RMSE: 1.76378\n",
      "Validation RMSE: 1.61627\n",
      "Minibatch loss at step 213000: 130.198959\n",
      "Minibatch RMSE: 1.86332\n",
      "Validation RMSE: 1.61461\n",
      "Minibatch loss at step 214000: 128.023010\n",
      "Minibatch RMSE: 1.84769\n",
      "Validation RMSE: 1.61274\n",
      "Minibatch loss at step 215000: 125.192329\n",
      "Minibatch RMSE: 1.82715\n",
      "Validation RMSE: 1.61126\n",
      "Minibatch loss at step 216000: 143.498398\n",
      "Minibatch RMSE: 1.95618\n",
      "Validation RMSE: 1.60943\n",
      "Minibatch loss at step 217000: 118.209145\n",
      "Minibatch RMSE: 1.77546\n",
      "Validation RMSE: 1.60775\n",
      "Minibatch loss at step 218000: 120.622871\n",
      "Minibatch RMSE: 1.79349\n",
      "Validation RMSE: 1.60569\n",
      "Minibatch loss at step 219000: 118.260216\n",
      "Minibatch RMSE: 1.77584\n",
      "Validation RMSE: 1.60404\n",
      "Minibatch loss at step 220000: 139.233185\n",
      "Minibatch RMSE: 1.92688\n",
      "Validation RMSE: 1.60200\n",
      "Minibatch loss at step 221000: 123.357018\n",
      "Minibatch RMSE: 1.81370\n",
      "Validation RMSE: 1.60050\n",
      "Minibatch loss at step 222000: 126.056953\n",
      "Minibatch RMSE: 1.83344\n",
      "Validation RMSE: 1.59869\n",
      "Minibatch loss at step 223000: 115.275665\n",
      "Minibatch RMSE: 1.75329\n",
      "Validation RMSE: 1.59712\n",
      "Minibatch loss at step 224000: 130.327942\n",
      "Minibatch RMSE: 1.86425\n",
      "Validation RMSE: 1.59518\n",
      "Minibatch loss at step 225000: 118.693192\n",
      "Minibatch RMSE: 1.77909\n",
      "Validation RMSE: 1.59382\n",
      "Minibatch loss at step 226000: 131.173889\n",
      "Minibatch RMSE: 1.87029\n",
      "Validation RMSE: 1.59209\n",
      "Minibatch loss at step 227000: 128.201141\n",
      "Minibatch RMSE: 1.84897\n",
      "Validation RMSE: 1.59043\n",
      "Minibatch loss at step 228000: 137.235947\n",
      "Minibatch RMSE: 1.91301\n",
      "Validation RMSE: 1.58884\n",
      "Minibatch loss at step 229000: 131.868423\n",
      "Minibatch RMSE: 1.87523\n",
      "Validation RMSE: 1.58704\n",
      "Minibatch loss at step 230000: 129.678558\n",
      "Minibatch RMSE: 1.85960\n",
      "Validation RMSE: 1.58544\n",
      "Minibatch loss at step 231000: 118.558167\n",
      "Minibatch RMSE: 1.77808\n",
      "Validation RMSE: 1.58369\n",
      "Minibatch loss at step 232000: 114.792816\n",
      "Minibatch RMSE: 1.74961\n",
      "Validation RMSE: 1.58204\n"
     ]
    }
   ],
   "source": [
    "n = 5\n",
    "lambda_ = 0 * .001\n",
    "hidden = 5\n",
    "\n",
    "train_x = np.random.normal(size=(20000, n))\n",
    "val_x = np.random.normal(size=(1000, n))\n",
    "test_x = np.random.normal(size=(1000, n))\n",
    "\n",
    "train_x = train_x.astype(np.float32)\n",
    "val_x = val_x.astype(np.float32)\n",
    "test_x = test_x.astype(np.float32)\n",
    "\n",
    "mults = np.array([np.arange(n) + 1, n - np.arange(n)])\n",
    "\n",
    "\"\"\"\n",
    "train_y = (np.dot(train_x, mults.T) + [5, -1])\n",
    "val_y = (np.dot(val_x, mults.T) + [5, -1])\n",
    "test_y = (np.dot(test_x, mults.T) + [5, -1])\n",
    "\n",
    "train_y += np.random.normal(size=train_y.shape)\n",
    "val_y += np.random.normal(size=val_y.shape)\n",
    "test_y += np.random.normal(size=test_y.shape)\n",
    "\"\"\"\n",
    "\n",
    "train_y = train_x\n",
    "val_y = val_x\n",
    "test_y = test_x\n",
    "\n",
    "train_y = train_y.astype(np.float32)\n",
    "val_y = val_y.astype(np.float32)\n",
    "test_y = test_y.astype(np.float32)\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 75\n",
    "num_labels = train_y.shape[1]\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,shape=(batch_size, n))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(val_x)\n",
    "    tf_test_dataset = tf.constant(test_x)\n",
    "    tf_tr_dataset = tf.constant(train_x)\n",
    "  \n",
    "    # Variables.\n",
    "    w0 = tf.Variable(tf.truncated_normal([n, hidden]))\n",
    "    b0 = tf.Variable(tf.zeros([hidden]))\n",
    "    w1 = tf.Variable(tf.truncated_normal([hidden, num_labels]))\n",
    "    b1 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "    # Training computation.\n",
    "    logits = tf.matmul(tf.nn.relu(tf.matmul(tf.nn.dropout(tf_train_dataset, .5), w0) + b0), w1) + b1\n",
    "    # loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    loss = tf.nn.l2_loss(logits - tf_train_labels) + lambda_ * (tf.nn.l2_loss(w0) + tf.nn.l2_loss(w1))\n",
    "  \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.AdamOptimizer(1e-5).minimize(loss)\n",
    "  \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    # train_prediction = tf.nn.softmax(logits)\n",
    "    train_prediction = logits\n",
    "    valid_prediction = tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, w0) + b0), w1) + b1\n",
    "    test_prediction = tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, w0) + b0), w1) + b1\n",
    "    tr_preds = tf.matmul(tf.nn.relu(tf.matmul(tf_tr_dataset, w0) + b0), w1) + b1\n",
    "    \n",
    "num_steps = 500001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_y.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_x[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_y[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 1000 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch RMSE: %.5f\" % rmse(predictions, batch_labels))\n",
    "            print(\"Validation RMSE: %.5f\" % rmse(valid_prediction.eval(), val_y))\n",
    "        \n",
    "    print(\"Test RMSE: %.5f\" % rmse(test_prediction.eval(), test_y))\n",
    "    print(\"\")\n",
    "    print w0.eval()\n",
    "    print b0.eval()\n",
    "    print(\"\")\n",
    "    preds = tr_preds.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(train_y.reshape([-1,1]), preds.reshape([-1,1]))\n",
    "plt.plot([-5, 5], [-5, 5], 'k-', lw=2, color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [tensorflow]",
   "language": "python",
   "name": "Python [tensorflow]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
