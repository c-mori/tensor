{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import cPickle as pickle\n",
    "\n",
    "pd.set_option('display.width', 115)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle('df.pkl')\n",
    "with open('train.pkl', 'rb') as ffile:\n",
    "    train = pickle.load(ffile)\n",
    "with open('test.pkl', 'rb') as ffile:\n",
    "    test = pickle.load(ffile)\n",
    "with open('val.pkl', 'rb') as ffile:\n",
    "    val = pickle.load(ffile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    cik   asof_date      cusip        company     pv class_type  shares  gross_pv         bps       te  item  \\\n",
      "0  2230  2016-03-31  037833100      APPLE INC  63345        COM  581200   1436665  440.916985  AAPL:US  AAPL   \n",
      "1  2230  2016-03-31  00287Y109     ABBVIE INC  21706        COM  380000   1436665  151.086022  ABBV:US  ABBV   \n",
      "2  2230  2016-03-31  00724F101  ADOBE SYS INC  16509        COM  176000   1436665  114.911966  ADBE:US  ADBE   \n",
      "3  2230  2016-03-31  00817Y108  AETNA INC NEW  20661        COM  183900   1436665  143.812232   AET:US   AET   \n",
      "4  2230  2016-03-31  G0177J108   ALLERGAN PLC  28705        SHS  107096   1436665  199.803016   AGN:US   AGN   \n",
      "\n",
      "     rating  \n",
      "0  8.517425  \n",
      "1  8.106413  \n",
      "2  8.008257  \n",
      "3  8.088518  \n",
      "4  8.209460  \n"
     ]
    }
   ],
   "source": [
    "print df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "items = df.drop_duplicates('item').item.values\n",
    "users = df.drop_duplicates('cik').cik.values\n",
    "items.sort()\n",
    "users.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4128\n",
      "14772\n",
      "(4128, 14772)\n"
     ]
    }
   ],
   "source": [
    "print len(users)\n",
    "print len(items)\n",
    "print train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, actual):\n",
    "    sse = np.sum(np.square(predictions - actual))\n",
    "    return np.sqrt(sse / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Avg Minibatch loss for epoch 0: 23554666340.23264\n",
      "  ***   Training RMSE: 404.01425 ***\n",
      "  *** Validation RMSE: 6398.68833 ***\n",
      "Avg Minibatch loss for epoch 1: 6909437650.43792\n",
      "Avg Minibatch loss for epoch 2: 4119671821.53378\n",
      "Avg Minibatch loss for epoch 3: 2877381840.36486\n",
      "Avg Minibatch loss for epoch 4: 2220169234.43243\n",
      "Avg Minibatch loss for epoch 5: 1736126954.81081\n",
      "  ***   Training RMSE: 549.20302 ***\n",
      "  *** Validation RMSE: 5544.60926 ***\n",
      "Avg Minibatch loss for epoch 6: 1434765843.24324\n",
      "Avg Minibatch loss for epoch 7: 1148722175.29730\n",
      "Avg Minibatch loss for epoch 8: 943981748.83784\n",
      "Avg Minibatch loss for epoch 9: 782010182.21622\n",
      "Avg Minibatch loss for epoch 10: 653782169.68919\n",
      "  ***   Training RMSE: 569.15455 ***\n",
      "  *** Validation RMSE: 5003.59599 ***\n",
      "Avg Minibatch loss for epoch 11: 533259501.69595\n",
      "Avg Minibatch loss for epoch 12: 438961064.79730\n",
      "Avg Minibatch loss for epoch 13: 367489606.63514\n",
      "Avg Minibatch loss for epoch 14: 307572455.57432\n",
      "Avg Minibatch loss for epoch 15: 253672807.16554\n",
      "  ***   Training RMSE: 483.42916 ***\n",
      "  *** Validation RMSE: 4474.76929 ***\n",
      "Avg Minibatch loss for epoch 16: 213369325.52787\n",
      "Avg Minibatch loss for epoch 17: 173200782.79392\n",
      "Avg Minibatch loss for epoch 18: 148739199.45524\n",
      "Avg Minibatch loss for epoch 19: 119832422.98902\n",
      "Avg Minibatch loss for epoch 20: 101735707.00486\n",
      "  ***   Training RMSE: 301.87162 ***\n",
      "  *** Validation RMSE: 4111.58299 ***\n",
      "Avg Minibatch loss for epoch 21: 85874168.80068\n",
      "Avg Minibatch loss for epoch 22: 70233015.03289\n",
      "Avg Minibatch loss for epoch 23: 57394130.61946\n",
      "Avg Minibatch loss for epoch 24: 48419546.77753\n",
      "Avg Minibatch loss for epoch 25: 40884113.65628\n",
      "  ***   Training RMSE: 127.34137 ***\n",
      "  *** Validation RMSE: 3780.57541 ***\n",
      "Avg Minibatch loss for epoch 26: 34334978.46491\n",
      "Avg Minibatch loss for epoch 27: 27137995.80763\n",
      "Avg Minibatch loss for epoch 28: 23153737.27944\n",
      "Avg Minibatch loss for epoch 29: 18696184.32311\n",
      "Avg Minibatch loss for epoch 30: 15685572.47794\n",
      "  ***   Training RMSE: 46.40654 ***\n",
      "  *** Validation RMSE: 3520.95722 ***\n",
      "Avg Minibatch loss for epoch 31: 13241602.61185\n",
      "Avg Minibatch loss for epoch 32: 10853768.39685\n",
      "Avg Minibatch loss for epoch 33: 8962273.40027\n",
      "Avg Minibatch loss for epoch 34: 6871428.72372\n",
      "Avg Minibatch loss for epoch 35: 5954028.29524\n",
      "  ***   Training RMSE: 17.00466 ***\n",
      "  *** Validation RMSE: 3374.06011 ***\n",
      "Avg Minibatch loss for epoch 36: 4645178.52256\n",
      "Avg Minibatch loss for epoch 37: 3835116.32271\n",
      "Avg Minibatch loss for epoch 38: 3353927.88987\n",
      "Avg Minibatch loss for epoch 39: 2623368.31507\n",
      "Avg Minibatch loss for epoch 40: 2191097.81559\n",
      "  ***   Training RMSE: 6.45190 ***\n",
      "  *** Validation RMSE: 3238.21094 ***\n",
      "Avg Minibatch loss for epoch 41: 1797046.09780\n",
      "Avg Minibatch loss for epoch 42: 1592362.98034\n",
      "Avg Minibatch loss for epoch 43: 1309403.91226\n",
      "Avg Minibatch loss for epoch 44: 969851.13178\n",
      "Avg Minibatch loss for epoch 45: 868780.60726\n",
      "  ***   Training RMSE: 3.49077 ***\n",
      "  *** Validation RMSE: 3159.55348 ***\n",
      "Avg Minibatch loss for epoch 46: 690637.60419\n",
      "Avg Minibatch loss for epoch 47: 630120.65294\n",
      "Avg Minibatch loss for epoch 48: 527079.86734\n",
      "Avg Minibatch loss for epoch 49: 402954.29174\n",
      "Avg Minibatch loss for epoch 50: 367693.91665\n",
      "  ***   Training RMSE: 1.80509 ***\n",
      "  *** Validation RMSE: 3119.15516 ***\n",
      "Avg Minibatch loss for epoch 51: 292871.64818\n",
      "Avg Minibatch loss for epoch 52: 289331.51450\n",
      "Avg Minibatch loss for epoch 53: 243210.78078\n",
      "Avg Minibatch loss for epoch 54: 223116.91821\n",
      "Avg Minibatch loss for epoch 55: 186943.97641\n",
      "  ***   Training RMSE: 1.48674 ***\n",
      "  *** Validation RMSE: 3054.51936 ***\n",
      "Avg Minibatch loss for epoch 56: 186363.63913\n",
      "Avg Minibatch loss for epoch 57: 164830.27499\n",
      "Avg Minibatch loss for epoch 58: 155627.14625\n",
      "Avg Minibatch loss for epoch 59: 155670.13587\n",
      "Avg Minibatch loss for epoch 60: 142718.43093\n",
      "  ***   Training RMSE: 1.43872 ***\n",
      "  *** Validation RMSE: 3013.25643 ***\n",
      "Avg Minibatch loss for epoch 61: 341217.74132\n",
      "Avg Minibatch loss for epoch 62: 661283.09128\n",
      "Avg Minibatch loss for epoch 63: 497251.00762\n",
      "Avg Minibatch loss for epoch 64: 765798.00710\n",
      "Avg Minibatch loss for epoch 65: 976776.91167\n",
      "  ***   Training RMSE: 4.18692 ***\n",
      "  *** Validation RMSE: 2265.22665 ***\n",
      "Avg Minibatch loss for epoch 66: 515020.68650\n",
      "Avg Minibatch loss for epoch 67: 877299.76912\n",
      "Avg Minibatch loss for epoch 68: 475927.11507\n",
      "Avg Minibatch loss for epoch 69: 451638.35753\n",
      "Avg Minibatch loss for epoch 70: 1267915.89493\n",
      "  ***   Training RMSE: 2.20780 ***\n",
      "  *** Validation RMSE: 1572.56539 ***\n",
      "Avg Minibatch loss for epoch 71: 372544.43117\n",
      "Avg Minibatch loss for epoch 72: 354756.40105\n",
      "Avg Minibatch loss for epoch 73: 290108.40205\n",
      "Avg Minibatch loss for epoch 74: 115883.92906\n",
      "Avg Minibatch loss for epoch 75: 116658.48599\n",
      "  ***   Training RMSE: 1.14322 ***\n",
      "  *** Validation RMSE: 1239.34782 ***\n",
      "Avg Minibatch loss for epoch 76: 117752.49871\n",
      "Avg Minibatch loss for epoch 77: 108104.61376\n",
      "Avg Minibatch loss for epoch 78: 106654.49095\n",
      "Avg Minibatch loss for epoch 79: 106778.24943\n",
      "Avg Minibatch loss for epoch 80: 108720.65033\n",
      "  ***   Training RMSE: 1.11781 ***\n",
      "  *** Validation RMSE: 1223.09694 ***\n",
      "Avg Minibatch loss for epoch 81: 109286.82404\n",
      "Avg Minibatch loss for epoch 82: 106646.19634\n",
      "Avg Minibatch loss for epoch 83: 105153.98390\n",
      "Avg Minibatch loss for epoch 84: 104669.83863\n",
      "Avg Minibatch loss for epoch 85: 108419.18144\n",
      "  ***   Training RMSE: 1.17154 ***\n",
      "  *** Validation RMSE: 1205.79556 ***\n",
      "Avg Minibatch loss for epoch 86: 109776.05711\n",
      "Avg Minibatch loss for epoch 87: 110431.05688\n",
      "Avg Minibatch loss for epoch 88: 104675.25190\n",
      "Avg Minibatch loss for epoch 89: 104094.79050\n",
      "Avg Minibatch loss for epoch 90: 103818.58667\n",
      "  ***   Training RMSE: 1.06597 ***\n",
      "  *** Validation RMSE: 1200.32995 ***\n",
      "Avg Minibatch loss for epoch 91: 106503.19752\n",
      "Avg Minibatch loss for epoch 92: 113564.52048\n",
      "Avg Minibatch loss for epoch 93: 111741.46042\n",
      "Avg Minibatch loss for epoch 94: 115439.15087\n",
      "Avg Minibatch loss for epoch 95: 133726.85316\n",
      "  ***   Training RMSE: 1.07689 ***\n",
      "  *** Validation RMSE: 1197.11413 ***\n",
      "Avg Minibatch loss for epoch 96: 146959.04695\n",
      "Avg Minibatch loss for epoch 97: 6002993.72164\n",
      "Avg Minibatch loss for epoch 98: 62490780.32068\n",
      "Avg Minibatch loss for epoch 99: 41523349.06393\n",
      "Avg Minibatch loss for epoch 100: 18078580.56716\n",
      "  ***   Training RMSE: 84.49061 ***\n",
      "  *** Validation RMSE: 1281.55917 ***\n",
      "Avg Minibatch loss for epoch 101: 38649405.16889\n",
      "Avg Minibatch loss for epoch 102: 32390275.69564\n",
      "Avg Minibatch loss for epoch 103: 78439349.17444\n",
      "Avg Minibatch loss for epoch 104: 51537567.74312\n",
      "Avg Minibatch loss for epoch 105: 4086847.67480\n",
      "  ***   Training RMSE: 25.35832 ***\n",
      "  *** Validation RMSE: 1195.21284 ***\n",
      "Avg Minibatch loss for epoch 106: 1643890.18177\n",
      "Avg Minibatch loss for epoch 107: 1540785.01047\n",
      "Avg Minibatch loss for epoch 108: 836130.16199\n",
      "Avg Minibatch loss for epoch 109: 617941.47515\n",
      "Avg Minibatch loss for epoch 110: 1023903.97582\n",
      "  ***   Training RMSE: 6.41505 ***\n",
      "  *** Validation RMSE: 1163.38884 ***\n",
      "Avg Minibatch loss for epoch 111: 2391297.74872\n",
      "Avg Minibatch loss for epoch 112: 918728.80376\n",
      "Avg Minibatch loss for epoch 113: 437004.00889\n",
      "Avg Minibatch loss for epoch 114: 328219.99490\n",
      "Avg Minibatch loss for epoch 115: 526459.80032\n",
      "  ***   Training RMSE: 6.63056 ***\n",
      "  *** Validation RMSE: 1168.52830 ***\n",
      "Avg Minibatch loss for epoch 116: 1142040.55386\n",
      "Avg Minibatch loss for epoch 117: 1361580.09857\n",
      "Avg Minibatch loss for epoch 118: 1066916.45773\n",
      "Avg Minibatch loss for epoch 119: 1127224.86483\n",
      "Avg Minibatch loss for epoch 120: 888351.51967\n",
      "  ***   Training RMSE: 9.44978 ***\n",
      "  *** Validation RMSE: 1165.72177 ***\n",
      "Avg Minibatch loss for epoch 121: 672078.32959\n",
      "Avg Minibatch loss for epoch 122: 435970.65242\n",
      "Avg Minibatch loss for epoch 123: 551353.21960\n",
      "Avg Minibatch loss for epoch 124: 1212658.08759\n",
      "Avg Minibatch loss for epoch 125: 1712531.55489\n",
      "  ***   Training RMSE: 18.14305 ***\n",
      "  *** Validation RMSE: 1176.71933 ***\n",
      "Avg Minibatch loss for epoch 126: 1204248.39015\n",
      "Avg Minibatch loss for epoch 127: 792916.24132\n",
      "Avg Minibatch loss for epoch 128: 587670.60568\n",
      "Avg Minibatch loss for epoch 129: 696092.12297\n",
      "Avg Minibatch loss for epoch 130: 850554.88499\n",
      "  ***   Training RMSE: 9.75308 ***\n",
      "  *** Validation RMSE: 1177.09328 ***\n",
      "Avg Minibatch loss for epoch 131: 619985.20125\n",
      "Avg Minibatch loss for epoch 132: 555100.88421\n",
      "Avg Minibatch loss for epoch 133: 482701.59474\n",
      "Avg Minibatch loss for epoch 134: 398388.81770\n",
      "Avg Minibatch loss for epoch 135: 276723.81514\n",
      "  ***   Training RMSE: 3.72456 ***\n",
      "  *** Validation RMSE: 1157.58178 ***\n",
      "Avg Minibatch loss for epoch 136: 208431.50169\n",
      "Avg Minibatch loss for epoch 137: 230840.64540\n",
      "Avg Minibatch loss for epoch 138: 261572.72566\n",
      "Avg Minibatch loss for epoch 139: 423867.23401\n",
      "Avg Minibatch loss for epoch 140: 676968.20804\n",
      "  ***   Training RMSE: 14.65748 ***\n",
      "  *** Validation RMSE: 1195.27782 ***\n",
      "Avg Minibatch loss for epoch 141: 1033535.10082\n",
      "Avg Minibatch loss for epoch 142: 836269.32687\n",
      "Avg Minibatch loss for epoch 143: 469599.82115\n",
      "Avg Minibatch loss for epoch 144: 391713.11534\n",
      "Avg Minibatch loss for epoch 145: 284392.30433\n",
      "  ***   Training RMSE: 2.64626 ***\n",
      "  *** Validation RMSE: 1169.25548 ***\n",
      "Avg Minibatch loss for epoch 146: 209679.77500\n",
      "Avg Minibatch loss for epoch 147: 152609.45864\n",
      "Avg Minibatch loss for epoch 148: 200664.95283\n",
      "Avg Minibatch loss for epoch 149: 275440.60146\n",
      "Avg Minibatch loss for epoch 150: 325933.28421\n",
      "  ***   Training RMSE: 4.35573 ***\n",
      "  *** Validation RMSE: 1166.03926 ***\n",
      "Avg Minibatch loss for epoch 151: 240122.75741\n",
      "Avg Minibatch loss for epoch 152: 357382.26987\n",
      "Avg Minibatch loss for epoch 153: 248880.29319\n",
      "Avg Minibatch loss for epoch 154: 453567.41632\n",
      "Avg Minibatch loss for epoch 155: 335707.14888\n",
      "  ***   Training RMSE: 8.09108 ***\n",
      "  *** Validation RMSE: 1188.07609 ***\n",
      "Avg Minibatch loss for epoch 156: 209113.53986\n",
      "Avg Minibatch loss for epoch 157: 161054.24824\n",
      "Avg Minibatch loss for epoch 158: 165753.57582\n",
      "Avg Minibatch loss for epoch 159: 153652.46593\n",
      "Avg Minibatch loss for epoch 160: 165150.32202\n",
      "  ***   Training RMSE: 2.70094 ***\n",
      "  *** Validation RMSE: 1208.39439 ***\n",
      "Avg Minibatch loss for epoch 161: 193748.42875\n",
      "Avg Minibatch loss for epoch 162: 371075.98246\n",
      "Avg Minibatch loss for epoch 163: 301152.53753\n",
      "Avg Minibatch loss for epoch 164: 697579.95399\n",
      "Avg Minibatch loss for epoch 165: 2194988.00839\n",
      "  ***   Training RMSE: 8.74216 ***\n",
      "  *** Validation RMSE: 1215.77701 ***\n",
      "Avg Minibatch loss for epoch 166: 1640159.10488\n",
      "Avg Minibatch loss for epoch 167: 3075322.98141\n",
      "Avg Minibatch loss for epoch 168: 2207297.30017\n",
      "Avg Minibatch loss for epoch 169: 3153912.89052\n",
      "Avg Minibatch loss for epoch 170: 800661.02590\n",
      "  ***   Training RMSE: 3.89349 ***\n",
      "  *** Validation RMSE: 1181.04915 ***\n",
      "Avg Minibatch loss for epoch 171: 2643832.24974\n",
      "Avg Minibatch loss for epoch 172: 1132414.21451\n",
      "Avg Minibatch loss for epoch 173: 566111.83370\n",
      "Avg Minibatch loss for epoch 174: 221110.90377\n",
      "Avg Minibatch loss for epoch 175: 154617.56058\n",
      "  ***   Training RMSE: 1.60918 ***\n",
      "  *** Validation RMSE: 1160.92946 ***\n",
      "Avg Minibatch loss for epoch 176: 123064.69922\n",
      "Avg Minibatch loss for epoch 177: 114134.75719\n",
      "Avg Minibatch loss for epoch 178: 111804.09491\n",
      "Avg Minibatch loss for epoch 179: 111876.45385\n",
      "Avg Minibatch loss for epoch 180: 111298.08113\n",
      "  ***   Training RMSE: 1.70550 ***\n",
      "  *** Validation RMSE: 1158.29096 ***\n",
      "Avg Minibatch loss for epoch 181: 111480.43503\n",
      "Avg Minibatch loss for epoch 182: 112421.82075\n",
      "Avg Minibatch loss for epoch 183: 109293.87772\n",
      "Avg Minibatch loss for epoch 184: 110935.43914\n",
      "Avg Minibatch loss for epoch 185: 112828.98811\n",
      "  ***   Training RMSE: 1.65839 ***\n",
      "  *** Validation RMSE: 1158.92702 ***\n",
      "Avg Minibatch loss for epoch 186: 185610.32446\n",
      "Avg Minibatch loss for epoch 187: 285578.40522\n",
      "Avg Minibatch loss for epoch 188: 280930.91012\n",
      "Avg Minibatch loss for epoch 189: 477313.56062\n",
      "Avg Minibatch loss for epoch 190: 914700.98053\n",
      "  ***   Training RMSE: 9.90010 ***\n",
      "  *** Validation RMSE: 1276.96561 ***\n",
      "Avg Minibatch loss for epoch 191: 786095.61859\n",
      "Avg Minibatch loss for epoch 192: 3107177.02572\n",
      "Avg Minibatch loss for epoch 193: 928910.21967\n",
      "Avg Minibatch loss for epoch 194: 860333.87874\n",
      "Avg Minibatch loss for epoch 195: 1963122.38692\n",
      "  ***   Training RMSE: 6.91041 ***\n",
      "  *** Validation RMSE: 1165.91971 ***\n",
      "Avg Minibatch loss for epoch 196: 635091.31552\n",
      "Avg Minibatch loss for epoch 197: 2187063.95205\n",
      "Avg Minibatch loss for epoch 198: 298761.46391\n",
      "Avg Minibatch loss for epoch 199: 1895890.18921\n",
      "Avg Minibatch loss for epoch 200: 177090.05492\n",
      "  ***   Training RMSE: 1.36645 ***\n",
      "  *** Validation RMSE: 1170.16748 ***\n",
      "Avg Minibatch loss for epoch 201: 157375.06300\n",
      "Avg Minibatch loss for epoch 202: 151191.88660\n",
      "Avg Minibatch loss for epoch 203: 140370.44095\n",
      "Avg Minibatch loss for epoch 204: 123697.22265\n",
      "Avg Minibatch loss for epoch 205: 114530.14192\n",
      "  ***   Training RMSE: 1.35155 ***\n",
      "  *** Validation RMSE: 1159.06891 ***\n",
      "Avg Minibatch loss for epoch 206: 114730.05995\n",
      "Avg Minibatch loss for epoch 207: 112025.55229\n",
      "Avg Minibatch loss for epoch 208: 129317.56810\n",
      "Avg Minibatch loss for epoch 209: 150809.58844\n",
      "Avg Minibatch loss for epoch 210: 155617.57961\n",
      "  ***   Training RMSE: 1.32369 ***\n",
      "  *** Validation RMSE: 1166.47157 ***\n",
      "Avg Minibatch loss for epoch 211: 174126.46584\n",
      "Avg Minibatch loss for epoch 212: 168051.15987\n",
      "Avg Minibatch loss for epoch 213: 136371.80011\n",
      "Avg Minibatch loss for epoch 214: 126146.97281\n",
      "Avg Minibatch loss for epoch 215: 110914.99194\n",
      "  ***   Training RMSE: 1.25799 ***\n",
      "  *** Validation RMSE: 1160.03534 ***\n",
      "Avg Minibatch loss for epoch 216: 107424.70719\n",
      "Avg Minibatch loss for epoch 217: 182309.73114\n",
      "Avg Minibatch loss for epoch 218: 122657.32833\n",
      "Avg Minibatch loss for epoch 219: 146393.90884\n",
      "Avg Minibatch loss for epoch 220: 159296.77831\n",
      "  ***   Training RMSE: 2.24703 ***\n",
      "  *** Validation RMSE: 1179.75454 ***\n",
      "Avg Minibatch loss for epoch 221: 200582.63923\n",
      "Avg Minibatch loss for epoch 222: 170495.84617\n",
      "Avg Minibatch loss for epoch 223: 211760.46899\n",
      "Avg Minibatch loss for epoch 224: 121558.40304\n",
      "Avg Minibatch loss for epoch 225: 117476.41197\n",
      "  ***   Training RMSE: 1.18687 ***\n",
      "  *** Validation RMSE: 1164.77254 ***\n",
      "Avg Minibatch loss for epoch 226: 124647.94459\n",
      "Avg Minibatch loss for epoch 227: 116347.82366\n",
      "Avg Minibatch loss for epoch 228: 106937.83972\n",
      "Avg Minibatch loss for epoch 229: 106260.71305\n",
      "Avg Minibatch loss for epoch 230: 104170.87796\n",
      "  ***   Training RMSE: 1.14073 ***\n",
      "  *** Validation RMSE: 1161.62822 ***\n",
      "Avg Minibatch loss for epoch 231: 105509.46543\n",
      "Avg Minibatch loss for epoch 232: 113980.83324\n",
      "Avg Minibatch loss for epoch 233: 110133.01080\n",
      "Avg Minibatch loss for epoch 234: 109470.58430\n",
      "Avg Minibatch loss for epoch 235: 185398.00711\n",
      "  ***   Training RMSE: 1.96792 ***\n",
      "  *** Validation RMSE: 1191.07514 ***\n",
      "Avg Minibatch loss for epoch 236: 54235601.16688\n",
      "Avg Minibatch loss for epoch 237: 69058756.14214\n",
      "Avg Minibatch loss for epoch 238: 26374927.93307\n",
      "Avg Minibatch loss for epoch 239: 27031392.44050\n",
      "Avg Minibatch loss for epoch 240: 4281488.09789\n",
      "  ***   Training RMSE: 25.62082 ***\n",
      "  *** Validation RMSE: 1183.00975 ***\n",
      "Avg Minibatch loss for epoch 241: 2120211.76448\n",
      "Avg Minibatch loss for epoch 242: 1418946.05536\n",
      "Avg Minibatch loss for epoch 243: 1112842.06565\n",
      "Avg Minibatch loss for epoch 244: 1409885.57024\n",
      "Avg Minibatch loss for epoch 245: 1588539.19248\n",
      "  ***   Training RMSE: 12.46615 ***\n",
      "  *** Validation RMSE: 1165.80084 ***\n",
      "Avg Minibatch loss for epoch 246: 3075203.30494\n",
      "Avg Minibatch loss for epoch 247: 2091560.83547\n",
      "Avg Minibatch loss for epoch 248: 1219609.18250\n",
      "Avg Minibatch loss for epoch 249: 1580109.23429\n",
      "Avg Minibatch loss for epoch 250: 1499597.37995\n",
      "  ***   Training RMSE: 18.78168 ***\n",
      "  *** Validation RMSE: 1162.03886 ***\n",
      "Avg Minibatch loss for epoch 251: 1666915.37011\n",
      "Avg Minibatch loss for epoch 252: 2233667.85473\n",
      "Avg Minibatch loss for epoch 253: 1063074.05807\n",
      "Avg Minibatch loss for epoch 254: 1022480.45151\n",
      "Avg Minibatch loss for epoch 255: 1834459.36147\n",
      "  ***   Training RMSE: 11.29558 ***\n",
      "  *** Validation RMSE: 1179.82386 ***\n",
      "Avg Minibatch loss for epoch 256: 2487883.54882\n",
      "Avg Minibatch loss for epoch 257: 3736014.87408\n",
      "Avg Minibatch loss for epoch 258: 3072035.89358\n",
      "Avg Minibatch loss for epoch 259: 2660289.70629\n",
      "Avg Minibatch loss for epoch 260: 1049127.92105\n",
      "  ***   Training RMSE: 15.82220 ***\n",
      "  *** Validation RMSE: 1164.82291 ***\n",
      "Avg Minibatch loss for epoch 261: 597969.47670\n",
      "Avg Minibatch loss for epoch 262: 609221.71313\n",
      "Avg Minibatch loss for epoch 263: 606488.16249\n",
      "Avg Minibatch loss for epoch 264: 370394.47426\n",
      "Avg Minibatch loss for epoch 265: 351134.08923\n",
      "  ***   Training RMSE: 7.41097 ***\n",
      "  *** Validation RMSE: 1165.75995 ***\n",
      "Avg Minibatch loss for epoch 266: 288683.18534\n",
      "Avg Minibatch loss for epoch 267: 394288.31101\n",
      "Avg Minibatch loss for epoch 268: 710293.36774\n",
      "Avg Minibatch loss for epoch 269: 887219.60941\n",
      "Avg Minibatch loss for epoch 270: 1638374.78198\n",
      "  ***   Training RMSE: 9.13062 ***\n",
      "  *** Validation RMSE: 1202.53304 ***\n",
      "Avg Minibatch loss for epoch 271: 979117.81755\n",
      "Avg Minibatch loss for epoch 272: 640103.67725\n",
      "Avg Minibatch loss for epoch 273: 741252.38163\n",
      "Avg Minibatch loss for epoch 274: 600014.02177\n",
      "Avg Minibatch loss for epoch 275: 492845.23920\n",
      "  ***   Training RMSE: 9.54501 ***\n",
      "  *** Validation RMSE: 1184.93379 ***\n",
      "Avg Minibatch loss for epoch 276: 568838.43445\n",
      "Avg Minibatch loss for epoch 277: 756191.71077\n",
      "Avg Minibatch loss for epoch 278: 687204.44570\n",
      "Avg Minibatch loss for epoch 279: 738623.08124\n",
      "Avg Minibatch loss for epoch 280: 402770.27237\n",
      "  ***   Training RMSE: 4.86190 ***\n",
      "  *** Validation RMSE: 1193.97319 ***\n",
      "Avg Minibatch loss for epoch 281: 301524.50731\n",
      "Avg Minibatch loss for epoch 282: 259096.32480\n",
      "Avg Minibatch loss for epoch 283: 287952.34942\n",
      "Avg Minibatch loss for epoch 284: 296198.45444\n",
      "Avg Minibatch loss for epoch 285: 445892.81622\n",
      "  ***   Training RMSE: 8.15468 ***\n",
      "  *** Validation RMSE: 1175.05050 ***\n",
      "Avg Minibatch loss for epoch 286: 439015.40652\n",
      "Avg Minibatch loss for epoch 287: 552609.02563\n",
      "Avg Minibatch loss for epoch 288: 645165.80842\n",
      "Avg Minibatch loss for epoch 289: 677937.04330\n",
      "Avg Minibatch loss for epoch 290: 1589240.58455\n",
      "  ***   Training RMSE: 2.41671 ***\n",
      "  *** Validation RMSE: 1178.19117 ***\n",
      "Avg Minibatch loss for epoch 291: 1055461.45975\n",
      "Avg Minibatch loss for epoch 292: 1377944.70316\n",
      "Avg Minibatch loss for epoch 293: 1273718.00296\n",
      "Avg Minibatch loss for epoch 294: 986774.56577\n",
      "Avg Minibatch loss for epoch 295: 403644.13473\n",
      "  ***   Training RMSE: 2.61083 ***\n",
      "  *** Validation RMSE: 1167.25305 ***\n",
      "Avg Minibatch loss for epoch 296: 297112.90686\n",
      "Avg Minibatch loss for epoch 297: 179896.71514\n",
      "Avg Minibatch loss for epoch 298: 140706.98814\n",
      "Avg Minibatch loss for epoch 299: 131908.85470\n",
      "\n",
      "Final Training set RMSE: 2.08750\n",
      "Final Validation set RMSE: 1157.21175\n",
      "Test set RMSE: 857.11075\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "h0 = 75\n",
    "h1 = len(users)\n",
    "lambda_ = 0\n",
    "lr = .005\n",
    "keep_prob = .75\n",
    "\n",
    "num_epochs = 300\n",
    "\n",
    "train_dataset = train.transpose().toarray().astype(np.float32)\n",
    "test_dataset = test.transpose().toarray().astype(np.float32)\n",
    "valid_dataset = val.transpose().toarray().astype(np.float32)\n",
    "\n",
    "means = np.mean(train_dataset, axis=0)\n",
    "stds = np.std(train_dataset, axis=0)\n",
    "stds[stds == 0] = 1\n",
    "train_dataset = (train_dataset - means) / stds\n",
    "valid_dataset = (valid_dataset - means) / stds\n",
    "test_dataset = (test_dataset - means) /stds\n",
    "\n",
    "train_labels = train_dataset\n",
    "valid_labels = valid_dataset\n",
    "test_labels = test_dataset\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                      shape=(batch_size, len(users)))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, len(users)))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    tf_tr_dataset = tf.constant(train_dataset)\n",
    "\n",
    "    # Variables.\n",
    "    w0 = tf.Variable(tf.truncated_normal([len(users), h0]))\n",
    "    b0 = tf.Variable(tf.zeros([h0]))\n",
    "\n",
    "    w1 = tf.Variable(tf.truncated_normal([h0, h1]))\n",
    "    b1 = tf.Variable(tf.zeros([h1]))\n",
    "    \n",
    "    # Training computation.\n",
    "    s0 = tf.nn.dropout(tf.nn.relu(tf.matmul(tf_train_dataset, w0) + b0), keep_prob)\n",
    "    logits = tf.matmul(s0, w1) + b1\n",
    "\n",
    "    reg = tf.nn.l2_loss(w0) + tf.nn.l2_loss(w1)\n",
    "    # loss = tf.reduce_sum((logits - tf_train_dataset) ** 2)+ (lambda_ * reg)\n",
    "    loss = tf.nn.l2_loss(logits - tf_train_dataset) + (lambda_ * reg)\n",
    "\n",
    "    # Optimizer.\n",
    "    # optimizer = tf.train.GradientDescentOptimizer(lr).minimize(loss)\n",
    "    optimizer = tf.train.AdamOptimizer(lr).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = logits\n",
    "\n",
    "    v0 = tf.nn.relu(tf.matmul(tf_valid_dataset, w0) + b0)\n",
    "    valid_prediction = tf.matmul(v0, w1) + b1\n",
    " \n",
    "\n",
    "    t0 = tf.nn.relu(tf.matmul(tf_test_dataset, w0) + b0)\n",
    "    test_prediction = tf.matmul(t0, w1) + b1\n",
    "    \n",
    "    r0 = tf.nn.relu(tf.matmul(tf_tr_dataset, w0) + b0)\n",
    "    tr_prediction = tf.matmul(r0, w1 + b1)\n",
    "\n",
    "batches = np.ceil(float(len(items)) / batch_size)\n",
    "num_steps = int(np.ceil(float(len(items)) / batch_size))\n",
    "# curves = []\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for epoch in range(num_epochs):\n",
    "        l_mean = 0\n",
    "        for step in range(num_steps):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            # Note: we could use better randomization across epochs.\n",
    "            # offset = (step * batch_size) % (int(batch_size * 2.5) - batch_size)\n",
    "            offset = (step * batch_size) % (len(items) - batch_size)\n",
    "            # Generate a minibatch.\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            # and the value is the numpy array to feed to it.\n",
    "            feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n",
    "\n",
    "            _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "            l_mean += l\n",
    "\n",
    "        print(\"Avg Minibatch loss for epoch %d: %.5f\" % (epoch, l_mean / num_steps))\n",
    "        # print(\" Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "        if (epoch % 5 == 0):\n",
    "            tra = accuracy(tr_prediction.eval(), train_labels)\n",
    "            va = accuracy(valid_prediction.eval(), valid_labels)\n",
    "            print(\"  ***   Training RMSE: %.5f ***\" % tra)\n",
    "            print(\"  *** Validation RMSE: %.5f ***\" % va)\n",
    "            # curves.append({'epoch': epoch, 'train': 1 - tra, 'val': 1- va})\n",
    "    print(\"\\nFinal Training set RMSE: %.5f\" % accuracy(tr_prediction.eval(), train_labels))\n",
    "    print(\"Final Validation set RMSE: %.5f\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test set RMSE: %.5f\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Avg Minibatch loss for epoch 0: 12364229823.35811\n",
      "  ***   Training RMSE: 144.40668 ***\n",
      "  *** Validation RMSE: 7327.30626 ***\n",
      "Avg Minibatch loss for epoch 1: 11215039529.15541\n",
      "Avg Minibatch loss for epoch 2: 10137079170.79730\n",
      "Avg Minibatch loss for epoch 3: 9534660136.63514\n",
      "Avg Minibatch loss for epoch 4: 8705885624.54730\n",
      "Avg Minibatch loss for epoch 5: 8217296850.04054\n",
      "  ***   Training RMSE: 121.07419 ***\n",
      "  *** Validation RMSE: 5946.68400 ***\n",
      "Avg Minibatch loss for epoch 6: 7763024707.12162\n",
      "Avg Minibatch loss for epoch 7: 7304332218.95946\n",
      "Avg Minibatch loss for epoch 8: 6873519303.97297\n",
      "Avg Minibatch loss for epoch 9: 6586906173.74324\n",
      "Avg Minibatch loss for epoch 10: 6308297295.28378\n",
      "  ***   Training RMSE: 138.55885 ***\n",
      "  *** Validation RMSE: 4954.58381 ***\n",
      "Avg Minibatch loss for epoch 11: 5950282645.79730\n",
      "Avg Minibatch loss for epoch 12: 5723468347.31081\n",
      "Avg Minibatch loss for epoch 13: 5447277902.97297\n",
      "Avg Minibatch loss for epoch 14: 5239917887.24324\n",
      "Avg Minibatch loss for epoch 15: 5030547829.02703\n",
      "  ***   Training RMSE: 164.33389 ***\n",
      "  *** Validation RMSE: 4152.12618 ***\n",
      "Avg Minibatch loss for epoch 16: 4816321084.00000\n",
      "Avg Minibatch loss for epoch 17: 4586289715.37838\n",
      "Avg Minibatch loss for epoch 18: 4489675249.94595\n",
      "Avg Minibatch loss for epoch 19: 4383165370.40541\n",
      "Avg Minibatch loss for epoch 20: 4197297467.75676\n",
      "  ***   Training RMSE: 188.24631 ***\n",
      "  *** Validation RMSE: 3462.79393 ***\n",
      "Avg Minibatch loss for epoch 21: 4005955295.27027\n",
      "Avg Minibatch loss for epoch 22: 3950533234.62162\n",
      "Avg Minibatch loss for epoch 23: 3763512492.54054\n",
      "Avg Minibatch loss for epoch 24: 3682714706.43243\n",
      "Avg Minibatch loss for epoch 25: 3528673236.62162\n",
      "  ***   Training RMSE: 206.65386 ***\n",
      "  *** Validation RMSE: 2853.14514 ***\n",
      "Avg Minibatch loss for epoch 26: 3429567396.89189\n",
      "Avg Minibatch loss for epoch 27: 3292635949.97297\n",
      "Avg Minibatch loss for epoch 28: 3251203013.86486\n",
      "Avg Minibatch loss for epoch 29: 3158178406.27027\n",
      "Avg Minibatch loss for epoch 30: 3052215111.29730\n",
      "  ***   Training RMSE: 218.52979 ***\n",
      "  *** Validation RMSE: 2310.86190 ***\n",
      "Avg Minibatch loss for epoch 31: 2934440294.91892\n",
      "Avg Minibatch loss for epoch 32: 2874173986.86486\n",
      "Avg Minibatch loss for epoch 33: 2789807611.18919\n",
      "Avg Minibatch loss for epoch 34: 2723367094.40541\n",
      "Avg Minibatch loss for epoch 35: 2642239327.08108\n",
      "  ***   Training RMSE: 224.31957 ***\n",
      "  *** Validation RMSE: 1831.44977 ***\n",
      "Avg Minibatch loss for epoch 36: 2545978220.10811\n",
      "Avg Minibatch loss for epoch 37: 2500766794.02703\n",
      "Avg Minibatch loss for epoch 38: 2428667189.81081\n",
      "Avg Minibatch loss for epoch 39: 2357917849.56757\n",
      "Avg Minibatch loss for epoch 40: 2291256543.13514\n",
      "  ***   Training RMSE: 225.48616 ***\n",
      "  *** Validation RMSE: 1418.31858 ***\n",
      "Avg Minibatch loss for epoch 41: 2212439903.56757\n",
      "Avg Minibatch loss for epoch 42: 2160506658.89189\n",
      "Avg Minibatch loss for epoch 43: 2096406440.05405\n",
      "Avg Minibatch loss for epoch 44: 2023443701.24324\n",
      "Avg Minibatch loss for epoch 45: 1971648118.24324\n",
      "  ***   Training RMSE: 223.65120 ***\n",
      "  *** Validation RMSE: 1069.00406 ***\n",
      "Avg Minibatch loss for epoch 46: 1935133833.91892\n",
      "Avg Minibatch loss for epoch 47: 1859302035.02703\n",
      "Avg Minibatch loss for epoch 48: 1805344189.51351\n",
      "Avg Minibatch loss for epoch 49: 1750086616.54054\n",
      "Avg Minibatch loss for epoch 50: 1708734079.29730\n",
      "  ***   Training RMSE: 219.90116 ***\n",
      "  *** Validation RMSE: 777.82487 ***\n",
      "Avg Minibatch loss for epoch 51: 1650546381.29730\n",
      "Avg Minibatch loss for epoch 52: 1608500031.32432\n",
      "Avg Minibatch loss for epoch 53: 1562195563.62162\n",
      "Avg Minibatch loss for epoch 54: 1503426594.64865\n",
      "Avg Minibatch loss for epoch 55: 1477519838.81081\n",
      "  ***   Training RMSE: 215.41074 ***\n",
      "  *** Validation RMSE: 540.00914 ***\n",
      "Avg Minibatch loss for epoch 56: 1428783975.05405\n",
      "Avg Minibatch loss for epoch 57: 1391701970.21622\n",
      "Avg Minibatch loss for epoch 58: 1333135976.00000\n",
      "Avg Minibatch loss for epoch 59: 1295252516.00000\n",
      "Avg Minibatch loss for epoch 60: 1269901256.45946\n",
      "  ***   Training RMSE: 210.52278 ***\n",
      "  *** Validation RMSE: 356.69009 ***\n",
      "Avg Minibatch loss for epoch 61: 1214289882.56757\n",
      "Avg Minibatch loss for epoch 62: 1181833942.08108\n",
      "Avg Minibatch loss for epoch 63: 1157658479.32432\n",
      "Avg Minibatch loss for epoch 64: 1116970414.28378\n",
      "Avg Minibatch loss for epoch 65: 1083704224.78378\n",
      "  ***   Training RMSE: 205.75955 ***\n",
      "  *** Validation RMSE: 225.02412 ***\n",
      "Avg Minibatch loss for epoch 66: 1040891907.31081\n",
      "Avg Minibatch loss for epoch 67: 1021244983.22973\n",
      "Avg Minibatch loss for epoch 68: 976385221.27027\n",
      "Avg Minibatch loss for epoch 69: 953876084.29730\n",
      "Avg Minibatch loss for epoch 70: 921637280.59459\n",
      "  ***   Training RMSE: 201.41917 ***\n",
      "  *** Validation RMSE: 145.19674 ***\n",
      "Avg Minibatch loss for epoch 71: 894425021.04054\n",
      "Avg Minibatch loss for epoch 72: 861172933.71622\n",
      "Avg Minibatch loss for epoch 73: 835923455.18919\n",
      "Avg Minibatch loss for epoch 74: 809276082.02703\n",
      "Avg Minibatch loss for epoch 75: 787788910.51351\n",
      "  ***   Training RMSE: 197.39333 ***\n",
      "  *** Validation RMSE: 115.78876 ***\n",
      "Avg Minibatch loss for epoch 76: 755737998.62162\n",
      "Avg Minibatch loss for epoch 77: 736801796.39189\n",
      "Avg Minibatch loss for epoch 78: 712050926.78378\n",
      "Avg Minibatch loss for epoch 79: 684272925.87838\n",
      "Avg Minibatch loss for epoch 80: 665590551.16216\n",
      "  ***   Training RMSE: 193.69600 ***\n",
      "  *** Validation RMSE: 108.13503 ***\n",
      "Avg Minibatch loss for epoch 81: 640154452.09459\n",
      "Avg Minibatch loss for epoch 82: 621069376.32432\n",
      "Avg Minibatch loss for epoch 83: 606968238.75676\n",
      "Avg Minibatch loss for epoch 84: 578851547.16216\n",
      "Avg Minibatch loss for epoch 85: 568352228.28378\n",
      "  ***   Training RMSE: 190.23303 ***\n",
      "  *** Validation RMSE: 102.82941 ***\n",
      "Avg Minibatch loss for epoch 86: 540772279.01351\n",
      "Avg Minibatch loss for epoch 87: 529697301.33784\n",
      "Avg Minibatch loss for epoch 88: 507327192.08108\n",
      "Avg Minibatch loss for epoch 89: 488521836.64189\n",
      "Avg Minibatch loss for epoch 90: 469092118.68243\n",
      "  ***   Training RMSE: 186.86791 ***\n",
      "  *** Validation RMSE: 97.28992 ***\n",
      "Avg Minibatch loss for epoch 91: 454556570.12162\n",
      "Avg Minibatch loss for epoch 92: 439154248.54054\n",
      "Avg Minibatch loss for epoch 93: 426792870.89865\n",
      "Avg Minibatch loss for epoch 94: 410800897.43243\n",
      "Avg Minibatch loss for epoch 95: 397671603.00676\n",
      "  ***   Training RMSE: 183.78767 ***\n",
      "  *** Validation RMSE: 91.66562 ***\n",
      "Avg Minibatch loss for epoch 96: 382035280.41892\n",
      "Avg Minibatch loss for epoch 97: 367707289.68243\n",
      "Avg Minibatch loss for epoch 98: 356301864.87162\n",
      "Avg Minibatch loss for epoch 99: 339651237.56081\n",
      "Avg Minibatch loss for epoch 100: 329254369.16216\n",
      "  ***   Training RMSE: 180.97532 ***\n",
      "  *** Validation RMSE: 86.03953 ***\n",
      "Avg Minibatch loss for epoch 101: 317333107.19595\n",
      "Avg Minibatch loss for epoch 102: 305852800.29054\n",
      "Avg Minibatch loss for epoch 103: 294030060.34459\n",
      "Avg Minibatch loss for epoch 104: 283206179.03378\n",
      "Avg Minibatch loss for epoch 105: 274249222.08108\n",
      "  ***   Training RMSE: 178.13690 ***\n",
      "  *** Validation RMSE: 80.65372 ***\n",
      "Avg Minibatch loss for epoch 106: 262146474.84459\n",
      "Avg Minibatch loss for epoch 107: 249892455.78378\n",
      "Avg Minibatch loss for epoch 108: 243805306.32095\n",
      "Avg Minibatch loss for epoch 109: 233819077.66554\n",
      "Avg Minibatch loss for epoch 110: 224061474.35473\n",
      "  ***   Training RMSE: 175.52413 ***\n",
      "  *** Validation RMSE: 75.61941 ***\n",
      "Avg Minibatch loss for epoch 111: 215979023.39527\n",
      "Avg Minibatch loss for epoch 112: 205549373.24662\n",
      "Avg Minibatch loss for epoch 113: 199691506.43581\n",
      "Avg Minibatch loss for epoch 114: 191105779.39189\n",
      "Avg Minibatch loss for epoch 115: 183322347.60473\n",
      "  ***   Training RMSE: 172.60630 ***\n",
      "  *** Validation RMSE: 70.54449 ***\n",
      "Avg Minibatch loss for epoch 116: 175554647.36824\n",
      "Avg Minibatch loss for epoch 117: 167871177.78716\n",
      "Avg Minibatch loss for epoch 118: 160290054.55405\n",
      "Avg Minibatch loss for epoch 119: 154382964.77027\n",
      "Avg Minibatch loss for epoch 120: 147277528.70608\n",
      "  ***   Training RMSE: 169.67860 ***\n",
      "  *** Validation RMSE: 65.80563 ***\n",
      "Avg Minibatch loss for epoch 121: 142149374.00338\n",
      "Avg Minibatch loss for epoch 122: 134357767.68919\n",
      "Avg Minibatch loss for epoch 123: 128702711.93243\n",
      "Avg Minibatch loss for epoch 124: 123092450.29392\n",
      "Avg Minibatch loss for epoch 125: 118758747.85980\n",
      "  ***   Training RMSE: 166.75624 ***\n",
      "  *** Validation RMSE: 61.05076 ***\n",
      "Avg Minibatch loss for epoch 126: 112567740.77872\n",
      "Avg Minibatch loss for epoch 127: 107641336.34966\n",
      "Avg Minibatch loss for epoch 128: 102244535.19088\n",
      "Avg Minibatch loss for epoch 129: 97018339.04223\n",
      "Avg Minibatch loss for epoch 130: 92547850.10811\n",
      "  ***   Training RMSE: 164.01623 ***\n",
      "  *** Validation RMSE: 56.65576 ***\n",
      "Avg Minibatch loss for epoch 131: 88179368.88851\n",
      "Avg Minibatch loss for epoch 132: 84374913.90709\n",
      "Avg Minibatch loss for epoch 133: 80152595.82939\n",
      "Avg Minibatch loss for epoch 134: 75706121.35642\n",
      "Avg Minibatch loss for epoch 135: 72145615.76182\n",
      "  ***   Training RMSE: 161.00188 ***\n",
      "  *** Validation RMSE: 52.44689 ***\n",
      "Avg Minibatch loss for epoch 136: 68586753.28209\n",
      "Avg Minibatch loss for epoch 137: 64982968.34966\n",
      "Avg Minibatch loss for epoch 138: 61727701.56081\n",
      "Avg Minibatch loss for epoch 139: 58547955.01351\n",
      "Avg Minibatch loss for epoch 140: 55057935.65878\n",
      "  ***   Training RMSE: 157.82897 ***\n",
      "  *** Validation RMSE: 48.46335 ***\n",
      "Avg Minibatch loss for epoch 141: 52221559.26014\n",
      "Avg Minibatch loss for epoch 142: 49616025.62922\n",
      "Avg Minibatch loss for epoch 143: 46790546.27872\n",
      "Avg Minibatch loss for epoch 144: 43968596.39274\n",
      "Avg Minibatch loss for epoch 145: 41726425.14105\n",
      "  ***   Training RMSE: 154.49284 ***\n",
      "  *** Validation RMSE: 44.84290 ***\n",
      "Avg Minibatch loss for epoch 146: 39079064.46368\n",
      "Avg Minibatch loss for epoch 147: 36804069.27872\n",
      "Avg Minibatch loss for epoch 148: 34815334.70439\n",
      "Avg Minibatch loss for epoch 149: 32781179.39105\n",
      "Avg Minibatch loss for epoch 150: 30727745.85220\n",
      "  ***   Training RMSE: 150.84804 ***\n",
      "  *** Validation RMSE: 41.46501 ***\n",
      "Avg Minibatch loss for epoch 151: 28756661.78632\n",
      "Avg Minibatch loss for epoch 152: 27017633.17061\n",
      "Avg Minibatch loss for epoch 153: 25338978.39274\n",
      "Avg Minibatch loss for epoch 154: 23491561.90836\n",
      "Avg Minibatch loss for epoch 155: 21801671.34502\n",
      "  ***   Training RMSE: 146.42889 ***\n",
      "  *** Validation RMSE: 38.48649 ***\n",
      "Avg Minibatch loss for epoch 156: 20677632.96875\n",
      "Avg Minibatch loss for epoch 157: 19320242.34122\n",
      "Avg Minibatch loss for epoch 158: 17926216.66596\n",
      "Avg Minibatch loss for epoch 159: 16673500.73860\n",
      "Avg Minibatch loss for epoch 160: 15560716.30870\n",
      "  ***   Training RMSE: 141.52873 ***\n",
      "  *** Validation RMSE: 35.87751 ***\n",
      "Avg Minibatch loss for epoch 161: 14294313.50380\n",
      "Avg Minibatch loss for epoch 162: 13419737.37774\n",
      "Avg Minibatch loss for epoch 163: 12396383.08784\n",
      "Avg Minibatch loss for epoch 164: 11387222.71706\n",
      "Avg Minibatch loss for epoch 165: 10550350.91660\n",
      "  ***   Training RMSE: 135.92740 ***\n",
      "  *** Validation RMSE: 33.54040 ***\n",
      "Avg Minibatch loss for epoch 166: 9703127.98247\n",
      "Avg Minibatch loss for epoch 167: 8911490.58699\n",
      "Avg Minibatch loss for epoch 168: 8192679.90076\n",
      "Avg Minibatch loss for epoch 169: 7565356.52703\n",
      "Avg Minibatch loss for epoch 170: 6922270.88355\n",
      "  ***   Training RMSE: 129.40779 ***\n",
      "  *** Validation RMSE: 31.72029 ***\n",
      "Avg Minibatch loss for epoch 171: 6317895.04286\n",
      "Avg Minibatch loss for epoch 172: 5759994.65340\n",
      "Avg Minibatch loss for epoch 173: 5229041.79614\n",
      "Avg Minibatch loss for epoch 174: 4778814.72371\n",
      "Avg Minibatch loss for epoch 175: 4378235.27365\n",
      "  ***   Training RMSE: 121.82544 ***\n",
      "  *** Validation RMSE: 30.25675 ***\n",
      "Avg Minibatch loss for epoch 176: 3952140.15804\n",
      "Avg Minibatch loss for epoch 177: 3551241.86550\n",
      "Avg Minibatch loss for epoch 178: 3234346.19616\n",
      "Avg Minibatch loss for epoch 179: 2914769.49630\n",
      "Avg Minibatch loss for epoch 180: 2659166.64928\n",
      "  ***   Training RMSE: 112.95603 ***\n",
      "  *** Validation RMSE: 29.23020 ***\n",
      "Avg Minibatch loss for epoch 181: 2380609.81926\n",
      "Avg Minibatch loss for epoch 182: 2153329.17277\n",
      "Avg Minibatch loss for epoch 183: 1932048.57132\n",
      "Avg Minibatch loss for epoch 184: 1730915.47292\n",
      "Avg Minibatch loss for epoch 185: 1563837.05033\n",
      "  ***   Training RMSE: 102.86263 ***\n",
      "  *** Validation RMSE: 28.46449 ***\n",
      "Avg Minibatch loss for epoch 186: 1386283.70138\n",
      "Avg Minibatch loss for epoch 187: 1230404.82493\n",
      "Avg Minibatch loss for epoch 188: 1112463.84623\n",
      "Avg Minibatch loss for epoch 189: 997948.61706\n",
      "Avg Minibatch loss for epoch 190: 894267.12860\n",
      "  ***   Training RMSE: 91.56046 ***\n",
      "  *** Validation RMSE: 27.98329 ***\n",
      "Avg Minibatch loss for epoch 191: 796564.92936\n",
      "Avg Minibatch loss for epoch 192: 714431.19653\n",
      "Avg Minibatch loss for epoch 193: 626313.64072\n",
      "Avg Minibatch loss for epoch 194: 570451.81034\n",
      "Avg Minibatch loss for epoch 195: 510752.69832\n",
      "  ***   Training RMSE: 79.73652 ***\n",
      "  *** Validation RMSE: 27.69410 ***\n",
      "Avg Minibatch loss for epoch 196: 447278.29203\n",
      "Avg Minibatch loss for epoch 197: 407255.06573\n",
      "Avg Minibatch loss for epoch 198: 365818.60447\n",
      "Avg Minibatch loss for epoch 199: 333272.79223\n",
      "Avg Minibatch loss for epoch 200: 303606.63723\n",
      "  ***   Training RMSE: 67.71716 ***\n",
      "  *** Validation RMSE: 27.53762 ***\n",
      "Avg Minibatch loss for epoch 201: 273271.78417\n",
      "Avg Minibatch loss for epoch 202: 247635.60929\n",
      "Avg Minibatch loss for epoch 203: 227986.86758\n",
      "Avg Minibatch loss for epoch 204: 208624.21629\n",
      "Avg Minibatch loss for epoch 205: 193266.94770\n",
      "  ***   Training RMSE: 56.33243 ***\n",
      "  *** Validation RMSE: 27.45303 ***\n",
      "Avg Minibatch loss for epoch 206: 178237.27133\n",
      "Avg Minibatch loss for epoch 207: 167512.95120\n",
      "Avg Minibatch loss for epoch 208: 156309.08088\n",
      "Avg Minibatch loss for epoch 209: 146704.77317\n",
      "Avg Minibatch loss for epoch 210: 139552.55859\n",
      "  ***   Training RMSE: 46.07697 ***\n",
      "  *** Validation RMSE: 27.41109 ***\n",
      "Avg Minibatch loss for epoch 211: 133074.61903\n",
      "Avg Minibatch loss for epoch 212: 127106.22456\n",
      "Avg Minibatch loss for epoch 213: 122077.13907\n",
      "Avg Minibatch loss for epoch 214: 118209.15908\n",
      "Avg Minibatch loss for epoch 215: 114116.69171\n",
      "  ***   Training RMSE: 37.04484 ***\n",
      "  *** Validation RMSE: 27.39459 ***\n",
      "Avg Minibatch loss for epoch 216: 111927.19617\n",
      "Avg Minibatch loss for epoch 217: 108454.41001\n",
      "Avg Minibatch loss for epoch 218: 106149.60844\n",
      "Avg Minibatch loss for epoch 219: 103399.64310\n",
      "Avg Minibatch loss for epoch 220: 102023.74413\n",
      "  ***   Training RMSE: 29.44547 ***\n",
      "  *** Validation RMSE: 27.38620 ***\n",
      "Avg Minibatch loss for epoch 221: 101516.98314\n",
      "Avg Minibatch loss for epoch 222: 99967.13286\n",
      "Avg Minibatch loss for epoch 223: 98072.91839\n",
      "Avg Minibatch loss for epoch 224: 97717.68899\n",
      "Avg Minibatch loss for epoch 225: 96427.64730\n",
      "  ***   Training RMSE: 23.43860 ***\n",
      "  *** Validation RMSE: 27.38263 ***\n",
      "Avg Minibatch loss for epoch 226: 95547.95294\n",
      "Avg Minibatch loss for epoch 227: 95296.57124\n",
      "Avg Minibatch loss for epoch 228: 94922.67318\n",
      "Avg Minibatch loss for epoch 229: 93922.40238\n",
      "Avg Minibatch loss for epoch 230: 93551.71726\n",
      "  ***   Training RMSE: 19.10967 ***\n",
      "  *** Validation RMSE: 27.38100 ***\n",
      "Avg Minibatch loss for epoch 231: 92623.08668\n",
      "Avg Minibatch loss for epoch 232: 93447.19696\n",
      "Avg Minibatch loss for epoch 233: 92260.48711\n",
      "Avg Minibatch loss for epoch 234: 92152.92584\n",
      "Avg Minibatch loss for epoch 235: 92220.23552\n",
      "  ***   Training RMSE: 15.93726 ***\n",
      "  *** Validation RMSE: 27.38101 ***\n",
      "Avg Minibatch loss for epoch 236: 91647.13486\n",
      "Avg Minibatch loss for epoch 237: 91831.62998\n",
      "Avg Minibatch loss for epoch 238: 92368.18121\n",
      "Avg Minibatch loss for epoch 239: 91049.44975\n",
      "Avg Minibatch loss for epoch 240: 90471.49957\n",
      "  ***   Training RMSE: 13.71718 ***\n",
      "  *** Validation RMSE: 27.38093 ***\n",
      "Avg Minibatch loss for epoch 241: 90322.14033\n",
      "Avg Minibatch loss for epoch 242: 90600.70776\n",
      "Avg Minibatch loss for epoch 243: 89915.19331\n",
      "Avg Minibatch loss for epoch 244: 90090.54055\n",
      "Avg Minibatch loss for epoch 245: 89923.31940\n",
      "  ***   Training RMSE: 12.28794 ***\n",
      "  *** Validation RMSE: 27.38254 ***\n",
      "Avg Minibatch loss for epoch 246: 89221.75626\n",
      "Avg Minibatch loss for epoch 247: 89748.58951\n",
      "Avg Minibatch loss for epoch 248: 89434.83261\n",
      "Avg Minibatch loss for epoch 249: 89908.03974\n",
      "Avg Minibatch loss for epoch 250: 89238.69876\n",
      "  ***   Training RMSE: 11.38148 ***\n",
      "  *** Validation RMSE: 27.38208 ***\n",
      "Avg Minibatch loss for epoch 251: 89544.90144\n",
      "Avg Minibatch loss for epoch 252: 89148.15201\n",
      "Avg Minibatch loss for epoch 253: 89094.24754\n",
      "Avg Minibatch loss for epoch 254: 88936.15282\n",
      "Avg Minibatch loss for epoch 255: 88771.04899\n",
      "  ***   Training RMSE: 10.89471 ***\n",
      "  *** Validation RMSE: 27.38197 ***\n",
      "Avg Minibatch loss for epoch 256: 88490.85027\n",
      "Avg Minibatch loss for epoch 257: 88422.83816\n",
      "Avg Minibatch loss for epoch 258: 87942.90193\n",
      "Avg Minibatch loss for epoch 259: 88414.64795\n",
      "Avg Minibatch loss for epoch 260: 88513.72767\n",
      "  ***   Training RMSE: 10.85852 ***\n",
      "  *** Validation RMSE: 27.38251 ***\n",
      "Avg Minibatch loss for epoch 261: 88198.64497\n",
      "Avg Minibatch loss for epoch 262: 88794.40690\n",
      "Avg Minibatch loss for epoch 263: 88251.29087\n",
      "Avg Minibatch loss for epoch 264: 87680.39315\n",
      "Avg Minibatch loss for epoch 265: 88262.78826\n",
      "  ***   Training RMSE: 11.09448 ***\n",
      "  *** Validation RMSE: 27.38276 ***\n",
      "Avg Minibatch loss for epoch 266: 86740.64574\n",
      "Avg Minibatch loss for epoch 267: 88084.12806\n",
      "Avg Minibatch loss for epoch 268: 89058.61644\n",
      "Avg Minibatch loss for epoch 269: 87781.70920\n",
      "Avg Minibatch loss for epoch 270: 86986.50185\n",
      "  ***   Training RMSE: 10.94869 ***\n",
      "  *** Validation RMSE: 27.38186 ***\n",
      "Avg Minibatch loss for epoch 271: 87195.24501\n",
      "Avg Minibatch loss for epoch 272: 87753.54963\n",
      "Avg Minibatch loss for epoch 273: 88011.96788\n",
      "Avg Minibatch loss for epoch 274: 87549.34610\n",
      "Avg Minibatch loss for epoch 275: 87883.06572\n",
      "  ***   Training RMSE: 11.16793 ***\n",
      "  *** Validation RMSE: 27.38150 ***\n",
      "Avg Minibatch loss for epoch 276: 87675.10369\n",
      "Avg Minibatch loss for epoch 277: 87266.17007\n",
      "Avg Minibatch loss for epoch 278: 88218.65628\n",
      "Avg Minibatch loss for epoch 279: 88001.13386\n",
      "Avg Minibatch loss for epoch 280: 87467.62369\n",
      "  ***   Training RMSE: 11.55653 ***\n",
      "  *** Validation RMSE: 27.38157 ***\n",
      "Avg Minibatch loss for epoch 281: 86941.67507\n",
      "Avg Minibatch loss for epoch 282: 86325.30288\n",
      "Avg Minibatch loss for epoch 283: 87495.44840\n",
      "Avg Minibatch loss for epoch 284: 86970.46773\n",
      "Avg Minibatch loss for epoch 285: 86652.97261\n",
      "  ***   Training RMSE: 11.63634 ***\n",
      "  *** Validation RMSE: 27.38222 ***\n",
      "Avg Minibatch loss for epoch 286: 86465.90101\n",
      "Avg Minibatch loss for epoch 287: 86899.95050\n",
      "Avg Minibatch loss for epoch 288: 86175.91610\n",
      "Avg Minibatch loss for epoch 289: 86320.04957\n",
      "Avg Minibatch loss for epoch 290: 85885.42712\n",
      "  ***   Training RMSE: 12.06096 ***\n",
      "  *** Validation RMSE: 27.38202 ***\n",
      "Avg Minibatch loss for epoch 291: 86793.50021\n",
      "Avg Minibatch loss for epoch 292: 87346.15951\n",
      "Avg Minibatch loss for epoch 293: 86292.71783\n",
      "Avg Minibatch loss for epoch 294: 86948.80008\n",
      "Avg Minibatch loss for epoch 295: 86372.72049\n",
      "  ***   Training RMSE: 12.01468 ***\n",
      "  *** Validation RMSE: 27.38317 ***\n",
      "Avg Minibatch loss for epoch 296: 86278.70936\n",
      "Avg Minibatch loss for epoch 297: 85791.73627\n",
      "Avg Minibatch loss for epoch 298: 86874.87327\n",
      "Avg Minibatch loss for epoch 299: 86182.59514\n",
      "\n",
      "Final Training set RMSE: 12.21765\n",
      "Final Validation set RMSE: 27.38317\n",
      "Test set RMSE: 12.35341\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "h0 = 75\n",
    "h1 = len(users)\n",
    "lambda_ = 0.002\n",
    "lr = .0001\n",
    "keep_prob = .75\n",
    "\n",
    "num_epochs = 300\n",
    "\n",
    "train_dataset = train.transpose().toarray().astype(np.float32)\n",
    "test_dataset = test.transpose().toarray().astype(np.float32)\n",
    "valid_dataset = val.transpose().toarray().astype(np.float32)\n",
    "\n",
    "means = np.mean(train_dataset, axis=0)\n",
    "stds = np.std(train_dataset, axis=0)\n",
    "stds[stds == 0] = 1\n",
    "train_dataset = (train_dataset - means) / stds\n",
    "valid_dataset = (valid_dataset - means) / stds\n",
    "test_dataset = (test_dataset - means) /stds\n",
    "\n",
    "train_labels = train_dataset\n",
    "valid_labels = valid_dataset\n",
    "test_labels = test_dataset\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                      shape=(batch_size, len(users)))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, len(users)))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    tf_tr_dataset = tf.constant(train_dataset)\n",
    "\n",
    "    # Variables.\n",
    "    w0 = tf.Variable(tf.truncated_normal([len(users), h0]))\n",
    "    b0 = tf.Variable(tf.zeros([h0]))\n",
    "\n",
    "    w1 = tf.Variable(tf.truncated_normal([h0, h1]))\n",
    "    b1 = tf.Variable(tf.zeros([h1]))\n",
    "    \n",
    "    # Training computation.\n",
    "    s0 = tf.nn.dropout(tf.nn.relu(tf.matmul(tf_train_dataset, w0) + b0), keep_prob)\n",
    "    logits = tf.matmul(s0, w1) + b1\n",
    "\n",
    "    reg = tf.nn.l2_loss(w0) + tf.nn.l2_loss(w1)\n",
    "    # loss = tf.reduce_sum((logits - tf_train_dataset) ** 2)+ (lambda_ * reg)\n",
    "    loss = tf.nn.l2_loss(logits - tf_train_dataset) + (lambda_ * reg)\n",
    "\n",
    "    # Optimizer.\n",
    "    # optimizer = tf.train.GradientDescentOptimizer(lr).minimize(loss)\n",
    "    optimizer = tf.train.AdamOptimizer(lr).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = logits\n",
    "\n",
    "    v0 = tf.nn.relu(tf.matmul(tf_valid_dataset, w0) + b0)\n",
    "    valid_prediction = tf.matmul(v0, w1) + b1\n",
    " \n",
    "\n",
    "    t0 = tf.nn.relu(tf.matmul(tf_test_dataset, w0) + b0)\n",
    "    test_prediction = tf.matmul(t0, w1) + b1\n",
    "    \n",
    "    r0 = tf.nn.relu(tf.matmul(tf_tr_dataset, w0) + b0)\n",
    "    tr_prediction = tf.matmul(r0, w1 + b1)\n",
    "\n",
    "batches = np.ceil(float(len(items)) / batch_size)\n",
    "num_steps = int(np.ceil(float(len(items)) / batch_size))\n",
    "# curves = []\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for epoch in range(num_epochs):\n",
    "        l_mean = 0\n",
    "        for step in range(num_steps):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            # Note: we could use better randomization across epochs.\n",
    "            # offset = (step * batch_size) % (int(batch_size * 2.5) - batch_size)\n",
    "            offset = (step * batch_size) % (len(items) - batch_size)\n",
    "            # Generate a minibatch.\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            # and the value is the numpy array to feed to it.\n",
    "            feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n",
    "\n",
    "            _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "            l_mean += l\n",
    "\n",
    "        print(\"Avg Minibatch loss for epoch %d: %.5f\" % (epoch, l_mean / num_steps))\n",
    "        # print(\" Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "        if (epoch % 5 == 0):\n",
    "            tra = accuracy(tr_prediction.eval(), train_labels)\n",
    "            va = accuracy(valid_prediction.eval(), valid_labels)\n",
    "            print(\"  ***   Training RMSE: %.5f ***\" % tra)\n",
    "            print(\"  *** Validation RMSE: %.5f ***\" % va)\n",
    "            # curves.append({'epoch': epoch, 'train': 1 - tra, 'val': 1- va})\n",
    "    print(\"\\nFinal Training set RMSE: %.5f\" % accuracy(tr_prediction.eval(), train_labels))\n",
    "    print(\"Final Validation set RMSE: %.5f\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test set RMSE: %.5f\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Avg Minibatch loss for epoch 0: 1478828.08636\n",
      "  ***   Training RMSE: 167.42828 ***\n",
      "  *** Validation RMSE: 3329.48739 ***\n",
      "Avg Minibatch loss for epoch 1: 925830.27094\n",
      "Avg Minibatch loss for epoch 2: 796838.57367\n",
      "Avg Minibatch loss for epoch 3: 709300.95462\n",
      "Avg Minibatch loss for epoch 4: 638029.11594\n",
      "Avg Minibatch loss for epoch 5: 577017.64820\n",
      "  ***   Training RMSE: 176.52730 ***\n",
      "  *** Validation RMSE: 3328.28293 ***\n",
      "Avg Minibatch loss for epoch 6: 523972.65522\n",
      "Avg Minibatch loss for epoch 7: 476024.80513\n",
      "Avg Minibatch loss for epoch 8: 434066.82322\n",
      "Avg Minibatch loss for epoch 9: 396588.01229\n",
      "Avg Minibatch loss for epoch 10: 362280.61263\n",
      "  ***   Training RMSE: 154.95703 ***\n",
      "  *** Validation RMSE: 3327.83986 ***\n",
      "Avg Minibatch loss for epoch 11: 332607.48494\n",
      "Avg Minibatch loss for epoch 12: 305523.03097\n",
      "Avg Minibatch loss for epoch 13: 281035.56332\n",
      "Avg Minibatch loss for epoch 14: 259283.91772\n",
      "Avg Minibatch loss for epoch 15: 239667.97712\n",
      "  ***   Training RMSE: 135.69429 ***\n",
      "  *** Validation RMSE: 3327.59072 ***\n",
      "Avg Minibatch loss for epoch 16: 222423.32085\n",
      "Avg Minibatch loss for epoch 17: 207307.27236\n",
      "Avg Minibatch loss for epoch 18: 193161.82343\n",
      "Avg Minibatch loss for epoch 19: 181105.86024\n",
      "Avg Minibatch loss for epoch 20: 170290.87905\n",
      "  ***   Training RMSE: 123.85715 ***\n",
      "  *** Validation RMSE: 3327.45439 ***\n",
      "Avg Minibatch loss for epoch 21: 160965.20375\n",
      "Avg Minibatch loss for epoch 22: 152614.08021\n",
      "Avg Minibatch loss for epoch 23: 145329.29627\n",
      "Avg Minibatch loss for epoch 24: 138886.80039\n",
      "Avg Minibatch loss for epoch 25: 133167.01151\n",
      "  ***   Training RMSE: 117.58499 ***\n",
      "  *** Validation RMSE: 3327.38023 ***\n",
      "Avg Minibatch loss for epoch 26: 128499.60564\n",
      "Avg Minibatch loss for epoch 27: 124384.79571\n",
      "Avg Minibatch loss for epoch 28: 120842.92141\n",
      "Avg Minibatch loss for epoch 29: 117884.85420\n",
      "Avg Minibatch loss for epoch 30: 115148.92418\n",
      "  ***   Training RMSE: 114.66011 ***\n",
      "  *** Validation RMSE: 3327.35406 ***\n",
      "Avg Minibatch loss for epoch 31: 112968.95408\n",
      "Avg Minibatch loss for epoch 32: 111000.17993\n",
      "Avg Minibatch loss for epoch 33: 109382.36240\n",
      "Avg Minibatch loss for epoch 34: 108043.84569\n",
      "Avg Minibatch loss for epoch 35: 106790.31632\n",
      "  ***   Training RMSE: 113.19019 ***\n",
      "  *** Validation RMSE: 3327.34389 ***\n",
      "Avg Minibatch loss for epoch 36: 105789.59941\n",
      "Avg Minibatch loss for epoch 37: 104988.01005\n",
      "Avg Minibatch loss for epoch 38: 104234.10504\n",
      "Avg Minibatch loss for epoch 39: 103630.65044\n",
      "Avg Minibatch loss for epoch 40: 103133.75314\n",
      "  ***   Training RMSE: 111.33915 ***\n",
      "  *** Validation RMSE: 3327.33939 ***\n",
      "Avg Minibatch loss for epoch 41: 102667.78931\n",
      "Avg Minibatch loss for epoch 42: 102279.24082\n",
      "Avg Minibatch loss for epoch 43: 101986.33922\n",
      "Avg Minibatch loss for epoch 44: 101718.06211\n",
      "Avg Minibatch loss for epoch 45: 101451.53988\n",
      "  ***   Training RMSE: 109.92596 ***\n",
      "  *** Validation RMSE: 3327.33806 ***\n",
      "Avg Minibatch loss for epoch 46: 101213.44180\n",
      "Avg Minibatch loss for epoch 47: 101058.64653\n",
      "Avg Minibatch loss for epoch 48: 100897.73847\n",
      "Avg Minibatch loss for epoch 49: 100803.14456\n",
      "Avg Minibatch loss for epoch 50: 100718.46374\n",
      "  ***   Training RMSE: 108.54919 ***\n",
      "  *** Validation RMSE: 3327.33739 ***\n",
      "Avg Minibatch loss for epoch 51: 100569.15428\n",
      "Avg Minibatch loss for epoch 52: 100522.39562\n",
      "Avg Minibatch loss for epoch 53: 100363.14849\n",
      "Avg Minibatch loss for epoch 54: 100324.44221\n",
      "Avg Minibatch loss for epoch 55: 100291.15694\n",
      "  ***   Training RMSE: 106.96466 ***\n",
      "  *** Validation RMSE: 3327.33722 ***\n",
      "Avg Minibatch loss for epoch 56: 100267.61339\n",
      "Avg Minibatch loss for epoch 57: 100169.50158\n",
      "Avg Minibatch loss for epoch 58: 100152.37395\n",
      "Avg Minibatch loss for epoch 59: 100079.67804\n",
      "Avg Minibatch loss for epoch 60: 100040.66313\n",
      "  ***   Training RMSE: 105.12378 ***\n",
      "  *** Validation RMSE: 3327.33706 ***\n",
      "Avg Minibatch loss for epoch 61: 100068.04888\n",
      "Avg Minibatch loss for epoch 62: 99999.47778\n",
      "Avg Minibatch loss for epoch 63: 100004.19715\n",
      "Avg Minibatch loss for epoch 64: 100060.75937\n",
      "Avg Minibatch loss for epoch 65: 99961.66676\n",
      "  ***   Training RMSE: 103.10496 ***\n",
      "  *** Validation RMSE: 3327.33772 ***\n",
      "Avg Minibatch loss for epoch 66: 99924.84770\n",
      "Avg Minibatch loss for epoch 67: 99922.58492\n",
      "Avg Minibatch loss for epoch 68: 99935.95490\n",
      "Avg Minibatch loss for epoch 69: 99856.36652\n",
      "Avg Minibatch loss for epoch 70: 99893.03246\n",
      "  ***   Training RMSE: 101.62992 ***\n",
      "  *** Validation RMSE: 3327.33772 ***\n",
      "Avg Minibatch loss for epoch 71: 99867.45407\n",
      "Avg Minibatch loss for epoch 72: 99825.31704\n",
      "Avg Minibatch loss for epoch 73: 99845.51848\n",
      "Avg Minibatch loss for epoch 74: 99834.04815\n",
      "Avg Minibatch loss for epoch 75: 99762.32479\n",
      "  ***   Training RMSE: 100.69597 ***\n",
      "  *** Validation RMSE: 3327.33772 ***\n",
      "Avg Minibatch loss for epoch 76: 99825.43921\n",
      "Avg Minibatch loss for epoch 77: 99802.40141\n",
      "Avg Minibatch loss for epoch 78: 99734.04795\n",
      "Avg Minibatch loss for epoch 79: 99809.37766\n",
      "Avg Minibatch loss for epoch 80: 99760.07199\n",
      "  ***   Training RMSE: 99.60051 ***\n",
      "  *** Validation RMSE: 3327.33789 ***\n",
      "Avg Minibatch loss for epoch 81: 99784.95220\n",
      "Avg Minibatch loss for epoch 82: 99805.24130\n",
      "Avg Minibatch loss for epoch 83: 99787.01949\n",
      "Avg Minibatch loss for epoch 84: 99770.71262\n",
      "Avg Minibatch loss for epoch 85: 99797.80192\n",
      "  ***   Training RMSE: 99.23786 ***\n",
      "  *** Validation RMSE: 3327.33806 ***\n",
      "Avg Minibatch loss for epoch 86: 99689.28126\n",
      "Avg Minibatch loss for epoch 87: 99718.49879\n",
      "Avg Minibatch loss for epoch 88: 99695.97244\n",
      "Avg Minibatch loss for epoch 89: 99714.42123\n",
      "Avg Minibatch loss for epoch 90: 99675.82343\n",
      "  ***   Training RMSE: 98.41470 ***\n",
      "  *** Validation RMSE: 3327.33822 ***\n",
      "Avg Minibatch loss for epoch 91: 99698.84688\n",
      "Avg Minibatch loss for epoch 92: 99684.61420\n",
      "Avg Minibatch loss for epoch 93: 99683.81560\n",
      "Avg Minibatch loss for epoch 94: 99780.51101\n",
      "Avg Minibatch loss for epoch 95: 99671.78544\n",
      "  ***   Training RMSE: 98.46024 ***\n",
      "  *** Validation RMSE: 3327.33839 ***\n",
      "Avg Minibatch loss for epoch 96: 99682.65300\n",
      "Avg Minibatch loss for epoch 97: 99711.25602\n",
      "Avg Minibatch loss for epoch 98: 99695.19112\n",
      "Avg Minibatch loss for epoch 99: 99707.02443\n",
      "Avg Minibatch loss for epoch 100: 99615.05190\n",
      "  ***   Training RMSE: 97.68299 ***\n",
      "  *** Validation RMSE: 3327.33872 ***\n",
      "Avg Minibatch loss for epoch 101: 99705.64905\n",
      "Avg Minibatch loss for epoch 102: 99599.75404\n",
      "Avg Minibatch loss for epoch 103: 99699.61381\n",
      "Avg Minibatch loss for epoch 104: 99745.52712\n",
      "Avg Minibatch loss for epoch 105: 99660.30480\n",
      "  ***   Training RMSE: 97.67075 ***\n",
      "  *** Validation RMSE: 3327.33889 ***\n",
      "Avg Minibatch loss for epoch 106: 99730.68719\n",
      "Avg Minibatch loss for epoch 107: 99648.28454\n",
      "Avg Minibatch loss for epoch 108: 99616.99693\n",
      "Avg Minibatch loss for epoch 109: 99647.34263\n",
      "Avg Minibatch loss for epoch 110: 99730.27363\n",
      "  ***   Training RMSE: 97.37556 ***\n",
      "  *** Validation RMSE: 3327.33906 ***\n",
      "Avg Minibatch loss for epoch 111: 99631.57639\n",
      "Avg Minibatch loss for epoch 112: 99643.74077\n",
      "Avg Minibatch loss for epoch 113: 99612.53548\n",
      "Avg Minibatch loss for epoch 114: 99731.79864\n",
      "Avg Minibatch loss for epoch 115: 99726.20965\n",
      "  ***   Training RMSE: 96.97246 ***\n",
      "  *** Validation RMSE: 3327.33939 ***\n",
      "Avg Minibatch loss for epoch 116: 99608.40477\n",
      "Avg Minibatch loss for epoch 117: 99686.52620\n",
      "Avg Minibatch loss for epoch 118: 99643.74784\n",
      "Avg Minibatch loss for epoch 119: 99532.10140\n",
      "Avg Minibatch loss for epoch 120: 99593.56038\n",
      "  ***   Training RMSE: 96.66205 ***\n",
      "  *** Validation RMSE: 3327.33972 ***\n",
      "Avg Minibatch loss for epoch 121: 99642.29749\n",
      "Avg Minibatch loss for epoch 122: 99541.27135\n",
      "Avg Minibatch loss for epoch 123: 99639.09952\n",
      "Avg Minibatch loss for epoch 124: 99622.15969\n",
      "Avg Minibatch loss for epoch 125: 99653.57005\n",
      "  ***   Training RMSE: 96.83135 ***\n",
      "  *** Validation RMSE: 3327.33972 ***\n",
      "Avg Minibatch loss for epoch 126: 99543.30949\n",
      "Avg Minibatch loss for epoch 127: 99554.87973\n",
      "Avg Minibatch loss for epoch 128: 99595.79282\n",
      "Avg Minibatch loss for epoch 129: 99635.66002\n",
      "Avg Minibatch loss for epoch 130: 99571.05152\n",
      "  ***   Training RMSE: 96.13573 ***\n",
      "  *** Validation RMSE: 3327.33972 ***\n",
      "Avg Minibatch loss for epoch 131: 99645.40964\n",
      "Avg Minibatch loss for epoch 132: 99546.68647\n",
      "Avg Minibatch loss for epoch 133: 99619.02538\n",
      "Avg Minibatch loss for epoch 134: 99506.99779\n",
      "Avg Minibatch loss for epoch 135: 99469.81249\n",
      "  ***   Training RMSE: 96.02803 ***\n",
      "  *** Validation RMSE: 3327.33989 ***\n",
      "Avg Minibatch loss for epoch 136: 99606.03811\n",
      "Avg Minibatch loss for epoch 137: 99622.40258\n",
      "Avg Minibatch loss for epoch 138: 99597.37395\n",
      "Avg Minibatch loss for epoch 139: 99505.48277\n",
      "Avg Minibatch loss for epoch 140: 99568.27265\n",
      "  ***   Training RMSE: 95.84133 ***\n",
      "  *** Validation RMSE: 3327.34022 ***\n",
      "Avg Minibatch loss for epoch 141: 99596.10889\n",
      "Avg Minibatch loss for epoch 142: 99587.85385\n",
      "Avg Minibatch loss for epoch 143: 99540.50677\n",
      "Avg Minibatch loss for epoch 144: 99526.79486\n",
      "Avg Minibatch loss for epoch 145: 99652.60714\n",
      "  ***   Training RMSE: 95.24153 ***\n",
      "  *** Validation RMSE: 3327.34022 ***\n",
      "Avg Minibatch loss for epoch 146: 99500.09109\n",
      "Avg Minibatch loss for epoch 147: 99536.59981\n",
      "Avg Minibatch loss for epoch 148: 99588.89501\n",
      "Avg Minibatch loss for epoch 149: 99454.54736\n",
      "Avg Minibatch loss for epoch 150: 99484.44918\n",
      "  ***   Training RMSE: 95.30972 ***\n",
      "  *** Validation RMSE: 3327.34022 ***\n",
      "Avg Minibatch loss for epoch 151: 99498.35433\n",
      "Avg Minibatch loss for epoch 152: 99515.90117\n",
      "Avg Minibatch loss for epoch 153: 99502.03157\n",
      "Avg Minibatch loss for epoch 154: 99497.18051\n",
      "Avg Minibatch loss for epoch 155: 99521.89235\n",
      "  ***   Training RMSE: 95.09808 ***\n",
      "  *** Validation RMSE: 3327.34039 ***\n",
      "Avg Minibatch loss for epoch 156: 99631.55848\n",
      "Avg Minibatch loss for epoch 157: 99415.67203\n",
      "Avg Minibatch loss for epoch 158: 99451.15794\n",
      "Avg Minibatch loss for epoch 159: 99431.24563\n",
      "Avg Minibatch loss for epoch 160: 99467.31068\n",
      "  ***   Training RMSE: 94.61496 ***\n",
      "  *** Validation RMSE: 3327.34072 ***\n",
      "Avg Minibatch loss for epoch 161: 99530.31959\n",
      "Avg Minibatch loss for epoch 162: 99502.41985\n",
      "Avg Minibatch loss for epoch 163: 99398.53384\n",
      "Avg Minibatch loss for epoch 164: 99471.34684\n",
      "Avg Minibatch loss for epoch 165: 99466.74827\n",
      "  ***   Training RMSE: 94.50150 ***\n",
      "  *** Validation RMSE: 3327.34072 ***\n",
      "Avg Minibatch loss for epoch 166: 99576.56636\n",
      "Avg Minibatch loss for epoch 167: 99463.37434\n",
      "Avg Minibatch loss for epoch 168: 99560.90504\n",
      "Avg Minibatch loss for epoch 169: 99535.66250\n",
      "Avg Minibatch loss for epoch 170: 99460.55807\n",
      "  ***   Training RMSE: 94.79404 ***\n",
      "  *** Validation RMSE: 3327.34056 ***\n",
      "Avg Minibatch loss for epoch 171: 99535.97025\n",
      "Avg Minibatch loss for epoch 172: 99457.15537\n",
      "Avg Minibatch loss for epoch 173: 99509.87431\n",
      "Avg Minibatch loss for epoch 174: 99449.61550\n",
      "Avg Minibatch loss for epoch 175: 99448.57186\n",
      "  ***   Training RMSE: 94.67808 ***\n",
      "  *** Validation RMSE: 3327.34039 ***\n",
      "Avg Minibatch loss for epoch 176: 99491.52376\n",
      "Avg Minibatch loss for epoch 177: 99457.66518\n",
      "Avg Minibatch loss for epoch 178: 99505.68742\n",
      "Avg Minibatch loss for epoch 179: 99546.63855\n",
      "Avg Minibatch loss for epoch 180: 99515.70607\n",
      "  ***   Training RMSE: 94.19233 ***\n",
      "  *** Validation RMSE: 3327.34056 ***\n",
      "Avg Minibatch loss for epoch 181: 99485.24128\n",
      "Avg Minibatch loss for epoch 182: 99415.55320\n",
      "Avg Minibatch loss for epoch 183: 99385.34805\n",
      "Avg Minibatch loss for epoch 184: 99517.27663\n",
      "Avg Minibatch loss for epoch 185: 99516.83532\n",
      "  ***   Training RMSE: 94.05899 ***\n",
      "  *** Validation RMSE: 3327.34072 ***\n",
      "Avg Minibatch loss for epoch 186: 99422.66321\n",
      "Avg Minibatch loss for epoch 187: 99527.56260\n",
      "Avg Minibatch loss for epoch 188: 99507.67266\n",
      "Avg Minibatch loss for epoch 189: 99440.40807\n",
      "Avg Minibatch loss for epoch 190: 99467.71227\n",
      "  ***   Training RMSE: 93.10613 ***\n",
      "  *** Validation RMSE: 3327.34106 ***\n",
      "Avg Minibatch loss for epoch 191: 99402.82487\n",
      "Avg Minibatch loss for epoch 192: 99572.74808\n",
      "Avg Minibatch loss for epoch 193: 99507.73537\n",
      "Avg Minibatch loss for epoch 194: 99445.95017\n",
      "Avg Minibatch loss for epoch 195: 99475.85744\n",
      "  ***   Training RMSE: 93.67343 ***\n",
      "  *** Validation RMSE: 3327.34056 ***\n",
      "Avg Minibatch loss for epoch 196: 99335.66453\n",
      "Avg Minibatch loss for epoch 197: 99439.96107\n",
      "Avg Minibatch loss for epoch 198: 99461.78346\n",
      "Avg Minibatch loss for epoch 199: 99512.40719\n",
      "Avg Minibatch loss for epoch 200: 99592.21708\n",
      "  ***   Training RMSE: 93.49012 ***\n",
      "  *** Validation RMSE: 3327.34056 ***\n",
      "Avg Minibatch loss for epoch 201: 99437.03110\n",
      "Avg Minibatch loss for epoch 202: 99482.31773\n",
      "Avg Minibatch loss for epoch 203: 99431.83102\n",
      "Avg Minibatch loss for epoch 204: 99387.73002\n",
      "Avg Minibatch loss for epoch 205: 99363.50522\n",
      "  ***   Training RMSE: 94.02951 ***\n",
      "  *** Validation RMSE: 3327.34056 ***\n",
      "Avg Minibatch loss for epoch 206: 99469.67508\n",
      "Avg Minibatch loss for epoch 207: 99459.30340\n",
      "Avg Minibatch loss for epoch 208: 99480.20918\n",
      "Avg Minibatch loss for epoch 209: 99452.70928\n",
      "Avg Minibatch loss for epoch 210: 99422.37240\n",
      "  ***   Training RMSE: 93.79904 ***\n",
      "  *** Validation RMSE: 3327.34056 ***\n",
      "Avg Minibatch loss for epoch 211: 99393.36539\n",
      "Avg Minibatch loss for epoch 212: 99485.18329\n",
      "Avg Minibatch loss for epoch 213: 99454.69548\n",
      "Avg Minibatch loss for epoch 214: 99498.96710\n",
      "Avg Minibatch loss for epoch 215: 99390.60181\n",
      "  ***   Training RMSE: 93.57992 ***\n",
      "  *** Validation RMSE: 3327.34056 ***\n",
      "Avg Minibatch loss for epoch 216: 99436.15911\n",
      "Avg Minibatch loss for epoch 217: 99443.83468\n",
      "Avg Minibatch loss for epoch 218: 99350.04449\n",
      "Avg Minibatch loss for epoch 219: 99413.56002\n",
      "Avg Minibatch loss for epoch 220: 99329.34083\n",
      "  ***   Training RMSE: 93.25509 ***\n",
      "  *** Validation RMSE: 3327.34089 ***\n",
      "Avg Minibatch loss for epoch 221: 99543.12341\n",
      "Avg Minibatch loss for epoch 222: 99379.99183\n",
      "Avg Minibatch loss for epoch 223: 99415.46364\n",
      "Avg Minibatch loss for epoch 224: 99469.24616\n",
      "Avg Minibatch loss for epoch 225: 99468.89202\n",
      "  ***   Training RMSE: 93.05350 ***\n",
      "  *** Validation RMSE: 3327.34089 ***\n",
      "Avg Minibatch loss for epoch 226: 99385.49828\n",
      "Avg Minibatch loss for epoch 227: 99366.97969\n",
      "Avg Minibatch loss for epoch 228: 99293.42671\n",
      "Avg Minibatch loss for epoch 229: 99382.81111\n",
      "Avg Minibatch loss for epoch 230: 99309.97833\n",
      "  ***   Training RMSE: 93.10812 ***\n",
      "  *** Validation RMSE: 3327.34072 ***\n",
      "Avg Minibatch loss for epoch 231: 99484.00151\n",
      "Avg Minibatch loss for epoch 232: 99410.45751\n",
      "Avg Minibatch loss for epoch 233: 99420.88868\n",
      "Avg Minibatch loss for epoch 234: 99350.87223\n",
      "Avg Minibatch loss for epoch 235: 99461.14779\n",
      "  ***   Training RMSE: 93.46183 ***\n",
      "  *** Validation RMSE: 3327.34039 ***\n",
      "Avg Minibatch loss for epoch 236: 99447.58330\n",
      "Avg Minibatch loss for epoch 237: 99382.79449\n",
      "Avg Minibatch loss for epoch 238: 99342.57423\n",
      "Avg Minibatch loss for epoch 239: 99459.79409\n",
      "Avg Minibatch loss for epoch 240: 99459.60485\n",
      "  ***   Training RMSE: 93.00564 ***\n",
      "  *** Validation RMSE: 3327.34089 ***\n",
      "Avg Minibatch loss for epoch 241: 99380.84497\n",
      "Avg Minibatch loss for epoch 242: 99474.47972\n",
      "Avg Minibatch loss for epoch 243: 99481.69595\n",
      "Avg Minibatch loss for epoch 244: 99597.49166\n",
      "Avg Minibatch loss for epoch 245: 99403.62822\n",
      "  ***   Training RMSE: 92.96455 ***\n",
      "  *** Validation RMSE: 3327.34072 ***\n",
      "Avg Minibatch loss for epoch 246: 99459.79167\n",
      "Avg Minibatch loss for epoch 247: 99343.44088\n",
      "Avg Minibatch loss for epoch 248: 99195.56374\n",
      "Avg Minibatch loss for epoch 249: 99429.65796\n",
      "Avg Minibatch loss for epoch 250: 99405.20072\n",
      "  ***   Training RMSE: 92.75626 ***\n",
      "  *** Validation RMSE: 3327.34072 ***\n",
      "Avg Minibatch loss for epoch 251: 99417.27727\n",
      "Avg Minibatch loss for epoch 252: 99452.06525\n",
      "Avg Minibatch loss for epoch 253: 99331.87231\n",
      "Avg Minibatch loss for epoch 254: 99465.29452\n",
      "Avg Minibatch loss for epoch 255: 99366.86310\n",
      "  ***   Training RMSE: 93.26411 ***\n",
      "  *** Validation RMSE: 3327.34039 ***\n",
      "Avg Minibatch loss for epoch 256: 99310.32746\n",
      "Avg Minibatch loss for epoch 257: 99334.57728\n",
      "Avg Minibatch loss for epoch 258: 99486.60131\n",
      "Avg Minibatch loss for epoch 259: 99385.94002\n",
      "Avg Minibatch loss for epoch 260: 99372.75833\n",
      "  ***   Training RMSE: 92.97741 ***\n",
      "  *** Validation RMSE: 3327.34106 ***\n",
      "Avg Minibatch loss for epoch 261: 99363.56986\n",
      "Avg Minibatch loss for epoch 262: 99401.59070\n",
      "Avg Minibatch loss for epoch 263: 99432.58379\n",
      "Avg Minibatch loss for epoch 264: 99413.17740\n",
      "Avg Minibatch loss for epoch 265: 99415.54248\n",
      "  ***   Training RMSE: 92.76983 ***\n",
      "  *** Validation RMSE: 3327.34072 ***\n",
      "Avg Minibatch loss for epoch 266: 99419.07862\n",
      "Avg Minibatch loss for epoch 267: 99352.44086\n",
      "Avg Minibatch loss for epoch 268: 99349.94474\n",
      "Avg Minibatch loss for epoch 269: 99494.24081\n",
      "Avg Minibatch loss for epoch 270: 99500.80028\n",
      "  ***   Training RMSE: 92.69429 ***\n",
      "  *** Validation RMSE: 3327.34056 ***\n",
      "Avg Minibatch loss for epoch 271: 99423.84744\n",
      "Avg Minibatch loss for epoch 272: 99469.63922\n",
      "Avg Minibatch loss for epoch 273: 99345.64956\n",
      "Avg Minibatch loss for epoch 274: 99415.65959\n",
      "Avg Minibatch loss for epoch 275: 99365.53680\n",
      "  ***   Training RMSE: 92.62043 ***\n",
      "  *** Validation RMSE: 3327.34089 ***\n",
      "Avg Minibatch loss for epoch 276: 99358.50130\n",
      "Avg Minibatch loss for epoch 277: 99501.28678\n",
      "Avg Minibatch loss for epoch 278: 99314.87514\n",
      "Avg Minibatch loss for epoch 279: 99419.45909\n",
      "Avg Minibatch loss for epoch 280: 99413.56621\n",
      "  ***   Training RMSE: 92.40957 ***\n",
      "  *** Validation RMSE: 3327.34056 ***\n",
      "Avg Minibatch loss for epoch 281: 99383.97632\n",
      "Avg Minibatch loss for epoch 282: 99268.03085\n",
      "Avg Minibatch loss for epoch 283: 99333.21290\n",
      "Avg Minibatch loss for epoch 284: 99353.25589\n",
      "Avg Minibatch loss for epoch 285: 99303.29614\n",
      "  ***   Training RMSE: 92.41867 ***\n",
      "  *** Validation RMSE: 3327.34089 ***\n",
      "Avg Minibatch loss for epoch 286: 99291.31938\n",
      "Avg Minibatch loss for epoch 287: 99293.96251\n",
      "Avg Minibatch loss for epoch 288: 99486.79984\n",
      "Avg Minibatch loss for epoch 289: 99392.66251\n",
      "Avg Minibatch loss for epoch 290: 99336.18315\n",
      "  ***   Training RMSE: 92.16409 ***\n",
      "  *** Validation RMSE: 3327.34089 ***\n",
      "Avg Minibatch loss for epoch 291: 99347.27552\n",
      "Avg Minibatch loss for epoch 292: 99364.29203\n",
      "Avg Minibatch loss for epoch 293: 99410.29288\n",
      "Avg Minibatch loss for epoch 294: 99390.94346\n",
      "Avg Minibatch loss for epoch 295: 99253.77419\n",
      "  ***   Training RMSE: 92.59097 ***\n",
      "  *** Validation RMSE: 3327.34089 ***\n",
      "Avg Minibatch loss for epoch 296: 99430.57272\n",
      "Avg Minibatch loss for epoch 297: 99325.93924\n",
      "Avg Minibatch loss for epoch 298: 99426.98326\n",
      "Avg Minibatch loss for epoch 299: 99441.75998\n",
      "Avg Minibatch loss for epoch 300: 99379.00602\n",
      "  ***   Training RMSE: 92.42002 ***\n",
      "  *** Validation RMSE: 3327.34072 ***\n",
      "Avg Minibatch loss for epoch 301: 99438.38336\n",
      "Avg Minibatch loss for epoch 302: 99299.79144\n",
      "Avg Minibatch loss for epoch 303: 99348.02542\n",
      "Avg Minibatch loss for epoch 304: 99310.58897\n",
      "Avg Minibatch loss for epoch 305: 99369.59124\n",
      "  ***   Training RMSE: 92.27729 ***\n",
      "  *** Validation RMSE: 3327.34072 ***\n",
      "Avg Minibatch loss for epoch 306: 99340.96698\n",
      "Avg Minibatch loss for epoch 307: 99456.78186\n",
      "Avg Minibatch loss for epoch 308: 99363.19321\n",
      "Avg Minibatch loss for epoch 309: 99278.59130\n",
      "Avg Minibatch loss for epoch 310: 99445.23320\n",
      "  ***   Training RMSE: 92.56709 ***\n",
      "  *** Validation RMSE: 3327.34072 ***\n",
      "Avg Minibatch loss for epoch 311: 99309.54584\n",
      "Avg Minibatch loss for epoch 312: 99382.76476\n",
      "Avg Minibatch loss for epoch 313: 99306.12877\n",
      "Avg Minibatch loss for epoch 314: 99344.81914\n",
      "Avg Minibatch loss for epoch 315: 99276.63902\n",
      "  ***   Training RMSE: 92.13934 ***\n",
      "  *** Validation RMSE: 3327.34106 ***\n",
      "Avg Minibatch loss for epoch 316: 99391.87449\n",
      "Avg Minibatch loss for epoch 317: 99356.11668\n",
      "Avg Minibatch loss for epoch 318: 99456.55031\n",
      "Avg Minibatch loss for epoch 319: 99450.43480\n",
      "Avg Minibatch loss for epoch 320: 99330.77973\n",
      "  ***   Training RMSE: 92.37950 ***\n",
      "  *** Validation RMSE: 3327.34089 ***\n",
      "Avg Minibatch loss for epoch 321: 99425.42610\n",
      "Avg Minibatch loss for epoch 322: 99373.61059\n",
      "Avg Minibatch loss for epoch 323: 99363.64111\n",
      "Avg Minibatch loss for epoch 324: 99320.95331\n",
      "Avg Minibatch loss for epoch 325: 99245.66747\n",
      "  ***   Training RMSE: 91.86849 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 326: 99278.68655\n",
      "Avg Minibatch loss for epoch 327: 99307.55107\n",
      "Avg Minibatch loss for epoch 328: 99437.34618\n",
      "Avg Minibatch loss for epoch 329: 99366.54581\n",
      "Avg Minibatch loss for epoch 330: 99283.64808\n",
      "  ***   Training RMSE: 92.13204 ***\n",
      "  *** Validation RMSE: 3327.34089 ***\n",
      "Avg Minibatch loss for epoch 331: 99297.99688\n",
      "Avg Minibatch loss for epoch 332: 99359.15857\n",
      "Avg Minibatch loss for epoch 333: 99391.40128\n",
      "Avg Minibatch loss for epoch 334: 99244.06152\n",
      "Avg Minibatch loss for epoch 335: 99449.52417\n",
      "  ***   Training RMSE: 91.68348 ***\n",
      "  *** Validation RMSE: 3327.34106 ***\n",
      "Avg Minibatch loss for epoch 336: 99422.28421\n",
      "Avg Minibatch loss for epoch 337: 99256.39946\n",
      "Avg Minibatch loss for epoch 338: 99308.63993\n",
      "Avg Minibatch loss for epoch 339: 99343.24834\n",
      "Avg Minibatch loss for epoch 340: 99243.69619\n",
      "  ***   Training RMSE: 91.82695 ***\n",
      "  *** Validation RMSE: 3327.34089 ***\n",
      "Avg Minibatch loss for epoch 341: 99351.95219\n",
      "Avg Minibatch loss for epoch 342: 99256.52460\n",
      "Avg Minibatch loss for epoch 343: 99324.37901\n",
      "Avg Minibatch loss for epoch 344: 99390.70369\n",
      "Avg Minibatch loss for epoch 345: 99382.56645\n",
      "  ***   Training RMSE: 91.64340 ***\n",
      "  *** Validation RMSE: 3327.34089 ***\n",
      "Avg Minibatch loss for epoch 346: 99319.38024\n",
      "Avg Minibatch loss for epoch 347: 99314.92108\n",
      "Avg Minibatch loss for epoch 348: 99263.20933\n",
      "Avg Minibatch loss for epoch 349: 99385.46820\n",
      "Avg Minibatch loss for epoch 350: 99321.76885\n",
      "  ***   Training RMSE: 92.04109 ***\n",
      "  *** Validation RMSE: 3327.34106 ***\n",
      "Avg Minibatch loss for epoch 351: 99305.18519\n",
      "Avg Minibatch loss for epoch 352: 99349.94564\n",
      "Avg Minibatch loss for epoch 353: 99406.48318\n",
      "Avg Minibatch loss for epoch 354: 99315.94297\n",
      "Avg Minibatch loss for epoch 355: 99331.56960\n",
      "  ***   Training RMSE: 91.87033 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 356: 99323.35723\n",
      "Avg Minibatch loss for epoch 357: 99256.21253\n",
      "Avg Minibatch loss for epoch 358: 99247.53195\n",
      "Avg Minibatch loss for epoch 359: 99405.00413\n",
      "Avg Minibatch loss for epoch 360: 99218.69987\n",
      "  ***   Training RMSE: 91.93356 ***\n",
      "  *** Validation RMSE: 3327.34089 ***\n",
      "Avg Minibatch loss for epoch 361: 99338.01991\n",
      "Avg Minibatch loss for epoch 362: 99324.50831\n",
      "Avg Minibatch loss for epoch 363: 99361.07445\n",
      "Avg Minibatch loss for epoch 364: 99361.14255\n",
      "Avg Minibatch loss for epoch 365: 99422.39269\n",
      "  ***   Training RMSE: 91.28790 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 366: 99252.53824\n",
      "Avg Minibatch loss for epoch 367: 99344.24788\n",
      "Avg Minibatch loss for epoch 368: 99259.58819\n",
      "Avg Minibatch loss for epoch 369: 99243.23281\n",
      "Avg Minibatch loss for epoch 370: 99230.19007\n",
      "  ***   Training RMSE: 91.86407 ***\n",
      "  *** Validation RMSE: 3327.34156 ***\n",
      "Avg Minibatch loss for epoch 371: 99385.40093\n",
      "Avg Minibatch loss for epoch 372: 99248.98591\n",
      "Avg Minibatch loss for epoch 373: 99165.65964\n",
      "Avg Minibatch loss for epoch 374: 99279.01399\n",
      "Avg Minibatch loss for epoch 375: 99280.99243\n",
      "  ***   Training RMSE: 91.50711 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 376: 99312.95102\n",
      "Avg Minibatch loss for epoch 377: 99356.46460\n",
      "Avg Minibatch loss for epoch 378: 99405.63518\n",
      "Avg Minibatch loss for epoch 379: 99310.03715\n",
      "Avg Minibatch loss for epoch 380: 99382.40630\n",
      "  ***   Training RMSE: 91.54494 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 381: 99338.75965\n",
      "Avg Minibatch loss for epoch 382: 99293.36441\n",
      "Avg Minibatch loss for epoch 383: 99269.78756\n",
      "Avg Minibatch loss for epoch 384: 99262.65291\n",
      "Avg Minibatch loss for epoch 385: 99363.95220\n",
      "  ***   Training RMSE: 91.75879 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 386: 99298.33390\n",
      "Avg Minibatch loss for epoch 387: 99360.37449\n",
      "Avg Minibatch loss for epoch 388: 99208.77574\n",
      "Avg Minibatch loss for epoch 389: 99411.43934\n",
      "Avg Minibatch loss for epoch 390: 99314.20056\n",
      "  ***   Training RMSE: 90.87242 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 391: 99216.23400\n",
      "Avg Minibatch loss for epoch 392: 99313.65358\n",
      "Avg Minibatch loss for epoch 393: 99224.36389\n",
      "Avg Minibatch loss for epoch 394: 99331.26941\n",
      "Avg Minibatch loss for epoch 395: 99302.69028\n",
      "  ***   Training RMSE: 91.35571 ***\n",
      "  *** Validation RMSE: 3327.34172 ***\n",
      "Avg Minibatch loss for epoch 396: 99353.36799\n",
      "Avg Minibatch loss for epoch 397: 99132.27901\n",
      "Avg Minibatch loss for epoch 398: 99173.32484\n",
      "Avg Minibatch loss for epoch 399: 99234.87704\n",
      "Avg Minibatch loss for epoch 400: 99316.57344\n",
      "  ***   Training RMSE: 91.40091 ***\n",
      "  *** Validation RMSE: 3327.34156 ***\n",
      "Avg Minibatch loss for epoch 401: 99318.68261\n",
      "Avg Minibatch loss for epoch 402: 99237.78461\n",
      "Avg Minibatch loss for epoch 403: 99262.76822\n",
      "Avg Minibatch loss for epoch 404: 99341.17458\n",
      "Avg Minibatch loss for epoch 405: 99200.23810\n",
      "  ***   Training RMSE: 91.55859 ***\n",
      "  *** Validation RMSE: 3327.34156 ***\n",
      "Avg Minibatch loss for epoch 406: 99368.87915\n",
      "Avg Minibatch loss for epoch 407: 99295.74531\n",
      "Avg Minibatch loss for epoch 408: 99274.05854\n",
      "Avg Minibatch loss for epoch 409: 99224.03308\n",
      "Avg Minibatch loss for epoch 410: 99200.93485\n",
      "  ***   Training RMSE: 91.36744 ***\n",
      "  *** Validation RMSE: 3327.34139 ***\n",
      "Avg Minibatch loss for epoch 411: 99292.22880\n",
      "Avg Minibatch loss for epoch 412: 99276.29349\n",
      "Avg Minibatch loss for epoch 413: 99315.74366\n",
      "Avg Minibatch loss for epoch 414: 99319.15672\n",
      "Avg Minibatch loss for epoch 415: 99345.07004\n",
      "  ***   Training RMSE: 91.12551 ***\n",
      "  *** Validation RMSE: 3327.34139 ***\n",
      "Avg Minibatch loss for epoch 416: 99363.16610\n",
      "Avg Minibatch loss for epoch 417: 99181.23923\n",
      "Avg Minibatch loss for epoch 418: 99260.14183\n",
      "Avg Minibatch loss for epoch 419: 99264.26718\n",
      "Avg Minibatch loss for epoch 420: 99217.65100\n",
      "  ***   Training RMSE: 91.20572 ***\n",
      "  *** Validation RMSE: 3327.34172 ***\n",
      "Avg Minibatch loss for epoch 421: 99286.43648\n",
      "Avg Minibatch loss for epoch 422: 99231.29617\n",
      "Avg Minibatch loss for epoch 423: 99342.03862\n",
      "Avg Minibatch loss for epoch 424: 99382.31030\n",
      "Avg Minibatch loss for epoch 425: 99274.13868\n",
      "  ***   Training RMSE: 91.38667 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 426: 99239.66604\n",
      "Avg Minibatch loss for epoch 427: 99347.40816\n",
      "Avg Minibatch loss for epoch 428: 99317.02319\n",
      "Avg Minibatch loss for epoch 429: 99273.24218\n",
      "Avg Minibatch loss for epoch 430: 99259.48563\n",
      "  ***   Training RMSE: 91.07431 ***\n",
      "  *** Validation RMSE: 3327.34139 ***\n",
      "Avg Minibatch loss for epoch 431: 99238.72289\n",
      "Avg Minibatch loss for epoch 432: 99307.60878\n",
      "Avg Minibatch loss for epoch 433: 99331.65357\n",
      "Avg Minibatch loss for epoch 434: 99360.63994\n",
      "Avg Minibatch loss for epoch 435: 99252.98755\n",
      "  ***   Training RMSE: 91.11827 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 436: 99233.90786\n",
      "Avg Minibatch loss for epoch 437: 99221.57551\n",
      "Avg Minibatch loss for epoch 438: 99300.07981\n",
      "Avg Minibatch loss for epoch 439: 99249.69228\n",
      "Avg Minibatch loss for epoch 440: 99250.37583\n",
      "  ***   Training RMSE: 90.91315 ***\n",
      "  *** Validation RMSE: 3327.34139 ***\n",
      "Avg Minibatch loss for epoch 441: 99325.22162\n",
      "Avg Minibatch loss for epoch 442: 99422.87440\n",
      "Avg Minibatch loss for epoch 443: 99331.37923\n",
      "Avg Minibatch loss for epoch 444: 99322.42088\n",
      "Avg Minibatch loss for epoch 445: 99313.10982\n",
      "  ***   Training RMSE: 91.48132 ***\n",
      "  *** Validation RMSE: 3327.34056 ***\n",
      "Avg Minibatch loss for epoch 446: 99341.49846\n",
      "Avg Minibatch loss for epoch 447: 99269.47186\n",
      "Avg Minibatch loss for epoch 448: 99302.61286\n",
      "Avg Minibatch loss for epoch 449: 99213.34118\n",
      "Avg Minibatch loss for epoch 450: 99301.13006\n",
      "  ***   Training RMSE: 91.59501 ***\n",
      "  *** Validation RMSE: 3327.34106 ***\n",
      "Avg Minibatch loss for epoch 451: 99365.32222\n",
      "Avg Minibatch loss for epoch 452: 99205.84197\n",
      "Avg Minibatch loss for epoch 453: 99322.20482\n",
      "Avg Minibatch loss for epoch 454: 99277.50957\n",
      "Avg Minibatch loss for epoch 455: 99346.56585\n",
      "  ***   Training RMSE: 91.10623 ***\n",
      "  *** Validation RMSE: 3327.34172 ***\n",
      "Avg Minibatch loss for epoch 456: 99247.80006\n",
      "Avg Minibatch loss for epoch 457: 99306.54587\n",
      "Avg Minibatch loss for epoch 458: 99265.46125\n",
      "Avg Minibatch loss for epoch 459: 99237.54191\n",
      "Avg Minibatch loss for epoch 460: 99223.55430\n",
      "  ***   Training RMSE: 90.77827 ***\n",
      "  *** Validation RMSE: 3327.34139 ***\n",
      "Avg Minibatch loss for epoch 461: 99263.55033\n",
      "Avg Minibatch loss for epoch 462: 99266.10654\n",
      "Avg Minibatch loss for epoch 463: 99272.26045\n",
      "Avg Minibatch loss for epoch 464: 99396.35498\n",
      "Avg Minibatch loss for epoch 465: 99240.71118\n",
      "  ***   Training RMSE: 90.86194 ***\n",
      "  *** Validation RMSE: 3327.34156 ***\n",
      "Avg Minibatch loss for epoch 466: 99244.20197\n",
      "Avg Minibatch loss for epoch 467: 99228.68872\n",
      "Avg Minibatch loss for epoch 468: 99350.57567\n",
      "Avg Minibatch loss for epoch 469: 99243.65527\n",
      "Avg Minibatch loss for epoch 470: 99287.04526\n",
      "  ***   Training RMSE: 91.05988 ***\n",
      "  *** Validation RMSE: 3327.34172 ***\n",
      "Avg Minibatch loss for epoch 471: 99320.53672\n",
      "Avg Minibatch loss for epoch 472: 99165.06016\n",
      "Avg Minibatch loss for epoch 473: 99181.79113\n",
      "Avg Minibatch loss for epoch 474: 99352.36468\n",
      "Avg Minibatch loss for epoch 475: 99303.54767\n",
      "  ***   Training RMSE: 91.04152 ***\n",
      "  *** Validation RMSE: 3327.34172 ***\n",
      "Avg Minibatch loss for epoch 476: 99238.43581\n",
      "Avg Minibatch loss for epoch 477: 99162.38085\n",
      "Avg Minibatch loss for epoch 478: 99350.28069\n",
      "Avg Minibatch loss for epoch 479: 99253.01383\n",
      "Avg Minibatch loss for epoch 480: 99242.53890\n",
      "  ***   Training RMSE: 90.71634 ***\n",
      "  *** Validation RMSE: 3327.34139 ***\n",
      "Avg Minibatch loss for epoch 481: 99350.07515\n",
      "Avg Minibatch loss for epoch 482: 99268.15223\n",
      "Avg Minibatch loss for epoch 483: 99266.93189\n",
      "Avg Minibatch loss for epoch 484: 99205.17219\n",
      "Avg Minibatch loss for epoch 485: 99275.70770\n",
      "  ***   Training RMSE: 90.84304 ***\n",
      "  *** Validation RMSE: 3327.34139 ***\n",
      "Avg Minibatch loss for epoch 486: 99275.04922\n",
      "Avg Minibatch loss for epoch 487: 99294.47392\n",
      "Avg Minibatch loss for epoch 488: 99338.79024\n",
      "Avg Minibatch loss for epoch 489: 99245.64691\n",
      "Avg Minibatch loss for epoch 490: 99308.96090\n",
      "  ***   Training RMSE: 91.16255 ***\n",
      "  *** Validation RMSE: 3327.34156 ***\n",
      "Avg Minibatch loss for epoch 491: 99294.92200\n",
      "Avg Minibatch loss for epoch 492: 99238.03436\n",
      "Avg Minibatch loss for epoch 493: 99233.39568\n",
      "Avg Minibatch loss for epoch 494: 99213.03436\n",
      "Avg Minibatch loss for epoch 495: 99381.26561\n",
      "  ***   Training RMSE: 90.75565 ***\n",
      "  *** Validation RMSE: 3327.34139 ***\n",
      "Avg Minibatch loss for epoch 496: 99223.91082\n",
      "Avg Minibatch loss for epoch 497: 99310.04411\n",
      "Avg Minibatch loss for epoch 498: 99284.64822\n",
      "Avg Minibatch loss for epoch 499: 99352.77558\n",
      "Avg Minibatch loss for epoch 500: 99278.00916\n",
      "  ***   Training RMSE: 90.41976 ***\n",
      "  *** Validation RMSE: 3327.34172 ***\n",
      "Avg Minibatch loss for epoch 501: 99177.38730\n",
      "Avg Minibatch loss for epoch 502: 99259.35127\n",
      "Avg Minibatch loss for epoch 503: 99299.27561\n",
      "Avg Minibatch loss for epoch 504: 99215.74158\n",
      "Avg Minibatch loss for epoch 505: 99268.29833\n",
      "  ***   Training RMSE: 90.62316 ***\n",
      "  *** Validation RMSE: 3327.34189 ***\n",
      "Avg Minibatch loss for epoch 506: 99365.39285\n",
      "Avg Minibatch loss for epoch 507: 99191.50974\n",
      "Avg Minibatch loss for epoch 508: 99246.75136\n",
      "Avg Minibatch loss for epoch 509: 99230.06758\n",
      "Avg Minibatch loss for epoch 510: 99252.21608\n",
      "  ***   Training RMSE: 90.65524 ***\n",
      "  *** Validation RMSE: 3327.34139 ***\n",
      "Avg Minibatch loss for epoch 511: 99298.30149\n",
      "Avg Minibatch loss for epoch 512: 99232.86561\n",
      "Avg Minibatch loss for epoch 513: 99280.43059\n",
      "Avg Minibatch loss for epoch 514: 99243.47087\n",
      "Avg Minibatch loss for epoch 515: 99317.50375\n",
      "  ***   Training RMSE: 90.81411 ***\n",
      "  *** Validation RMSE: 3327.34156 ***\n",
      "Avg Minibatch loss for epoch 516: 99312.39190\n",
      "Avg Minibatch loss for epoch 517: 99257.91874\n",
      "Avg Minibatch loss for epoch 518: 99340.46308\n",
      "Avg Minibatch loss for epoch 519: 99225.08898\n",
      "Avg Minibatch loss for epoch 520: 99299.39928\n",
      "  ***   Training RMSE: 90.53909 ***\n",
      "  *** Validation RMSE: 3327.34139 ***\n",
      "Avg Minibatch loss for epoch 521: 99292.39208\n",
      "Avg Minibatch loss for epoch 522: 99272.17013\n",
      "Avg Minibatch loss for epoch 523: 99382.86381\n",
      "Avg Minibatch loss for epoch 524: 99435.62767\n",
      "Avg Minibatch loss for epoch 525: 99132.47530\n",
      "  ***   Training RMSE: 90.91899 ***\n",
      "  *** Validation RMSE: 3327.34139 ***\n",
      "Avg Minibatch loss for epoch 526: 99319.09620\n",
      "Avg Minibatch loss for epoch 527: 99222.98029\n",
      "Avg Minibatch loss for epoch 528: 99361.16615\n",
      "Avg Minibatch loss for epoch 529: 99225.21158\n",
      "Avg Minibatch loss for epoch 530: 99391.90262\n",
      "  ***   Training RMSE: 90.92718 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 531: 99352.21491\n",
      "Avg Minibatch loss for epoch 532: 99239.96692\n",
      "Avg Minibatch loss for epoch 533: 99151.49706\n",
      "Avg Minibatch loss for epoch 534: 99213.57498\n",
      "Avg Minibatch loss for epoch 535: 99195.44106\n",
      "  ***   Training RMSE: 90.91474 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 536: 99236.96940\n",
      "Avg Minibatch loss for epoch 537: 99186.09799\n",
      "Avg Minibatch loss for epoch 538: 99262.78600\n",
      "Avg Minibatch loss for epoch 539: 99289.52444\n",
      "Avg Minibatch loss for epoch 540: 99132.84894\n",
      "  ***   Training RMSE: 90.46952 ***\n",
      "  *** Validation RMSE: 3327.34139 ***\n",
      "Avg Minibatch loss for epoch 541: 99278.02375\n",
      "Avg Minibatch loss for epoch 542: 99275.36878\n",
      "Avg Minibatch loss for epoch 543: 99288.51387\n",
      "Avg Minibatch loss for epoch 544: 99180.65813\n",
      "Avg Minibatch loss for epoch 545: 99264.36003\n",
      "  ***   Training RMSE: 90.75166 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 546: 99308.92133\n",
      "Avg Minibatch loss for epoch 547: 99302.26621\n",
      "Avg Minibatch loss for epoch 548: 99306.94982\n",
      "Avg Minibatch loss for epoch 549: 99224.43760\n",
      "Avg Minibatch loss for epoch 550: 99281.83911\n",
      "  ***   Training RMSE: 90.71710 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 551: 99239.41653\n",
      "Avg Minibatch loss for epoch 552: 99273.06660\n",
      "Avg Minibatch loss for epoch 553: 99162.62236\n",
      "Avg Minibatch loss for epoch 554: 99269.72408\n",
      "Avg Minibatch loss for epoch 555: 99229.98258\n",
      "  ***   Training RMSE: 90.55177 ***\n",
      "  *** Validation RMSE: 3327.34139 ***\n",
      "Avg Minibatch loss for epoch 556: 99197.71953\n",
      "Avg Minibatch loss for epoch 557: 99301.49434\n",
      "Avg Minibatch loss for epoch 558: 99181.69274\n",
      "Avg Minibatch loss for epoch 559: 99192.78597\n",
      "Avg Minibatch loss for epoch 560: 99284.47792\n",
      "  ***   Training RMSE: 90.88084 ***\n",
      "  *** Validation RMSE: 3327.34139 ***\n",
      "Avg Minibatch loss for epoch 561: 99262.97578\n",
      "Avg Minibatch loss for epoch 562: 99361.59706\n",
      "Avg Minibatch loss for epoch 563: 99289.39523\n",
      "Avg Minibatch loss for epoch 564: 99297.00937\n",
      "Avg Minibatch loss for epoch 565: 99318.79680\n",
      "  ***   Training RMSE: 90.83562 ***\n",
      "  *** Validation RMSE: 3327.34106 ***\n",
      "Avg Minibatch loss for epoch 566: 99231.15707\n",
      "Avg Minibatch loss for epoch 567: 99157.83455\n",
      "Avg Minibatch loss for epoch 568: 99232.22084\n",
      "Avg Minibatch loss for epoch 569: 99208.08533\n",
      "Avg Minibatch loss for epoch 570: 99241.39067\n",
      "  ***   Training RMSE: 90.70264 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 571: 99298.15259\n",
      "Avg Minibatch loss for epoch 572: 99279.59563\n",
      "Avg Minibatch loss for epoch 573: 99229.01992\n",
      "Avg Minibatch loss for epoch 574: 99111.78013\n",
      "Avg Minibatch loss for epoch 575: 99264.71017\n",
      "  ***   Training RMSE: 89.88341 ***\n",
      "  *** Validation RMSE: 3327.34139 ***\n",
      "Avg Minibatch loss for epoch 576: 99216.56219\n",
      "Avg Minibatch loss for epoch 577: 99357.21300\n",
      "Avg Minibatch loss for epoch 578: 99204.18275\n",
      "Avg Minibatch loss for epoch 579: 99311.98386\n",
      "Avg Minibatch loss for epoch 580: 99177.61854\n",
      "  ***   Training RMSE: 90.78310 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 581: 99243.96156\n",
      "Avg Minibatch loss for epoch 582: 99216.65752\n",
      "Avg Minibatch loss for epoch 583: 99281.75233\n",
      "Avg Minibatch loss for epoch 584: 99263.97949\n",
      "Avg Minibatch loss for epoch 585: 99118.78785\n",
      "  ***   Training RMSE: 90.28845 ***\n",
      "  *** Validation RMSE: 3327.34172 ***\n",
      "Avg Minibatch loss for epoch 586: 99180.60669\n",
      "Avg Minibatch loss for epoch 587: 99193.45223\n",
      "Avg Minibatch loss for epoch 588: 99166.21186\n",
      "Avg Minibatch loss for epoch 589: 99287.82833\n",
      "Avg Minibatch loss for epoch 590: 99309.56811\n",
      "  ***   Training RMSE: 90.55914 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 591: 99194.43850\n",
      "Avg Minibatch loss for epoch 592: 99289.12217\n",
      "Avg Minibatch loss for epoch 593: 99273.29871\n",
      "Avg Minibatch loss for epoch 594: 99240.91865\n",
      "Avg Minibatch loss for epoch 595: 99207.98903\n",
      "  ***   Training RMSE: 90.26994 ***\n",
      "  *** Validation RMSE: 3327.34139 ***\n",
      "Avg Minibatch loss for epoch 596: 99344.44114\n",
      "Avg Minibatch loss for epoch 597: 99206.94392\n",
      "Avg Minibatch loss for epoch 598: 99224.70302\n",
      "Avg Minibatch loss for epoch 599: 99284.32579\n",
      "Avg Minibatch loss for epoch 600: 99181.11676\n",
      "  ***   Training RMSE: 90.34115 ***\n",
      "  *** Validation RMSE: 3327.34156 ***\n",
      "Avg Minibatch loss for epoch 601: 99299.94991\n",
      "Avg Minibatch loss for epoch 602: 99275.65483\n",
      "Avg Minibatch loss for epoch 603: 99278.65353\n",
      "Avg Minibatch loss for epoch 604: 99242.81606\n",
      "Avg Minibatch loss for epoch 605: 99289.85504\n",
      "  ***   Training RMSE: 90.44688 ***\n",
      "  *** Validation RMSE: 3327.34156 ***\n",
      "Avg Minibatch loss for epoch 606: 99150.95563\n",
      "Avg Minibatch loss for epoch 607: 99191.64732\n",
      "Avg Minibatch loss for epoch 608: 99209.69080\n",
      "Avg Minibatch loss for epoch 609: 99185.97719\n",
      "Avg Minibatch loss for epoch 610: 99243.15047\n",
      "  ***   Training RMSE: 90.73596 ***\n",
      "  *** Validation RMSE: 3327.34139 ***\n",
      "Avg Minibatch loss for epoch 611: 99149.05880\n",
      "Avg Minibatch loss for epoch 612: 99272.95768\n",
      "Avg Minibatch loss for epoch 613: 99218.62912\n",
      "Avg Minibatch loss for epoch 614: 99256.47058\n",
      "Avg Minibatch loss for epoch 615: 99118.71026\n",
      "  ***   Training RMSE: 90.13491 ***\n",
      "  *** Validation RMSE: 3327.34139 ***\n",
      "Avg Minibatch loss for epoch 616: 99149.91428\n",
      "Avg Minibatch loss for epoch 617: 99279.42816\n",
      "Avg Minibatch loss for epoch 618: 99219.14359\n",
      "Avg Minibatch loss for epoch 619: 99247.83505\n",
      "Avg Minibatch loss for epoch 620: 99315.06960\n",
      "  ***   Training RMSE: 90.19663 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 621: 99200.77327\n",
      "Avg Minibatch loss for epoch 622: 99315.78113\n",
      "Avg Minibatch loss for epoch 623: 99216.78797\n",
      "Avg Minibatch loss for epoch 624: 99238.03839\n",
      "Avg Minibatch loss for epoch 625: 99213.47866\n",
      "  ***   Training RMSE: 90.34262 ***\n",
      "  *** Validation RMSE: 3327.34139 ***\n",
      "Avg Minibatch loss for epoch 626: 99217.36547\n",
      "Avg Minibatch loss for epoch 627: 99197.81447\n",
      "Avg Minibatch loss for epoch 628: 99212.93528\n",
      "Avg Minibatch loss for epoch 629: 99212.89112\n",
      "Avg Minibatch loss for epoch 630: 99312.23972\n",
      "  ***   Training RMSE: 90.40641 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 631: 99200.24879\n",
      "Avg Minibatch loss for epoch 632: 99167.11935\n",
      "Avg Minibatch loss for epoch 633: 99264.72214\n",
      "Avg Minibatch loss for epoch 634: 99204.67417\n",
      "Avg Minibatch loss for epoch 635: 99251.98063\n",
      "  ***   Training RMSE: 90.48334 ***\n",
      "  *** Validation RMSE: 3327.34156 ***\n",
      "Avg Minibatch loss for epoch 636: 99256.52403\n",
      "Avg Minibatch loss for epoch 637: 99364.25436\n",
      "Avg Minibatch loss for epoch 638: 99273.86177\n",
      "Avg Minibatch loss for epoch 639: 99303.87361\n",
      "Avg Minibatch loss for epoch 640: 99361.07315\n",
      "  ***   Training RMSE: 90.96097 ***\n",
      "  *** Validation RMSE: 3327.34106 ***\n",
      "Avg Minibatch loss for epoch 641: 99163.96386\n",
      "Avg Minibatch loss for epoch 642: 99344.32235\n",
      "Avg Minibatch loss for epoch 643: 99202.11810\n",
      "Avg Minibatch loss for epoch 644: 99180.47222\n",
      "Avg Minibatch loss for epoch 645: 99178.32402\n",
      "  ***   Training RMSE: 90.43726 ***\n",
      "  *** Validation RMSE: 3327.34139 ***\n",
      "Avg Minibatch loss for epoch 646: 99175.16242\n",
      "Avg Minibatch loss for epoch 647: 99341.78428\n",
      "Avg Minibatch loss for epoch 648: 99213.47380\n",
      "Avg Minibatch loss for epoch 649: 99216.99661\n",
      "Avg Minibatch loss for epoch 650: 99219.95143\n",
      "  ***   Training RMSE: 90.38480 ***\n",
      "  *** Validation RMSE: 3327.34156 ***\n",
      "Avg Minibatch loss for epoch 651: 99327.23509\n",
      "Avg Minibatch loss for epoch 652: 99223.85283\n",
      "Avg Minibatch loss for epoch 653: 99249.22510\n",
      "Avg Minibatch loss for epoch 654: 99328.15665\n",
      "Avg Minibatch loss for epoch 655: 99289.17996\n",
      "  ***   Training RMSE: 90.27881 ***\n",
      "  *** Validation RMSE: 3327.34139 ***\n",
      "Avg Minibatch loss for epoch 656: 99199.86146\n",
      "Avg Minibatch loss for epoch 657: 99165.91458\n",
      "Avg Minibatch loss for epoch 658: 99170.73803\n",
      "Avg Minibatch loss for epoch 659: 99116.59916\n",
      "Avg Minibatch loss for epoch 660: 99200.82036\n",
      "  ***   Training RMSE: 90.66631 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 661: 99282.41250\n",
      "Avg Minibatch loss for epoch 662: 99243.13857\n",
      "Avg Minibatch loss for epoch 663: 99176.38020\n",
      "Avg Minibatch loss for epoch 664: 99174.22202\n",
      "Avg Minibatch loss for epoch 665: 99201.23193\n",
      "  ***   Training RMSE: 90.18016 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 666: 99170.13140\n",
      "Avg Minibatch loss for epoch 667: 99151.26434\n",
      "Avg Minibatch loss for epoch 668: 99188.82441\n",
      "Avg Minibatch loss for epoch 669: 99114.35011\n",
      "Avg Minibatch loss for epoch 670: 99223.28976\n",
      "  ***   Training RMSE: 90.64154 ***\n",
      "  *** Validation RMSE: 3327.34156 ***\n",
      "Avg Minibatch loss for epoch 671: 99173.29028\n",
      "Avg Minibatch loss for epoch 672: 99163.24772\n",
      "Avg Minibatch loss for epoch 673: 99157.28226\n",
      "Avg Minibatch loss for epoch 674: 99182.60414\n",
      "Avg Minibatch loss for epoch 675: 99209.17432\n",
      "  ***   Training RMSE: 90.41013 ***\n",
      "  *** Validation RMSE: 3327.34156 ***\n",
      "Avg Minibatch loss for epoch 676: 99221.03315\n",
      "Avg Minibatch loss for epoch 677: 99283.68217\n",
      "Avg Minibatch loss for epoch 678: 99218.12987\n",
      "Avg Minibatch loss for epoch 679: 99225.89374\n",
      "Avg Minibatch loss for epoch 680: 99214.98510\n",
      "  ***   Training RMSE: 90.32721 ***\n",
      "  *** Validation RMSE: 3327.34139 ***\n",
      "Avg Minibatch loss for epoch 681: 99178.29877\n",
      "Avg Minibatch loss for epoch 682: 99209.19219\n",
      "Avg Minibatch loss for epoch 683: 99281.99832\n",
      "Avg Minibatch loss for epoch 684: 99147.37244\n",
      "Avg Minibatch loss for epoch 685: 99137.35480\n",
      "  ***   Training RMSE: 90.32867 ***\n",
      "  *** Validation RMSE: 3327.34139 ***\n",
      "Avg Minibatch loss for epoch 686: 99202.54896\n",
      "Avg Minibatch loss for epoch 687: 99215.92509\n",
      "Avg Minibatch loss for epoch 688: 99213.52845\n",
      "Avg Minibatch loss for epoch 689: 99236.22298\n",
      "Avg Minibatch loss for epoch 690: 99120.82077\n",
      "  ***   Training RMSE: 90.25864 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 691: 99286.31595\n",
      "Avg Minibatch loss for epoch 692: 99252.32487\n",
      "Avg Minibatch loss for epoch 693: 99156.94331\n",
      "Avg Minibatch loss for epoch 694: 99166.03163\n",
      "Avg Minibatch loss for epoch 695: 99288.99508\n",
      "  ***   Training RMSE: 90.23032 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 696: 99338.15781\n",
      "Avg Minibatch loss for epoch 697: 99209.73888\n",
      "Avg Minibatch loss for epoch 698: 99147.61119\n",
      "Avg Minibatch loss for epoch 699: 99106.14251\n",
      "Avg Minibatch loss for epoch 700: 99232.11418\n",
      "  ***   Training RMSE: 89.66855 ***\n",
      "  *** Validation RMSE: 3327.34139 ***\n",
      "Avg Minibatch loss for epoch 701: 99235.08411\n",
      "Avg Minibatch loss for epoch 702: 99270.87496\n",
      "Avg Minibatch loss for epoch 703: 99173.38233\n",
      "Avg Minibatch loss for epoch 704: 99213.92990\n",
      "Avg Minibatch loss for epoch 705: 99268.64817\n",
      "  ***   Training RMSE: 89.75949 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 706: 99228.94445\n",
      "Avg Minibatch loss for epoch 707: 99091.92436\n",
      "Avg Minibatch loss for epoch 708: 99268.14597\n",
      "Avg Minibatch loss for epoch 709: 99152.86464\n",
      "Avg Minibatch loss for epoch 710: 99006.09478\n",
      "  ***   Training RMSE: 90.11856 ***\n",
      "  *** Validation RMSE: 3327.34139 ***\n",
      "Avg Minibatch loss for epoch 711: 99237.98875\n",
      "Avg Minibatch loss for epoch 712: 99337.82327\n",
      "Avg Minibatch loss for epoch 713: 99245.61175\n",
      "Avg Minibatch loss for epoch 714: 99241.55217\n",
      "Avg Minibatch loss for epoch 715: 99376.08494\n",
      "  ***   Training RMSE: 90.22045 ***\n",
      "  *** Validation RMSE: 3327.34089 ***\n",
      "Avg Minibatch loss for epoch 716: 99195.37305\n",
      "Avg Minibatch loss for epoch 717: 99233.47938\n",
      "Avg Minibatch loss for epoch 718: 99205.16484\n",
      "Avg Minibatch loss for epoch 719: 99253.96436\n",
      "Avg Minibatch loss for epoch 720: 99211.54277\n",
      "  ***   Training RMSE: 90.13561 ***\n",
      "  *** Validation RMSE: 3327.34139 ***\n",
      "Avg Minibatch loss for epoch 721: 99281.02668\n",
      "Avg Minibatch loss for epoch 722: 99161.34378\n",
      "Avg Minibatch loss for epoch 723: 99232.32183\n",
      "Avg Minibatch loss for epoch 724: 99275.13626\n",
      "Avg Minibatch loss for epoch 725: 99164.38379\n",
      "  ***   Training RMSE: 90.23264 ***\n",
      "  *** Validation RMSE: 3327.34139 ***\n",
      "Avg Minibatch loss for epoch 726: 99309.90625\n",
      "Avg Minibatch loss for epoch 727: 99212.64730\n",
      "Avg Minibatch loss for epoch 728: 99112.15140\n",
      "Avg Minibatch loss for epoch 729: 99198.75625\n",
      "Avg Minibatch loss for epoch 730: 99132.81838\n",
      "  ***   Training RMSE: 90.20165 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 731: 99174.00707\n",
      "Avg Minibatch loss for epoch 732: 99279.46908\n",
      "Avg Minibatch loss for epoch 733: 99095.44980\n",
      "Avg Minibatch loss for epoch 734: 99172.61297\n",
      "Avg Minibatch loss for epoch 735: 99137.78896\n",
      "  ***   Training RMSE: 90.05193 ***\n",
      "  *** Validation RMSE: 3327.34156 ***\n",
      "Avg Minibatch loss for epoch 736: 99092.26332\n",
      "Avg Minibatch loss for epoch 737: 99120.14957\n",
      "Avg Minibatch loss for epoch 738: 99227.79872\n",
      "Avg Minibatch loss for epoch 739: 99273.36677\n",
      "Avg Minibatch loss for epoch 740: 99154.35456\n",
      "  ***   Training RMSE: 89.87112 ***\n",
      "  *** Validation RMSE: 3327.34156 ***\n",
      "Avg Minibatch loss for epoch 741: 99155.85581\n",
      "Avg Minibatch loss for epoch 742: 99204.81430\n",
      "Avg Minibatch loss for epoch 743: 99167.95830\n",
      "Avg Minibatch loss for epoch 744: 99161.49125\n",
      "Avg Minibatch loss for epoch 745: 99238.02993\n",
      "  ***   Training RMSE: 90.11656 ***\n",
      "  *** Validation RMSE: 3327.34156 ***\n",
      "Avg Minibatch loss for epoch 746: 99237.09282\n",
      "Avg Minibatch loss for epoch 747: 99198.78136\n",
      "Avg Minibatch loss for epoch 748: 99129.37572\n",
      "Avg Minibatch loss for epoch 749: 99260.52393\n",
      "Avg Minibatch loss for epoch 750: 99272.64142\n",
      "  ***   Training RMSE: 90.17109 ***\n",
      "  *** Validation RMSE: 3327.34139 ***\n",
      "Avg Minibatch loss for epoch 751: 99216.00984\n",
      "Avg Minibatch loss for epoch 752: 99211.95180\n",
      "Avg Minibatch loss for epoch 753: 99209.12397\n",
      "Avg Minibatch loss for epoch 754: 99226.03955\n",
      "Avg Minibatch loss for epoch 755: 99189.29298\n",
      "  ***   Training RMSE: 89.88986 ***\n",
      "  *** Validation RMSE: 3327.34139 ***\n",
      "Avg Minibatch loss for epoch 756: 99150.52825\n",
      "Avg Minibatch loss for epoch 757: 99216.76556\n",
      "Avg Minibatch loss for epoch 758: 99238.04403\n",
      "Avg Minibatch loss for epoch 759: 99108.70862\n",
      "Avg Minibatch loss for epoch 760: 99180.61600\n",
      "  ***   Training RMSE: 89.95215 ***\n",
      "  *** Validation RMSE: 3327.34139 ***\n",
      "Avg Minibatch loss for epoch 761: 99209.34683\n",
      "Avg Minibatch loss for epoch 762: 99200.27109\n",
      "Avg Minibatch loss for epoch 763: 99083.65522\n",
      "Avg Minibatch loss for epoch 764: 99223.37529\n",
      "Avg Minibatch loss for epoch 765: 99140.39585\n",
      "  ***   Training RMSE: 89.79928 ***\n",
      "  *** Validation RMSE: 3327.34139 ***\n",
      "Avg Minibatch loss for epoch 766: 99335.41355\n",
      "Avg Minibatch loss for epoch 767: 99158.98037\n",
      "Avg Minibatch loss for epoch 768: 99135.98293\n",
      "Avg Minibatch loss for epoch 769: 99116.22424\n",
      "Avg Minibatch loss for epoch 770: 99201.81394\n",
      "  ***   Training RMSE: 89.85108 ***\n",
      "  *** Validation RMSE: 3327.34139 ***\n",
      "Avg Minibatch loss for epoch 771: 99142.89835\n",
      "Avg Minibatch loss for epoch 772: 99191.40365\n",
      "Avg Minibatch loss for epoch 773: 99238.62510\n",
      "Avg Minibatch loss for epoch 774: 99161.12914\n",
      "Avg Minibatch loss for epoch 775: 99066.90966\n",
      "  ***   Training RMSE: 89.73443 ***\n",
      "  *** Validation RMSE: 3327.34139 ***\n",
      "Avg Minibatch loss for epoch 776: 99178.23357\n",
      "Avg Minibatch loss for epoch 777: 99220.70262\n",
      "Avg Minibatch loss for epoch 778: 99164.06086\n",
      "Avg Minibatch loss for epoch 779: 99180.10112\n",
      "Avg Minibatch loss for epoch 780: 99180.08458\n",
      "  ***   Training RMSE: 89.68746 ***\n",
      "  *** Validation RMSE: 3327.34156 ***\n",
      "Avg Minibatch loss for epoch 781: 99198.07780\n",
      "Avg Minibatch loss for epoch 782: 99195.47548\n",
      "Avg Minibatch loss for epoch 783: 99184.39313\n",
      "Avg Minibatch loss for epoch 784: 99130.20179\n",
      "Avg Minibatch loss for epoch 785: 99205.78889\n",
      "  ***   Training RMSE: 89.76753 ***\n",
      "  *** Validation RMSE: 3327.34156 ***\n",
      "Avg Minibatch loss for epoch 786: 99244.59995\n",
      "Avg Minibatch loss for epoch 787: 99276.53167\n",
      "Avg Minibatch loss for epoch 788: 99115.52473\n",
      "Avg Minibatch loss for epoch 789: 99161.35429\n",
      "Avg Minibatch loss for epoch 790: 99281.42555\n",
      "  ***   Training RMSE: 89.79972 ***\n",
      "  *** Validation RMSE: 3327.34156 ***\n",
      "Avg Minibatch loss for epoch 791: 99333.58145\n",
      "Avg Minibatch loss for epoch 792: 99184.04898\n",
      "Avg Minibatch loss for epoch 793: 99187.67173\n",
      "Avg Minibatch loss for epoch 794: 99227.77814\n",
      "Avg Minibatch loss for epoch 795: 99262.68550\n",
      "  ***   Training RMSE: 89.87323 ***\n",
      "  *** Validation RMSE: 3327.34139 ***\n",
      "Avg Minibatch loss for epoch 796: 99193.68669\n",
      "Avg Minibatch loss for epoch 797: 99143.50996\n",
      "Avg Minibatch loss for epoch 798: 99272.16836\n",
      "Avg Minibatch loss for epoch 799: 99147.59511\n",
      "Avg Minibatch loss for epoch 800: 99126.86641\n",
      "  ***   Training RMSE: 90.14758 ***\n",
      "  *** Validation RMSE: 3327.34139 ***\n",
      "Avg Minibatch loss for epoch 801: 99175.61821\n",
      "Avg Minibatch loss for epoch 802: 99187.36263\n",
      "Avg Minibatch loss for epoch 803: 99165.71629\n",
      "Avg Minibatch loss for epoch 804: 99216.95920\n",
      "Avg Minibatch loss for epoch 805: 99207.79714\n",
      "  ***   Training RMSE: 89.46561 ***\n",
      "  *** Validation RMSE: 3327.34156 ***\n",
      "Avg Minibatch loss for epoch 806: 99324.51936\n",
      "Avg Minibatch loss for epoch 807: 99133.65099\n",
      "Avg Minibatch loss for epoch 808: 99124.14579\n",
      "Avg Minibatch loss for epoch 809: 99214.16291\n",
      "Avg Minibatch loss for epoch 810: 99155.51243\n",
      "  ***   Training RMSE: 89.83143 ***\n",
      "  *** Validation RMSE: 3327.34139 ***\n",
      "Avg Minibatch loss for epoch 811: 99180.40603\n",
      "Avg Minibatch loss for epoch 812: 99190.07105\n",
      "Avg Minibatch loss for epoch 813: 99225.48062\n",
      "Avg Minibatch loss for epoch 814: 99108.33574\n",
      "Avg Minibatch loss for epoch 815: 99172.93691\n",
      "  ***   Training RMSE: 89.70842 ***\n",
      "  *** Validation RMSE: 3327.34156 ***\n",
      "Avg Minibatch loss for epoch 816: 99226.52969\n",
      "Avg Minibatch loss for epoch 817: 99087.20770\n",
      "Avg Minibatch loss for epoch 818: 99245.06337\n",
      "Avg Minibatch loss for epoch 819: 99257.01901\n",
      "Avg Minibatch loss for epoch 820: 99170.63312\n",
      "  ***   Training RMSE: 89.71615 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 821: 99124.50108\n",
      "Avg Minibatch loss for epoch 822: 99176.22079\n",
      "Avg Minibatch loss for epoch 823: 99135.86898\n",
      "Avg Minibatch loss for epoch 824: 99176.96115\n",
      "Avg Minibatch loss for epoch 825: 99271.14601\n",
      "  ***   Training RMSE: 89.57051 ***\n",
      "  *** Validation RMSE: 3327.34139 ***\n",
      "Avg Minibatch loss for epoch 826: 99177.79454\n",
      "Avg Minibatch loss for epoch 827: 99201.82152\n",
      "Avg Minibatch loss for epoch 828: 99229.83347\n",
      "Avg Minibatch loss for epoch 829: 99131.86282\n",
      "Avg Minibatch loss for epoch 830: 99148.23439\n",
      "  ***   Training RMSE: 89.54642 ***\n",
      "  *** Validation RMSE: 3327.34139 ***\n",
      "Avg Minibatch loss for epoch 831: 99203.22691\n",
      "Avg Minibatch loss for epoch 832: 99227.89580\n",
      "Avg Minibatch loss for epoch 833: 99049.83511\n",
      "Avg Minibatch loss for epoch 834: 99182.54629\n",
      "Avg Minibatch loss for epoch 835: 99182.29051\n",
      "  ***   Training RMSE: 89.96741 ***\n",
      "  *** Validation RMSE: 3327.34139 ***\n",
      "Avg Minibatch loss for epoch 836: 99223.56257\n",
      "Avg Minibatch loss for epoch 837: 99299.28015\n",
      "Avg Minibatch loss for epoch 838: 99168.66779\n",
      "Avg Minibatch loss for epoch 839: 99211.50969\n",
      "Avg Minibatch loss for epoch 840: 99140.11039\n",
      "  ***   Training RMSE: 89.68744 ***\n",
      "  *** Validation RMSE: 3327.34156 ***\n",
      "Avg Minibatch loss for epoch 841: 99121.57113\n",
      "Avg Minibatch loss for epoch 842: 99236.66888\n",
      "Avg Minibatch loss for epoch 843: 99210.11409\n",
      "Avg Minibatch loss for epoch 844: 99177.46464\n",
      "Avg Minibatch loss for epoch 845: 99193.60327\n",
      "  ***   Training RMSE: 89.99084 ***\n",
      "  *** Validation RMSE: 3327.34139 ***\n",
      "Avg Minibatch loss for epoch 846: 99168.09115\n",
      "Avg Minibatch loss for epoch 847: 99145.74990\n",
      "Avg Minibatch loss for epoch 848: 99208.19423\n",
      "Avg Minibatch loss for epoch 849: 99265.13324\n",
      "Avg Minibatch loss for epoch 850: 99217.29769\n",
      "  ***   Training RMSE: 89.44425 ***\n",
      "  *** Validation RMSE: 3327.34156 ***\n",
      "Avg Minibatch loss for epoch 851: 99091.22446\n",
      "Avg Minibatch loss for epoch 852: 99119.15251\n",
      "Avg Minibatch loss for epoch 853: 99124.51176\n",
      "Avg Minibatch loss for epoch 854: 99190.12938\n",
      "Avg Minibatch loss for epoch 855: 99190.39794\n",
      "  ***   Training RMSE: 89.51293 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 856: 99167.39903\n",
      "Avg Minibatch loss for epoch 857: 99144.94349\n",
      "Avg Minibatch loss for epoch 858: 99260.14717\n",
      "Avg Minibatch loss for epoch 859: 99131.24645\n",
      "Avg Minibatch loss for epoch 860: 99164.15008\n",
      "  ***   Training RMSE: 89.62350 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 861: 99209.51844\n",
      "Avg Minibatch loss for epoch 862: 99072.51009\n",
      "Avg Minibatch loss for epoch 863: 99266.28079\n",
      "Avg Minibatch loss for epoch 864: 99149.48749\n",
      "Avg Minibatch loss for epoch 865: 99223.14785\n",
      "  ***   Training RMSE: 89.71608 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 866: 99195.99788\n",
      "Avg Minibatch loss for epoch 867: 99182.93297\n",
      "Avg Minibatch loss for epoch 868: 99180.45188\n",
      "Avg Minibatch loss for epoch 869: 99126.41645\n",
      "Avg Minibatch loss for epoch 870: 99291.38537\n",
      "  ***   Training RMSE: 89.36303 ***\n",
      "  *** Validation RMSE: 3327.34156 ***\n",
      "Avg Minibatch loss for epoch 871: 99221.81912\n",
      "Avg Minibatch loss for epoch 872: 99196.05797\n",
      "Avg Minibatch loss for epoch 873: 99187.13877\n",
      "Avg Minibatch loss for epoch 874: 99137.04622\n",
      "Avg Minibatch loss for epoch 875: 99221.74252\n",
      "  ***   Training RMSE: 89.67779 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 876: 99228.60421\n",
      "Avg Minibatch loss for epoch 877: 99334.69630\n",
      "Avg Minibatch loss for epoch 878: 99160.78111\n",
      "Avg Minibatch loss for epoch 879: 99170.93251\n",
      "Avg Minibatch loss for epoch 880: 99187.57286\n",
      "  ***   Training RMSE: 89.93955 ***\n",
      "  *** Validation RMSE: 3327.34139 ***\n",
      "Avg Minibatch loss for epoch 881: 99191.56060\n",
      "Avg Minibatch loss for epoch 882: 99236.97329\n",
      "Avg Minibatch loss for epoch 883: 99176.23177\n",
      "Avg Minibatch loss for epoch 884: 99109.80585\n",
      "Avg Minibatch loss for epoch 885: 99179.15662\n",
      "  ***   Training RMSE: 89.90401 ***\n",
      "  *** Validation RMSE: 3327.34106 ***\n",
      "Avg Minibatch loss for epoch 886: 99165.95839\n",
      "Avg Minibatch loss for epoch 887: 99183.10067\n",
      "Avg Minibatch loss for epoch 888: 99164.54621\n",
      "Avg Minibatch loss for epoch 889: 99056.03074\n",
      "Avg Minibatch loss for epoch 890: 99191.19214\n",
      "  ***   Training RMSE: 89.65192 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 891: 99232.83099\n",
      "Avg Minibatch loss for epoch 892: 99182.49541\n",
      "Avg Minibatch loss for epoch 893: 99205.83424\n",
      "Avg Minibatch loss for epoch 894: 99137.48515\n",
      "Avg Minibatch loss for epoch 895: 99143.70762\n",
      "  ***   Training RMSE: 89.55241 ***\n",
      "  *** Validation RMSE: 3327.34106 ***\n",
      "Avg Minibatch loss for epoch 896: 99180.87413\n",
      "Avg Minibatch loss for epoch 897: 99139.04315\n",
      "Avg Minibatch loss for epoch 898: 99248.93696\n",
      "Avg Minibatch loss for epoch 899: 99227.63904\n",
      "Avg Minibatch loss for epoch 900: 99169.00756\n",
      "  ***   Training RMSE: 89.70171 ***\n",
      "  *** Validation RMSE: 3327.34106 ***\n",
      "Avg Minibatch loss for epoch 901: 99155.23719\n",
      "Avg Minibatch loss for epoch 902: 99168.47624\n",
      "Avg Minibatch loss for epoch 903: 99153.87704\n",
      "Avg Minibatch loss for epoch 904: 99110.88038\n",
      "Avg Minibatch loss for epoch 905: 99213.35968\n",
      "  ***   Training RMSE: 89.96836 ***\n",
      "  *** Validation RMSE: 3327.34089 ***\n",
      "Avg Minibatch loss for epoch 906: 99081.86440\n",
      "Avg Minibatch loss for epoch 907: 99298.87618\n",
      "Avg Minibatch loss for epoch 908: 99204.05980\n",
      "Avg Minibatch loss for epoch 909: 99217.21980\n",
      "Avg Minibatch loss for epoch 910: 99208.07986\n",
      "  ***   Training RMSE: 89.55707 ***\n",
      "  *** Validation RMSE: 3327.34156 ***\n",
      "Avg Minibatch loss for epoch 911: 99217.43748\n",
      "Avg Minibatch loss for epoch 912: 99116.58801\n",
      "Avg Minibatch loss for epoch 913: 99161.94914\n",
      "Avg Minibatch loss for epoch 914: 99105.87335\n",
      "Avg Minibatch loss for epoch 915: 99134.01811\n",
      "  ***   Training RMSE: 89.17815 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 916: 99217.90185\n",
      "Avg Minibatch loss for epoch 917: 99181.54709\n",
      "Avg Minibatch loss for epoch 918: 99163.35035\n",
      "Avg Minibatch loss for epoch 919: 99168.82639\n",
      "Avg Minibatch loss for epoch 920: 99171.73152\n",
      "  ***   Training RMSE: 89.48113 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 921: 99213.15837\n",
      "Avg Minibatch loss for epoch 922: 99200.17703\n",
      "Avg Minibatch loss for epoch 923: 99155.88433\n",
      "Avg Minibatch loss for epoch 924: 99241.59247\n",
      "Avg Minibatch loss for epoch 925: 99215.95219\n",
      "  ***   Training RMSE: 89.50487 ***\n",
      "  *** Validation RMSE: 3327.34139 ***\n",
      "Avg Minibatch loss for epoch 926: 99027.13834\n",
      "Avg Minibatch loss for epoch 927: 99070.43844\n",
      "Avg Minibatch loss for epoch 928: 99151.07021\n",
      "Avg Minibatch loss for epoch 929: 99233.62746\n",
      "Avg Minibatch loss for epoch 930: 99117.14537\n",
      "  ***   Training RMSE: 89.91453 ***\n",
      "  *** Validation RMSE: 3327.34106 ***\n",
      "Avg Minibatch loss for epoch 931: 99127.17520\n",
      "Avg Minibatch loss for epoch 932: 99153.99436\n",
      "Avg Minibatch loss for epoch 933: 99160.73024\n",
      "Avg Minibatch loss for epoch 934: 99338.72199\n",
      "Avg Minibatch loss for epoch 935: 99111.96082\n",
      "  ***   Training RMSE: 89.79740 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 936: 99144.98227\n",
      "Avg Minibatch loss for epoch 937: 99211.56481\n",
      "Avg Minibatch loss for epoch 938: 99159.15227\n",
      "Avg Minibatch loss for epoch 939: 99156.46813\n",
      "Avg Minibatch loss for epoch 940: 99178.96640\n",
      "  ***   Training RMSE: 89.62571 ***\n",
      "  *** Validation RMSE: 3327.34139 ***\n",
      "Avg Minibatch loss for epoch 941: 99157.16950\n",
      "Avg Minibatch loss for epoch 942: 99171.06394\n",
      "Avg Minibatch loss for epoch 943: 99207.20886\n",
      "Avg Minibatch loss for epoch 944: 99186.84826\n",
      "Avg Minibatch loss for epoch 945: 99089.91636\n",
      "  ***   Training RMSE: 89.57768 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 946: 99201.40027\n",
      "Avg Minibatch loss for epoch 947: 99099.09382\n",
      "Avg Minibatch loss for epoch 948: 99189.41593\n",
      "Avg Minibatch loss for epoch 949: 99145.97378\n",
      "Avg Minibatch loss for epoch 950: 99151.06379\n",
      "  ***   Training RMSE: 89.21735 ***\n",
      "  *** Validation RMSE: 3327.34139 ***\n",
      "Avg Minibatch loss for epoch 951: 99213.81998\n",
      "Avg Minibatch loss for epoch 952: 99240.98300\n",
      "Avg Minibatch loss for epoch 953: 99232.73190\n",
      "Avg Minibatch loss for epoch 954: 99142.24107\n",
      "Avg Minibatch loss for epoch 955: 99090.20567\n",
      "  ***   Training RMSE: 89.68569 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 956: 99194.39469\n",
      "Avg Minibatch loss for epoch 957: 99100.23072\n",
      "Avg Minibatch loss for epoch 958: 99186.46632\n",
      "Avg Minibatch loss for epoch 959: 99161.19869\n",
      "Avg Minibatch loss for epoch 960: 99089.19133\n",
      "  ***   Training RMSE: 90.00310 ***\n",
      "  *** Validation RMSE: 3327.34156 ***\n",
      "Avg Minibatch loss for epoch 961: 99174.17660\n",
      "Avg Minibatch loss for epoch 962: 99222.52954\n",
      "Avg Minibatch loss for epoch 963: 99176.09264\n",
      "Avg Minibatch loss for epoch 964: 99115.05242\n",
      "Avg Minibatch loss for epoch 965: 99133.18113\n",
      "  ***   Training RMSE: 90.17719 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 966: 99150.41092\n",
      "Avg Minibatch loss for epoch 967: 99135.23545\n",
      "Avg Minibatch loss for epoch 968: 99145.34893\n",
      "Avg Minibatch loss for epoch 969: 99202.68505\n",
      "Avg Minibatch loss for epoch 970: 99319.66199\n",
      "  ***   Training RMSE: 90.16068 ***\n",
      "  *** Validation RMSE: 3327.34106 ***\n",
      "Avg Minibatch loss for epoch 971: 99066.79467\n",
      "Avg Minibatch loss for epoch 972: 99044.15909\n",
      "Avg Minibatch loss for epoch 973: 99120.09314\n",
      "Avg Minibatch loss for epoch 974: 99206.89393\n",
      "Avg Minibatch loss for epoch 975: 99184.51787\n",
      "  ***   Training RMSE: 89.78540 ***\n",
      "  *** Validation RMSE: 3327.34106 ***\n",
      "Avg Minibatch loss for epoch 976: 99196.54849\n",
      "Avg Minibatch loss for epoch 977: 99147.93343\n",
      "Avg Minibatch loss for epoch 978: 99182.54569\n",
      "Avg Minibatch loss for epoch 979: 99226.23508\n",
      "Avg Minibatch loss for epoch 980: 99174.96908\n",
      "  ***   Training RMSE: 89.69027 ***\n",
      "  *** Validation RMSE: 3327.34139 ***\n",
      "Avg Minibatch loss for epoch 981: 99077.32957\n",
      "Avg Minibatch loss for epoch 982: 99186.33444\n",
      "Avg Minibatch loss for epoch 983: 99152.58870\n",
      "Avg Minibatch loss for epoch 984: 99067.00993\n",
      "Avg Minibatch loss for epoch 985: 99215.85722\n",
      "  ***   Training RMSE: 89.41469 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 986: 99038.12497\n",
      "Avg Minibatch loss for epoch 987: 99199.53840\n",
      "Avg Minibatch loss for epoch 988: 99067.81909\n",
      "Avg Minibatch loss for epoch 989: 99099.24923\n",
      "Avg Minibatch loss for epoch 990: 99232.91131\n",
      "  ***   Training RMSE: 89.75978 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 991: 99075.66623\n",
      "Avg Minibatch loss for epoch 992: 99221.58754\n",
      "Avg Minibatch loss for epoch 993: 99175.96156\n",
      "Avg Minibatch loss for epoch 994: 99200.34248\n",
      "Avg Minibatch loss for epoch 995: 99185.26808\n",
      "  ***   Training RMSE: 89.86427 ***\n",
      "  *** Validation RMSE: 3327.34139 ***\n",
      "Avg Minibatch loss for epoch 996: 99084.42165\n",
      "Avg Minibatch loss for epoch 997: 99195.25873\n",
      "Avg Minibatch loss for epoch 998: 99105.23807\n",
      "Avg Minibatch loss for epoch 999: 99166.05836\n",
      "Avg Minibatch loss for epoch 1000: 99399.40802\n",
      "  ***   Training RMSE: 89.26228 ***\n",
      "  *** Validation RMSE: 3327.34139 ***\n",
      "Avg Minibatch loss for epoch 1001: 99159.35977\n",
      "Avg Minibatch loss for epoch 1002: 99125.38486\n",
      "Avg Minibatch loss for epoch 1003: 99016.77167\n",
      "Avg Minibatch loss for epoch 1004: 99273.80759\n",
      "Avg Minibatch loss for epoch 1005: 99273.22829\n",
      "  ***   Training RMSE: 89.18774 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 1006: 99100.58370\n",
      "Avg Minibatch loss for epoch 1007: 99127.76710\n",
      "Avg Minibatch loss for epoch 1008: 99144.01498\n",
      "Avg Minibatch loss for epoch 1009: 99124.03369\n",
      "Avg Minibatch loss for epoch 1010: 99145.22683\n",
      "  ***   Training RMSE: 89.43175 ***\n",
      "  *** Validation RMSE: 3327.34139 ***\n",
      "Avg Minibatch loss for epoch 1011: 99225.45176\n",
      "Avg Minibatch loss for epoch 1012: 99172.81217\n",
      "Avg Minibatch loss for epoch 1013: 99104.78570\n",
      "Avg Minibatch loss for epoch 1014: 99154.11767\n",
      "Avg Minibatch loss for epoch 1015: 99111.38342\n",
      "  ***   Training RMSE: 89.35733 ***\n",
      "  *** Validation RMSE: 3327.34139 ***\n",
      "Avg Minibatch loss for epoch 1016: 99205.59793\n",
      "Avg Minibatch loss for epoch 1017: 99084.93626\n",
      "Avg Minibatch loss for epoch 1018: 99045.21942\n",
      "Avg Minibatch loss for epoch 1019: 99136.61073\n",
      "Avg Minibatch loss for epoch 1020: 99104.53761\n",
      "  ***   Training RMSE: 89.24870 ***\n",
      "  *** Validation RMSE: 3327.34139 ***\n",
      "Avg Minibatch loss for epoch 1021: 99096.02862\n",
      "Avg Minibatch loss for epoch 1022: 99099.39779\n",
      "Avg Minibatch loss for epoch 1023: 99153.00802\n",
      "Avg Minibatch loss for epoch 1024: 99139.82620\n",
      "Avg Minibatch loss for epoch 1025: 99145.00932\n",
      "  ***   Training RMSE: 89.00290 ***\n",
      "  *** Validation RMSE: 3327.34172 ***\n",
      "Avg Minibatch loss for epoch 1026: 99036.00852\n",
      "Avg Minibatch loss for epoch 1027: 99077.62657\n",
      "Avg Minibatch loss for epoch 1028: 99081.31937\n",
      "Avg Minibatch loss for epoch 1029: 99140.17268\n",
      "Avg Minibatch loss for epoch 1030: 99105.03405\n",
      "  ***   Training RMSE: 89.41131 ***\n",
      "  *** Validation RMSE: 3327.34139 ***\n",
      "Avg Minibatch loss for epoch 1031: 99148.55567\n",
      "Avg Minibatch loss for epoch 1032: 99163.04555\n",
      "Avg Minibatch loss for epoch 1033: 99237.51910\n",
      "Avg Minibatch loss for epoch 1034: 99084.12069\n",
      "Avg Minibatch loss for epoch 1035: 99188.51602\n",
      "  ***   Training RMSE: 89.61040 ***\n",
      "  *** Validation RMSE: 3327.34139 ***\n",
      "Avg Minibatch loss for epoch 1036: 99352.80622\n",
      "Avg Minibatch loss for epoch 1037: 99153.99414\n",
      "Avg Minibatch loss for epoch 1038: 99072.50724\n",
      "Avg Minibatch loss for epoch 1039: 99054.15447\n",
      "Avg Minibatch loss for epoch 1040: 99174.46982\n",
      "  ***   Training RMSE: 89.10210 ***\n",
      "  *** Validation RMSE: 3327.34139 ***\n",
      "Avg Minibatch loss for epoch 1041: 99151.05838\n",
      "Avg Minibatch loss for epoch 1042: 99200.06454\n",
      "Avg Minibatch loss for epoch 1043: 99145.61570\n",
      "Avg Minibatch loss for epoch 1044: 99125.89833\n",
      "Avg Minibatch loss for epoch 1045: 99196.20064\n",
      "  ***   Training RMSE: 89.65931 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 1046: 99096.56867\n",
      "Avg Minibatch loss for epoch 1047: 99182.35419\n",
      "Avg Minibatch loss for epoch 1048: 99105.60861\n",
      "Avg Minibatch loss for epoch 1049: 99158.71961\n",
      "Avg Minibatch loss for epoch 1050: 99157.99667\n",
      "  ***   Training RMSE: 89.28063 ***\n",
      "  *** Validation RMSE: 3327.34139 ***\n",
      "Avg Minibatch loss for epoch 1051: 99072.71394\n",
      "Avg Minibatch loss for epoch 1052: 99103.42693\n",
      "Avg Minibatch loss for epoch 1053: 99199.81708\n",
      "Avg Minibatch loss for epoch 1054: 99122.11729\n",
      "Avg Minibatch loss for epoch 1055: 99161.29457\n",
      "  ***   Training RMSE: 89.09687 ***\n",
      "  *** Validation RMSE: 3327.34139 ***\n",
      "Avg Minibatch loss for epoch 1056: 99081.55788\n",
      "Avg Minibatch loss for epoch 1057: 99087.77052\n",
      "Avg Minibatch loss for epoch 1058: 99122.35165\n",
      "Avg Minibatch loss for epoch 1059: 99081.06351\n",
      "Avg Minibatch loss for epoch 1060: 99026.65360\n",
      "  ***   Training RMSE: 89.19762 ***\n",
      "  *** Validation RMSE: 3327.34156 ***\n",
      "Avg Minibatch loss for epoch 1061: 99103.41614\n",
      "Avg Minibatch loss for epoch 1062: 99230.42488\n",
      "Avg Minibatch loss for epoch 1063: 99123.16021\n",
      "Avg Minibatch loss for epoch 1064: 99161.20459\n",
      "Avg Minibatch loss for epoch 1065: 99157.93048\n",
      "  ***   Training RMSE: 89.56308 ***\n",
      "  *** Validation RMSE: 3327.34156 ***\n",
      "Avg Minibatch loss for epoch 1066: 99090.51071\n",
      "Avg Minibatch loss for epoch 1067: 99165.79536\n",
      "Avg Minibatch loss for epoch 1068: 99144.69394\n",
      "Avg Minibatch loss for epoch 1069: 99096.47296\n",
      "Avg Minibatch loss for epoch 1070: 99055.85966\n",
      "  ***   Training RMSE: 89.43157 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 1071: 99121.07727\n",
      "Avg Minibatch loss for epoch 1072: 99127.68457\n",
      "Avg Minibatch loss for epoch 1073: 98995.01657\n",
      "Avg Minibatch loss for epoch 1074: 99004.81921\n",
      "Avg Minibatch loss for epoch 1075: 99141.73651\n",
      "  ***   Training RMSE: 89.47738 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 1076: 99233.44092\n",
      "Avg Minibatch loss for epoch 1077: 99210.67689\n",
      "Avg Minibatch loss for epoch 1078: 99203.85973\n",
      "Avg Minibatch loss for epoch 1079: 99119.36952\n",
      "Avg Minibatch loss for epoch 1080: 99188.48726\n",
      "  ***   Training RMSE: 89.68544 ***\n",
      "  *** Validation RMSE: 3327.34139 ***\n",
      "Avg Minibatch loss for epoch 1081: 99210.67309\n",
      "Avg Minibatch loss for epoch 1082: 99130.22673\n",
      "Avg Minibatch loss for epoch 1083: 99085.13585\n",
      "Avg Minibatch loss for epoch 1084: 99186.68584\n",
      "Avg Minibatch loss for epoch 1085: 99008.72185\n",
      "  ***   Training RMSE: 89.62835 ***\n",
      "  *** Validation RMSE: 3327.34189 ***\n",
      "Avg Minibatch loss for epoch 1086: 99100.49404\n",
      "Avg Minibatch loss for epoch 1087: 99198.43808\n",
      "Avg Minibatch loss for epoch 1088: 99175.25712\n",
      "Avg Minibatch loss for epoch 1089: 99099.32775\n",
      "Avg Minibatch loss for epoch 1090: 99223.11671\n",
      "  ***   Training RMSE: 89.57020 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 1091: 99136.50002\n",
      "Avg Minibatch loss for epoch 1092: 99089.42119\n",
      "Avg Minibatch loss for epoch 1093: 99100.06120\n",
      "Avg Minibatch loss for epoch 1094: 99138.56950\n",
      "Avg Minibatch loss for epoch 1095: 99105.30555\n",
      "  ***   Training RMSE: 89.50737 ***\n",
      "  *** Validation RMSE: 3327.34139 ***\n",
      "Avg Minibatch loss for epoch 1096: 99075.30992\n",
      "Avg Minibatch loss for epoch 1097: 99248.46527\n",
      "Avg Minibatch loss for epoch 1098: 99161.54326\n",
      "Avg Minibatch loss for epoch 1099: 99124.37930\n",
      "Avg Minibatch loss for epoch 1100: 99136.78074\n",
      "  ***   Training RMSE: 89.51247 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 1101: 99006.02316\n",
      "Avg Minibatch loss for epoch 1102: 99169.62789\n",
      "Avg Minibatch loss for epoch 1103: 99066.32382\n",
      "Avg Minibatch loss for epoch 1104: 99165.02312\n",
      "Avg Minibatch loss for epoch 1105: 99182.93199\n",
      "  ***   Training RMSE: 89.58965 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 1106: 99254.80663\n",
      "Avg Minibatch loss for epoch 1107: 99232.42067\n",
      "Avg Minibatch loss for epoch 1108: 99037.38603\n",
      "Avg Minibatch loss for epoch 1109: 99131.24972\n",
      "Avg Minibatch loss for epoch 1110: 99065.11010\n",
      "  ***   Training RMSE: 89.69732 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 1111: 99217.61379\n",
      "Avg Minibatch loss for epoch 1112: 99117.29107\n",
      "Avg Minibatch loss for epoch 1113: 99129.30932\n",
      "Avg Minibatch loss for epoch 1114: 99210.32610\n",
      "Avg Minibatch loss for epoch 1115: 99221.76225\n",
      "  ***   Training RMSE: 89.17056 ***\n",
      "  *** Validation RMSE: 3327.34156 ***\n",
      "Avg Minibatch loss for epoch 1116: 99092.14265\n",
      "Avg Minibatch loss for epoch 1117: 99116.74132\n",
      "Avg Minibatch loss for epoch 1118: 99014.11603\n",
      "Avg Minibatch loss for epoch 1119: 99157.40154\n",
      "Avg Minibatch loss for epoch 1120: 99122.71178\n",
      "  ***   Training RMSE: 88.89729 ***\n",
      "  *** Validation RMSE: 3327.34172 ***\n",
      "Avg Minibatch loss for epoch 1121: 99149.24186\n",
      "Avg Minibatch loss for epoch 1122: 99087.72598\n",
      "Avg Minibatch loss for epoch 1123: 99172.54113\n",
      "Avg Minibatch loss for epoch 1124: 99143.09394\n",
      "Avg Minibatch loss for epoch 1125: 99091.24243\n",
      "  ***   Training RMSE: 88.97478 ***\n",
      "  *** Validation RMSE: 3327.34139 ***\n",
      "Avg Minibatch loss for epoch 1126: 99155.93893\n",
      "Avg Minibatch loss for epoch 1127: 99240.83404\n",
      "Avg Minibatch loss for epoch 1128: 99112.55147\n",
      "Avg Minibatch loss for epoch 1129: 99140.13009\n",
      "Avg Minibatch loss for epoch 1130: 99177.17183\n",
      "  ***   Training RMSE: 89.60218 ***\n",
      "  *** Validation RMSE: 3327.34139 ***\n",
      "Avg Minibatch loss for epoch 1131: 99111.40466\n",
      "Avg Minibatch loss for epoch 1132: 99212.77069\n",
      "Avg Minibatch loss for epoch 1133: 99213.35540\n",
      "Avg Minibatch loss for epoch 1134: 99131.25343\n",
      "Avg Minibatch loss for epoch 1135: 99151.03320\n",
      "  ***   Training RMSE: 89.29341 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 1136: 99134.91091\n",
      "Avg Minibatch loss for epoch 1137: 99117.48648\n",
      "Avg Minibatch loss for epoch 1138: 99193.58859\n",
      "Avg Minibatch loss for epoch 1139: 99268.59726\n",
      "Avg Minibatch loss for epoch 1140: 99110.00119\n",
      "  ***   Training RMSE: 89.71001 ***\n",
      "  *** Validation RMSE: 3327.34106 ***\n",
      "Avg Minibatch loss for epoch 1141: 99175.05303\n",
      "Avg Minibatch loss for epoch 1142: 99123.05407\n",
      "Avg Minibatch loss for epoch 1143: 99115.93300\n",
      "Avg Minibatch loss for epoch 1144: 99013.70129\n",
      "Avg Minibatch loss for epoch 1145: 99130.25571\n",
      "  ***   Training RMSE: 89.37248 ***\n",
      "  *** Validation RMSE: 3327.34106 ***\n",
      "Avg Minibatch loss for epoch 1146: 99195.12885\n",
      "Avg Minibatch loss for epoch 1147: 99015.87952\n",
      "Avg Minibatch loss for epoch 1148: 99157.21778\n",
      "Avg Minibatch loss for epoch 1149: 99155.03667\n",
      "Avg Minibatch loss for epoch 1150: 99211.17392\n",
      "  ***   Training RMSE: 89.50766 ***\n",
      "  *** Validation RMSE: 3327.34139 ***\n",
      "Avg Minibatch loss for epoch 1151: 99019.03379\n",
      "Avg Minibatch loss for epoch 1152: 99212.36919\n",
      "Avg Minibatch loss for epoch 1153: 99049.36358\n",
      "Avg Minibatch loss for epoch 1154: 99134.48829\n",
      "Avg Minibatch loss for epoch 1155: 99074.92629\n",
      "  ***   Training RMSE: 89.73773 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 1156: 99014.50102\n",
      "Avg Minibatch loss for epoch 1157: 99047.34898\n",
      "Avg Minibatch loss for epoch 1158: 99158.92153\n",
      "Avg Minibatch loss for epoch 1159: 99234.31068\n",
      "Avg Minibatch loss for epoch 1160: 99145.52547\n",
      "  ***   Training RMSE: 89.49874 ***\n",
      "  *** Validation RMSE: 3327.34139 ***\n",
      "Avg Minibatch loss for epoch 1161: 99109.64902\n",
      "Avg Minibatch loss for epoch 1162: 99064.26470\n",
      "Avg Minibatch loss for epoch 1163: 99131.46625\n",
      "Avg Minibatch loss for epoch 1164: 99211.29904\n",
      "Avg Minibatch loss for epoch 1165: 99086.05438\n",
      "  ***   Training RMSE: 89.37598 ***\n",
      "  *** Validation RMSE: 3327.34139 ***\n",
      "Avg Minibatch loss for epoch 1166: 99220.44509\n",
      "Avg Minibatch loss for epoch 1167: 99114.18979\n",
      "Avg Minibatch loss for epoch 1168: 99006.43539\n",
      "Avg Minibatch loss for epoch 1169: 99123.15451\n",
      "Avg Minibatch loss for epoch 1170: 99114.81287\n",
      "  ***   Training RMSE: 89.64955 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 1171: 99101.66337\n",
      "Avg Minibatch loss for epoch 1172: 99136.86355\n",
      "Avg Minibatch loss for epoch 1173: 99116.76452\n",
      "Avg Minibatch loss for epoch 1174: 99123.50297\n",
      "Avg Minibatch loss for epoch 1175: 99093.39859\n",
      "  ***   Training RMSE: 89.33422 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 1176: 99081.58798\n",
      "Avg Minibatch loss for epoch 1177: 99165.35620\n",
      "Avg Minibatch loss for epoch 1178: 99118.23845\n",
      "Avg Minibatch loss for epoch 1179: 99181.31010\n",
      "Avg Minibatch loss for epoch 1180: 99108.57311\n",
      "  ***   Training RMSE: 89.09859 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 1181: 99177.66307\n",
      "Avg Minibatch loss for epoch 1182: 99154.07973\n",
      "Avg Minibatch loss for epoch 1183: 99170.66121\n",
      "Avg Minibatch loss for epoch 1184: 99225.94256\n",
      "Avg Minibatch loss for epoch 1185: 99127.66858\n",
      "  ***   Training RMSE: 89.31528 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 1186: 99069.99890\n",
      "Avg Minibatch loss for epoch 1187: 99079.38512\n",
      "Avg Minibatch loss for epoch 1188: 99152.32491\n",
      "Avg Minibatch loss for epoch 1189: 99113.09571\n",
      "Avg Minibatch loss for epoch 1190: 99033.01683\n",
      "  ***   Training RMSE: 89.48392 ***\n",
      "  *** Validation RMSE: 3327.34139 ***\n",
      "Avg Minibatch loss for epoch 1191: 99165.74629\n",
      "Avg Minibatch loss for epoch 1192: 99043.49140\n",
      "Avg Minibatch loss for epoch 1193: 99145.58003\n",
      "Avg Minibatch loss for epoch 1194: 99087.83856\n",
      "Avg Minibatch loss for epoch 1195: 99053.71018\n",
      "  ***   Training RMSE: 89.15348 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 1196: 99124.97496\n",
      "Avg Minibatch loss for epoch 1197: 99061.85465\n",
      "Avg Minibatch loss for epoch 1198: 99213.85767\n",
      "Avg Minibatch loss for epoch 1199: 99157.93468\n",
      "Avg Minibatch loss for epoch 1200: 99056.75685\n",
      "  ***   Training RMSE: 89.20265 ***\n",
      "  *** Validation RMSE: 3327.34139 ***\n",
      "Avg Minibatch loss for epoch 1201: 99106.40198\n",
      "Avg Minibatch loss for epoch 1202: 99180.14919\n",
      "Avg Minibatch loss for epoch 1203: 99107.77135\n",
      "Avg Minibatch loss for epoch 1204: 99245.28681\n",
      "Avg Minibatch loss for epoch 1205: 99105.63338\n",
      "  ***   Training RMSE: 89.57351 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 1206: 99094.71201\n",
      "Avg Minibatch loss for epoch 1207: 99195.31966\n",
      "Avg Minibatch loss for epoch 1208: 99158.31169\n",
      "Avg Minibatch loss for epoch 1209: 99123.58245\n",
      "Avg Minibatch loss for epoch 1210: 99085.88573\n",
      "  ***   Training RMSE: 89.78670 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 1211: 99222.21894\n",
      "Avg Minibatch loss for epoch 1212: 99184.88706\n",
      "Avg Minibatch loss for epoch 1213: 99120.43960\n",
      "Avg Minibatch loss for epoch 1214: 99068.07896\n",
      "Avg Minibatch loss for epoch 1215: 99109.22626\n",
      "  ***   Training RMSE: 89.43637 ***\n",
      "  *** Validation RMSE: 3327.34089 ***\n",
      "Avg Minibatch loss for epoch 1216: 99171.04090\n",
      "Avg Minibatch loss for epoch 1217: 99154.65210\n",
      "Avg Minibatch loss for epoch 1218: 99138.95213\n",
      "Avg Minibatch loss for epoch 1219: 99151.46567\n",
      "Avg Minibatch loss for epoch 1220: 99088.10402\n",
      "  ***   Training RMSE: 89.68809 ***\n",
      "  *** Validation RMSE: 3327.34089 ***\n",
      "Avg Minibatch loss for epoch 1221: 99141.99184\n",
      "Avg Minibatch loss for epoch 1222: 99170.40248\n",
      "Avg Minibatch loss for epoch 1223: 99128.32827\n",
      "Avg Minibatch loss for epoch 1224: 99133.02376\n",
      "Avg Minibatch loss for epoch 1225: 99178.99189\n",
      "  ***   Training RMSE: 89.40984 ***\n",
      "  *** Validation RMSE: 3327.34089 ***\n",
      "Avg Minibatch loss for epoch 1226: 99171.68512\n",
      "Avg Minibatch loss for epoch 1227: 99220.61575\n",
      "Avg Minibatch loss for epoch 1228: 99122.04050\n",
      "Avg Minibatch loss for epoch 1229: 99073.96763\n",
      "Avg Minibatch loss for epoch 1230: 99173.97889\n",
      "  ***   Training RMSE: 89.05120 ***\n",
      "  *** Validation RMSE: 3327.34072 ***\n",
      "Avg Minibatch loss for epoch 1231: 99132.23736\n",
      "Avg Minibatch loss for epoch 1232: 99162.65458\n",
      "Avg Minibatch loss for epoch 1233: 99150.31980\n",
      "Avg Minibatch loss for epoch 1234: 99065.54091\n",
      "Avg Minibatch loss for epoch 1235: 99127.94417\n",
      "  ***   Training RMSE: 89.28036 ***\n",
      "  *** Validation RMSE: 3327.34089 ***\n",
      "Avg Minibatch loss for epoch 1236: 99151.61584\n",
      "Avg Minibatch loss for epoch 1237: 99055.99653\n",
      "Avg Minibatch loss for epoch 1238: 99104.42118\n",
      "Avg Minibatch loss for epoch 1239: 99138.50548\n",
      "Avg Minibatch loss for epoch 1240: 99124.38231\n",
      "  ***   Training RMSE: 89.15619 ***\n",
      "  *** Validation RMSE: 3327.34089 ***\n",
      "Avg Minibatch loss for epoch 1241: 99096.94473\n",
      "Avg Minibatch loss for epoch 1242: 99118.64020\n",
      "Avg Minibatch loss for epoch 1243: 99187.67077\n",
      "Avg Minibatch loss for epoch 1244: 99179.70996\n",
      "Avg Minibatch loss for epoch 1245: 99188.59819\n",
      "  ***   Training RMSE: 89.47135 ***\n",
      "  *** Validation RMSE: 3327.34106 ***\n",
      "Avg Minibatch loss for epoch 1246: 99062.98295\n",
      "Avg Minibatch loss for epoch 1247: 99064.13248\n",
      "Avg Minibatch loss for epoch 1248: 99108.97885\n",
      "Avg Minibatch loss for epoch 1249: 99099.57703\n",
      "Avg Minibatch loss for epoch 1250: 99264.01850\n",
      "  ***   Training RMSE: 89.51555 ***\n",
      "  *** Validation RMSE: 3327.34072 ***\n",
      "Avg Minibatch loss for epoch 1251: 98971.95442\n",
      "Avg Minibatch loss for epoch 1252: 99099.40664\n",
      "Avg Minibatch loss for epoch 1253: 99217.93838\n",
      "Avg Minibatch loss for epoch 1254: 99013.91501\n",
      "Avg Minibatch loss for epoch 1255: 99116.06718\n",
      "  ***   Training RMSE: 89.37778 ***\n",
      "  *** Validation RMSE: 3327.34072 ***\n",
      "Avg Minibatch loss for epoch 1256: 99178.38768\n",
      "Avg Minibatch loss for epoch 1257: 99134.58113\n",
      "Avg Minibatch loss for epoch 1258: 99191.94577\n",
      "Avg Minibatch loss for epoch 1259: 99155.22835\n",
      "Avg Minibatch loss for epoch 1260: 99083.80936\n",
      "  ***   Training RMSE: 88.84329 ***\n",
      "  *** Validation RMSE: 3327.34106 ***\n",
      "Avg Minibatch loss for epoch 1261: 99104.11757\n",
      "Avg Minibatch loss for epoch 1262: 99098.59598\n",
      "Avg Minibatch loss for epoch 1263: 99174.25814\n",
      "Avg Minibatch loss for epoch 1264: 99072.92116\n",
      "Avg Minibatch loss for epoch 1265: 99105.52782\n",
      "  ***   Training RMSE: 89.36198 ***\n",
      "  *** Validation RMSE: 3327.34089 ***\n",
      "Avg Minibatch loss for epoch 1266: 99136.64696\n",
      "Avg Minibatch loss for epoch 1267: 99205.79379\n",
      "Avg Minibatch loss for epoch 1268: 99077.06765\n",
      "Avg Minibatch loss for epoch 1269: 99128.30807\n",
      "Avg Minibatch loss for epoch 1270: 99165.11769\n",
      "  ***   Training RMSE: 89.21454 ***\n",
      "  *** Validation RMSE: 3327.34089 ***\n",
      "Avg Minibatch loss for epoch 1271: 99143.17424\n",
      "Avg Minibatch loss for epoch 1272: 99150.46418\n",
      "Avg Minibatch loss for epoch 1273: 99123.87679\n",
      "Avg Minibatch loss for epoch 1274: 99130.77065\n",
      "Avg Minibatch loss for epoch 1275: 99028.80370\n",
      "  ***   Training RMSE: 89.39201 ***\n",
      "  *** Validation RMSE: 3327.34072 ***\n",
      "Avg Minibatch loss for epoch 1276: 99138.57086\n",
      "Avg Minibatch loss for epoch 1277: 99131.20745\n",
      "Avg Minibatch loss for epoch 1278: 99090.21286\n",
      "Avg Minibatch loss for epoch 1279: 99142.90427\n",
      "Avg Minibatch loss for epoch 1280: 99129.90986\n",
      "  ***   Training RMSE: 89.12636 ***\n",
      "  *** Validation RMSE: 3327.34072 ***\n",
      "Avg Minibatch loss for epoch 1281: 99215.68442\n",
      "Avg Minibatch loss for epoch 1282: 99163.79210\n",
      "Avg Minibatch loss for epoch 1283: 99146.76558\n",
      "Avg Minibatch loss for epoch 1284: 99169.30775\n",
      "Avg Minibatch loss for epoch 1285: 99149.90870\n",
      "  ***   Training RMSE: 89.40874 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 1286: 99143.03916\n",
      "Avg Minibatch loss for epoch 1287: 99128.73429\n",
      "Avg Minibatch loss for epoch 1288: 99167.64288\n",
      "Avg Minibatch loss for epoch 1289: 99099.42688\n",
      "Avg Minibatch loss for epoch 1290: 99150.53501\n",
      "  ***   Training RMSE: 89.37693 ***\n",
      "  *** Validation RMSE: 3327.34089 ***\n",
      "Avg Minibatch loss for epoch 1291: 99113.53427\n",
      "Avg Minibatch loss for epoch 1292: 99066.53848\n",
      "Avg Minibatch loss for epoch 1293: 98976.58274\n",
      "Avg Minibatch loss for epoch 1294: 99016.61892\n",
      "Avg Minibatch loss for epoch 1295: 99154.59255\n",
      "  ***   Training RMSE: 89.20376 ***\n",
      "  *** Validation RMSE: 3327.34072 ***\n",
      "Avg Minibatch loss for epoch 1296: 98955.65472\n",
      "Avg Minibatch loss for epoch 1297: 99061.90163\n",
      "Avg Minibatch loss for epoch 1298: 99089.76559\n",
      "Avg Minibatch loss for epoch 1299: 99201.11399\n",
      "Avg Minibatch loss for epoch 1300: 99206.68698\n",
      "  ***   Training RMSE: 89.09186 ***\n",
      "  *** Validation RMSE: 3327.34106 ***\n",
      "Avg Minibatch loss for epoch 1301: 99052.38619\n",
      "Avg Minibatch loss for epoch 1302: 99096.90820\n",
      "Avg Minibatch loss for epoch 1303: 99071.81952\n",
      "Avg Minibatch loss for epoch 1304: 99249.89584\n",
      "Avg Minibatch loss for epoch 1305: 99186.22347\n",
      "  ***   Training RMSE: 89.22247 ***\n",
      "  *** Validation RMSE: 3327.34089 ***\n",
      "Avg Minibatch loss for epoch 1306: 99100.88504\n",
      "Avg Minibatch loss for epoch 1307: 99102.00856\n",
      "Avg Minibatch loss for epoch 1308: 99092.26943\n",
      "Avg Minibatch loss for epoch 1309: 99112.55787\n",
      "Avg Minibatch loss for epoch 1310: 99147.31202\n",
      "  ***   Training RMSE: 88.94253 ***\n",
      "  *** Validation RMSE: 3327.34072 ***\n",
      "Avg Minibatch loss for epoch 1311: 99062.98868\n",
      "Avg Minibatch loss for epoch 1312: 99068.05290\n",
      "Avg Minibatch loss for epoch 1313: 99118.94125\n",
      "Avg Minibatch loss for epoch 1314: 99148.68993\n",
      "Avg Minibatch loss for epoch 1315: 99150.66379\n",
      "  ***   Training RMSE: 89.35978 ***\n",
      "  *** Validation RMSE: 3327.34072 ***\n",
      "Avg Minibatch loss for epoch 1316: 99090.96263\n",
      "Avg Minibatch loss for epoch 1317: 99058.58639\n",
      "Avg Minibatch loss for epoch 1318: 99119.97715\n",
      "Avg Minibatch loss for epoch 1319: 99253.66084\n",
      "Avg Minibatch loss for epoch 1320: 99171.80771\n",
      "  ***   Training RMSE: 88.77723 ***\n",
      "  *** Validation RMSE: 3327.34089 ***\n",
      "Avg Minibatch loss for epoch 1321: 99165.09719\n",
      "Avg Minibatch loss for epoch 1322: 99142.93134\n",
      "Avg Minibatch loss for epoch 1323: 99065.33976\n",
      "Avg Minibatch loss for epoch 1324: 99173.64178\n",
      "Avg Minibatch loss for epoch 1325: 99125.13847\n",
      "  ***   Training RMSE: 89.21779 ***\n",
      "  *** Validation RMSE: 3327.34089 ***\n",
      "Avg Minibatch loss for epoch 1326: 98976.53653\n",
      "Avg Minibatch loss for epoch 1327: 99041.27104\n",
      "Avg Minibatch loss for epoch 1328: 99060.56573\n",
      "Avg Minibatch loss for epoch 1329: 99131.88062\n",
      "Avg Minibatch loss for epoch 1330: 99102.55522\n",
      "  ***   Training RMSE: 88.67582 ***\n",
      "  *** Validation RMSE: 3327.34106 ***\n",
      "Avg Minibatch loss for epoch 1331: 99136.19356\n",
      "Avg Minibatch loss for epoch 1332: 99122.25225\n",
      "Avg Minibatch loss for epoch 1333: 99203.81891\n",
      "Avg Minibatch loss for epoch 1334: 99156.67785\n",
      "Avg Minibatch loss for epoch 1335: 99054.16598\n",
      "  ***   Training RMSE: 88.98152 ***\n",
      "  *** Validation RMSE: 3327.34106 ***\n",
      "Avg Minibatch loss for epoch 1336: 99168.17200\n",
      "Avg Minibatch loss for epoch 1337: 99172.62093\n",
      "Avg Minibatch loss for epoch 1338: 99177.80570\n",
      "Avg Minibatch loss for epoch 1339: 99127.00922\n",
      "Avg Minibatch loss for epoch 1340: 99026.79880\n",
      "  ***   Training RMSE: 89.21392 ***\n",
      "  *** Validation RMSE: 3327.34089 ***\n",
      "Avg Minibatch loss for epoch 1341: 99006.48272\n",
      "Avg Minibatch loss for epoch 1342: 99183.74741\n",
      "Avg Minibatch loss for epoch 1343: 99120.46242\n",
      "Avg Minibatch loss for epoch 1344: 99014.09110\n",
      "Avg Minibatch loss for epoch 1345: 99250.79307\n",
      "  ***   Training RMSE: 88.52561 ***\n",
      "  *** Validation RMSE: 3327.34139 ***\n",
      "Avg Minibatch loss for epoch 1346: 99116.08258\n",
      "Avg Minibatch loss for epoch 1347: 99215.99213\n",
      "Avg Minibatch loss for epoch 1348: 99171.48569\n",
      "Avg Minibatch loss for epoch 1349: 99124.19075\n",
      "Avg Minibatch loss for epoch 1350: 99247.97851\n",
      "  ***   Training RMSE: 89.04988 ***\n",
      "  *** Validation RMSE: 3327.34089 ***\n",
      "Avg Minibatch loss for epoch 1351: 99085.86468\n",
      "Avg Minibatch loss for epoch 1352: 99092.66439\n",
      "Avg Minibatch loss for epoch 1353: 99116.61787\n",
      "Avg Minibatch loss for epoch 1354: 99105.84752\n",
      "Avg Minibatch loss for epoch 1355: 99026.48827\n",
      "  ***   Training RMSE: 89.28794 ***\n",
      "  *** Validation RMSE: 3327.34089 ***\n",
      "Avg Minibatch loss for epoch 1356: 99113.48648\n",
      "Avg Minibatch loss for epoch 1357: 99128.38481\n",
      "Avg Minibatch loss for epoch 1358: 99157.76475\n",
      "Avg Minibatch loss for epoch 1359: 99185.92681\n",
      "Avg Minibatch loss for epoch 1360: 99082.79996\n",
      "  ***   Training RMSE: 88.96818 ***\n",
      "  *** Validation RMSE: 3327.34072 ***\n",
      "Avg Minibatch loss for epoch 1361: 99102.10156\n",
      "Avg Minibatch loss for epoch 1362: 99058.64218\n",
      "Avg Minibatch loss for epoch 1363: 99145.64753\n",
      "Avg Minibatch loss for epoch 1364: 99185.38916\n",
      "Avg Minibatch loss for epoch 1365: 99123.70226\n",
      "  ***   Training RMSE: 88.72942 ***\n",
      "  *** Validation RMSE: 3327.34072 ***\n",
      "Avg Minibatch loss for epoch 1366: 99077.36543\n",
      "Avg Minibatch loss for epoch 1367: 99108.13901\n",
      "Avg Minibatch loss for epoch 1368: 99163.21793\n",
      "Avg Minibatch loss for epoch 1369: 99150.45084\n",
      "Avg Minibatch loss for epoch 1370: 99074.17303\n",
      "  ***   Training RMSE: 88.77772 ***\n",
      "  *** Validation RMSE: 3327.34072 ***\n",
      "Avg Minibatch loss for epoch 1371: 99153.11743\n",
      "Avg Minibatch loss for epoch 1372: 99170.30460\n",
      "Avg Minibatch loss for epoch 1373: 99046.50969\n",
      "Avg Minibatch loss for epoch 1374: 99086.06901\n",
      "Avg Minibatch loss for epoch 1375: 99063.63717\n",
      "  ***   Training RMSE: 89.20407 ***\n",
      "  *** Validation RMSE: 3327.34072 ***\n",
      "Avg Minibatch loss for epoch 1376: 98990.51939\n",
      "Avg Minibatch loss for epoch 1377: 98941.84652\n",
      "Avg Minibatch loss for epoch 1378: 98974.79892\n",
      "Avg Minibatch loss for epoch 1379: 99153.43897\n",
      "Avg Minibatch loss for epoch 1380: 99165.08100\n",
      "  ***   Training RMSE: 88.99590 ***\n",
      "  *** Validation RMSE: 3327.34106 ***\n",
      "Avg Minibatch loss for epoch 1381: 99207.26304\n",
      "Avg Minibatch loss for epoch 1382: 99123.14996\n",
      "Avg Minibatch loss for epoch 1383: 99221.42644\n",
      "Avg Minibatch loss for epoch 1384: 99147.83756\n",
      "Avg Minibatch loss for epoch 1385: 99210.97448\n",
      "  ***   Training RMSE: 88.83416 ***\n",
      "  *** Validation RMSE: 3327.34072 ***\n",
      "Avg Minibatch loss for epoch 1386: 99173.28154\n",
      "Avg Minibatch loss for epoch 1387: 99087.04642\n",
      "Avg Minibatch loss for epoch 1388: 99069.25378\n",
      "Avg Minibatch loss for epoch 1389: 99097.18632\n",
      "Avg Minibatch loss for epoch 1390: 99068.27992\n",
      "  ***   Training RMSE: 88.84979 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 1391: 99104.58124\n",
      "Avg Minibatch loss for epoch 1392: 99063.52787\n",
      "Avg Minibatch loss for epoch 1393: 99124.24382\n",
      "Avg Minibatch loss for epoch 1394: 99106.00707\n",
      "Avg Minibatch loss for epoch 1395: 99223.89200\n",
      "  ***   Training RMSE: 88.92498 ***\n",
      "  *** Validation RMSE: 3327.34106 ***\n",
      "Avg Minibatch loss for epoch 1396: 99152.83373\n",
      "Avg Minibatch loss for epoch 1397: 99080.66567\n",
      "Avg Minibatch loss for epoch 1398: 99136.60287\n",
      "Avg Minibatch loss for epoch 1399: 99080.57542\n",
      "Avg Minibatch loss for epoch 1400: 99207.70489\n",
      "  ***   Training RMSE: 89.08546 ***\n",
      "  *** Validation RMSE: 3327.34106 ***\n",
      "Avg Minibatch loss for epoch 1401: 99097.79121\n",
      "Avg Minibatch loss for epoch 1402: 99195.33614\n",
      "Avg Minibatch loss for epoch 1403: 99161.12514\n",
      "Avg Minibatch loss for epoch 1404: 99086.26573\n",
      "Avg Minibatch loss for epoch 1405: 99109.27975\n",
      "  ***   Training RMSE: 89.41657 ***\n",
      "  *** Validation RMSE: 3327.34089 ***\n",
      "Avg Minibatch loss for epoch 1406: 99088.09107\n",
      "Avg Minibatch loss for epoch 1407: 99131.56263\n",
      "Avg Minibatch loss for epoch 1408: 99126.96564\n",
      "Avg Minibatch loss for epoch 1409: 99146.25532\n",
      "Avg Minibatch loss for epoch 1410: 99187.43643\n",
      "  ***   Training RMSE: 88.94313 ***\n",
      "  *** Validation RMSE: 3327.34089 ***\n",
      "Avg Minibatch loss for epoch 1411: 99022.21326\n",
      "Avg Minibatch loss for epoch 1412: 99021.43944\n",
      "Avg Minibatch loss for epoch 1413: 99114.68106\n",
      "Avg Minibatch loss for epoch 1414: 99058.52141\n",
      "Avg Minibatch loss for epoch 1415: 99243.94049\n",
      "  ***   Training RMSE: 89.20776 ***\n",
      "  *** Validation RMSE: 3327.34089 ***\n",
      "Avg Minibatch loss for epoch 1416: 99094.66333\n",
      "Avg Minibatch loss for epoch 1417: 99148.42373\n",
      "Avg Minibatch loss for epoch 1418: 99056.55935\n",
      "Avg Minibatch loss for epoch 1419: 99174.19127\n",
      "Avg Minibatch loss for epoch 1420: 99043.60844\n",
      "  ***   Training RMSE: 89.05007 ***\n",
      "  *** Validation RMSE: 3327.34089 ***\n",
      "Avg Minibatch loss for epoch 1421: 99147.22168\n",
      "Avg Minibatch loss for epoch 1422: 99204.16769\n",
      "Avg Minibatch loss for epoch 1423: 99086.14960\n",
      "Avg Minibatch loss for epoch 1424: 99129.18456\n",
      "Avg Minibatch loss for epoch 1425: 98994.62128\n",
      "  ***   Training RMSE: 88.77821 ***\n",
      "  *** Validation RMSE: 3327.34106 ***\n",
      "Avg Minibatch loss for epoch 1426: 99182.28432\n",
      "Avg Minibatch loss for epoch 1427: 99114.00900\n",
      "Avg Minibatch loss for epoch 1428: 99164.31562\n",
      "Avg Minibatch loss for epoch 1429: 99105.64263\n",
      "Avg Minibatch loss for epoch 1430: 99155.24023\n",
      "  ***   Training RMSE: 88.85099 ***\n",
      "  *** Validation RMSE: 3327.34089 ***\n",
      "Avg Minibatch loss for epoch 1431: 99146.59028\n",
      "Avg Minibatch loss for epoch 1432: 99051.06493\n",
      "Avg Minibatch loss for epoch 1433: 99080.78948\n",
      "Avg Minibatch loss for epoch 1434: 99077.15351\n",
      "Avg Minibatch loss for epoch 1435: 99099.88731\n",
      "  ***   Training RMSE: 89.02902 ***\n",
      "  *** Validation RMSE: 3327.34072 ***\n",
      "Avg Minibatch loss for epoch 1436: 99157.95230\n",
      "Avg Minibatch loss for epoch 1437: 99063.49917\n",
      "Avg Minibatch loss for epoch 1438: 99115.07446\n",
      "Avg Minibatch loss for epoch 1439: 99069.56010\n",
      "Avg Minibatch loss for epoch 1440: 99110.59896\n",
      "  ***   Training RMSE: 89.18727 ***\n",
      "  *** Validation RMSE: 3327.34089 ***\n",
      "Avg Minibatch loss for epoch 1441: 99217.98305\n",
      "Avg Minibatch loss for epoch 1442: 99127.40091\n",
      "Avg Minibatch loss for epoch 1443: 99040.66415\n",
      "Avg Minibatch loss for epoch 1444: 99040.15233\n",
      "Avg Minibatch loss for epoch 1445: 99097.88900\n",
      "  ***   Training RMSE: 88.80558 ***\n",
      "  *** Validation RMSE: 3327.34089 ***\n",
      "Avg Minibatch loss for epoch 1446: 99063.21193\n",
      "Avg Minibatch loss for epoch 1447: 99173.30873\n",
      "Avg Minibatch loss for epoch 1448: 99119.20228\n",
      "Avg Minibatch loss for epoch 1449: 99031.97698\n",
      "Avg Minibatch loss for epoch 1450: 99139.27251\n",
      "  ***   Training RMSE: 89.10704 ***\n",
      "  *** Validation RMSE: 3327.34106 ***\n",
      "Avg Minibatch loss for epoch 1451: 99134.82409\n",
      "Avg Minibatch loss for epoch 1452: 99143.26635\n",
      "Avg Minibatch loss for epoch 1453: 99042.08917\n",
      "Avg Minibatch loss for epoch 1454: 99124.87333\n",
      "Avg Minibatch loss for epoch 1455: 99007.58875\n",
      "  ***   Training RMSE: 88.62320 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 1456: 99223.35018\n",
      "Avg Minibatch loss for epoch 1457: 99150.39799\n",
      "Avg Minibatch loss for epoch 1458: 99107.11371\n",
      "Avg Minibatch loss for epoch 1459: 99069.98713\n",
      "Avg Minibatch loss for epoch 1460: 99181.64210\n",
      "  ***   Training RMSE: 88.76899 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 1461: 99097.34896\n",
      "Avg Minibatch loss for epoch 1462: 99140.00553\n",
      "Avg Minibatch loss for epoch 1463: 99096.45724\n",
      "Avg Minibatch loss for epoch 1464: 99078.37356\n",
      "Avg Minibatch loss for epoch 1465: 99149.00283\n",
      "  ***   Training RMSE: 89.02401 ***\n",
      "  *** Validation RMSE: 3327.34139 ***\n",
      "Avg Minibatch loss for epoch 1466: 99058.70119\n",
      "Avg Minibatch loss for epoch 1467: 99106.74988\n",
      "Avg Minibatch loss for epoch 1468: 99149.84923\n",
      "Avg Minibatch loss for epoch 1469: 99181.35484\n",
      "Avg Minibatch loss for epoch 1470: 99119.81536\n",
      "  ***   Training RMSE: 88.80472 ***\n",
      "  *** Validation RMSE: 3327.34106 ***\n",
      "Avg Minibatch loss for epoch 1471: 99079.68068\n",
      "Avg Minibatch loss for epoch 1472: 99050.02996\n",
      "Avg Minibatch loss for epoch 1473: 99053.91280\n",
      "Avg Minibatch loss for epoch 1474: 99212.76337\n",
      "Avg Minibatch loss for epoch 1475: 99095.84848\n",
      "  ***   Training RMSE: 89.19428 ***\n",
      "  *** Validation RMSE: 3327.34106 ***\n",
      "Avg Minibatch loss for epoch 1476: 99168.72262\n",
      "Avg Minibatch loss for epoch 1477: 99056.43577\n",
      "Avg Minibatch loss for epoch 1478: 99159.19336\n",
      "Avg Minibatch loss for epoch 1479: 99176.85225\n",
      "Avg Minibatch loss for epoch 1480: 99133.65753\n",
      "  ***   Training RMSE: 89.14259 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 1481: 99070.96417\n",
      "Avg Minibatch loss for epoch 1482: 99004.29041\n",
      "Avg Minibatch loss for epoch 1483: 99072.95559\n",
      "Avg Minibatch loss for epoch 1484: 99119.97488\n",
      "Avg Minibatch loss for epoch 1485: 99074.93249\n",
      "  ***   Training RMSE: 89.11684 ***\n",
      "  *** Validation RMSE: 3327.34106 ***\n",
      "Avg Minibatch loss for epoch 1486: 99201.72381\n",
      "Avg Minibatch loss for epoch 1487: 99122.41369\n",
      "Avg Minibatch loss for epoch 1488: 99100.61063\n",
      "Avg Minibatch loss for epoch 1489: 99062.21745\n",
      "Avg Minibatch loss for epoch 1490: 99145.93043\n",
      "  ***   Training RMSE: 88.94106 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 1491: 99249.97550\n",
      "Avg Minibatch loss for epoch 1492: 99020.84959\n",
      "Avg Minibatch loss for epoch 1493: 98965.72087\n",
      "Avg Minibatch loss for epoch 1494: 99086.66226\n",
      "Avg Minibatch loss for epoch 1495: 99090.74283\n",
      "  ***   Training RMSE: 88.77912 ***\n",
      "  *** Validation RMSE: 3327.34139 ***\n",
      "Avg Minibatch loss for epoch 1496: 99064.33707\n",
      "Avg Minibatch loss for epoch 1497: 99127.80335\n",
      "Avg Minibatch loss for epoch 1498: 99016.90535\n",
      "Avg Minibatch loss for epoch 1499: 99119.18620\n",
      "Avg Minibatch loss for epoch 1500: 98971.44904\n",
      "  ***   Training RMSE: 88.99138 ***\n",
      "  *** Validation RMSE: 3327.34072 ***\n",
      "Avg Minibatch loss for epoch 1501: 99018.60453\n",
      "Avg Minibatch loss for epoch 1502: 99204.83553\n",
      "Avg Minibatch loss for epoch 1503: 99062.16072\n",
      "Avg Minibatch loss for epoch 1504: 99177.76617\n",
      "Avg Minibatch loss for epoch 1505: 99169.45352\n",
      "  ***   Training RMSE: 88.82953 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 1506: 99190.94066\n",
      "Avg Minibatch loss for epoch 1507: 99081.61537\n",
      "Avg Minibatch loss for epoch 1508: 99084.63207\n",
      "Avg Minibatch loss for epoch 1509: 99005.47904\n",
      "Avg Minibatch loss for epoch 1510: 99125.20868\n",
      "  ***   Training RMSE: 89.10551 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 1511: 99050.15371\n",
      "Avg Minibatch loss for epoch 1512: 99134.54442\n",
      "Avg Minibatch loss for epoch 1513: 99163.66387\n",
      "Avg Minibatch loss for epoch 1514: 99166.13647\n",
      "Avg Minibatch loss for epoch 1515: 99133.13775\n",
      "  ***   Training RMSE: 88.72763 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 1516: 99055.82093\n",
      "Avg Minibatch loss for epoch 1517: 99141.63918\n",
      "Avg Minibatch loss for epoch 1518: 99053.61323\n",
      "Avg Minibatch loss for epoch 1519: 99103.16229\n",
      "Avg Minibatch loss for epoch 1520: 99162.02425\n",
      "  ***   Training RMSE: 88.79111 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 1521: 99228.31294\n",
      "Avg Minibatch loss for epoch 1522: 99211.85570\n",
      "Avg Minibatch loss for epoch 1523: 99054.42465\n",
      "Avg Minibatch loss for epoch 1524: 99166.98430\n",
      "Avg Minibatch loss for epoch 1525: 99191.24559\n",
      "  ***   Training RMSE: 88.91714 ***\n",
      "  *** Validation RMSE: 3327.34106 ***\n",
      "Avg Minibatch loss for epoch 1526: 99145.96217\n",
      "Avg Minibatch loss for epoch 1527: 99049.39600\n",
      "Avg Minibatch loss for epoch 1528: 99062.97140\n",
      "Avg Minibatch loss for epoch 1529: 99091.12842\n",
      "Avg Minibatch loss for epoch 1530: 99199.37878\n",
      "  ***   Training RMSE: 88.83105 ***\n",
      "  *** Validation RMSE: 3327.34089 ***\n",
      "Avg Minibatch loss for epoch 1531: 99110.66568\n",
      "Avg Minibatch loss for epoch 1532: 99206.63008\n",
      "Avg Minibatch loss for epoch 1533: 99203.41380\n",
      "Avg Minibatch loss for epoch 1534: 99025.41290\n",
      "Avg Minibatch loss for epoch 1535: 99041.39435\n",
      "  ***   Training RMSE: 89.41710 ***\n",
      "  *** Validation RMSE: 3327.34089 ***\n",
      "Avg Minibatch loss for epoch 1536: 99153.22911\n",
      "Avg Minibatch loss for epoch 1537: 99112.40799\n",
      "Avg Minibatch loss for epoch 1538: 99117.79393\n",
      "Avg Minibatch loss for epoch 1539: 99214.50084\n",
      "Avg Minibatch loss for epoch 1540: 99024.29797\n",
      "  ***   Training RMSE: 88.81665 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 1541: 99165.84767\n",
      "Avg Minibatch loss for epoch 1542: 99182.69819\n",
      "Avg Minibatch loss for epoch 1543: 99146.08232\n",
      "Avg Minibatch loss for epoch 1544: 99065.82696\n",
      "Avg Minibatch loss for epoch 1545: 99078.14882\n",
      "  ***   Training RMSE: 88.73090 ***\n",
      "  *** Validation RMSE: 3327.34089 ***\n",
      "Avg Minibatch loss for epoch 1546: 99164.48577\n",
      "Avg Minibatch loss for epoch 1547: 99219.65482\n",
      "Avg Minibatch loss for epoch 1548: 99187.79236\n",
      "Avg Minibatch loss for epoch 1549: 99050.58518\n",
      "Avg Minibatch loss for epoch 1550: 99053.30465\n",
      "  ***   Training RMSE: 89.06091 ***\n",
      "  *** Validation RMSE: 3327.34106 ***\n",
      "Avg Minibatch loss for epoch 1551: 99151.92337\n",
      "Avg Minibatch loss for epoch 1552: 99141.85926\n",
      "Avg Minibatch loss for epoch 1553: 99092.68263\n",
      "Avg Minibatch loss for epoch 1554: 99112.01166\n",
      "Avg Minibatch loss for epoch 1555: 99153.43978\n",
      "  ***   Training RMSE: 89.33291 ***\n",
      "  *** Validation RMSE: 3327.34106 ***\n",
      "Avg Minibatch loss for epoch 1556: 99076.40997\n",
      "Avg Minibatch loss for epoch 1557: 99148.02358\n",
      "Avg Minibatch loss for epoch 1558: 99162.14919\n",
      "Avg Minibatch loss for epoch 1559: 99141.80670\n",
      "Avg Minibatch loss for epoch 1560: 99080.20851\n",
      "  ***   Training RMSE: 88.44995 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 1561: 99072.17438\n",
      "Avg Minibatch loss for epoch 1562: 99174.15738\n",
      "Avg Minibatch loss for epoch 1563: 99131.16025\n",
      "Avg Minibatch loss for epoch 1564: 99201.46996\n",
      "Avg Minibatch loss for epoch 1565: 99123.68160\n",
      "  ***   Training RMSE: 89.09854 ***\n",
      "  *** Validation RMSE: 3327.34106 ***\n",
      "Avg Minibatch loss for epoch 1566: 99104.51568\n",
      "Avg Minibatch loss for epoch 1567: 99130.27729\n",
      "Avg Minibatch loss for epoch 1568: 99136.65861\n",
      "Avg Minibatch loss for epoch 1569: 98999.68925\n",
      "Avg Minibatch loss for epoch 1570: 99232.69236\n",
      "  ***   Training RMSE: 88.80416 ***\n",
      "  *** Validation RMSE: 3327.34156 ***\n",
      "Avg Minibatch loss for epoch 1571: 99108.76069\n",
      "Avg Minibatch loss for epoch 1572: 99130.86570\n",
      "Avg Minibatch loss for epoch 1573: 99145.46782\n",
      "Avg Minibatch loss for epoch 1574: 99043.09135\n",
      "Avg Minibatch loss for epoch 1575: 99176.07386\n",
      "  ***   Training RMSE: 88.93654 ***\n",
      "  *** Validation RMSE: 3327.34089 ***\n",
      "Avg Minibatch loss for epoch 1576: 99094.93021\n",
      "Avg Minibatch loss for epoch 1577: 99139.21934\n",
      "Avg Minibatch loss for epoch 1578: 99088.13601\n",
      "Avg Minibatch loss for epoch 1579: 99197.87031\n",
      "Avg Minibatch loss for epoch 1580: 99070.56865\n",
      "  ***   Training RMSE: 88.92606 ***\n",
      "  *** Validation RMSE: 3327.34106 ***\n",
      "Avg Minibatch loss for epoch 1581: 99087.38965\n",
      "Avg Minibatch loss for epoch 1582: 99116.78279\n",
      "Avg Minibatch loss for epoch 1583: 99118.87693\n",
      "Avg Minibatch loss for epoch 1584: 99173.10477\n",
      "Avg Minibatch loss for epoch 1585: 99091.92875\n",
      "  ***   Training RMSE: 89.17857 ***\n",
      "  *** Validation RMSE: 3327.34089 ***\n",
      "Avg Minibatch loss for epoch 1586: 99097.12091\n",
      "Avg Minibatch loss for epoch 1587: 99230.79368\n",
      "Avg Minibatch loss for epoch 1588: 99105.33721\n",
      "Avg Minibatch loss for epoch 1589: 99030.51362\n",
      "Avg Minibatch loss for epoch 1590: 99073.26122\n",
      "  ***   Training RMSE: 89.27018 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 1591: 99052.36606\n",
      "Avg Minibatch loss for epoch 1592: 98968.32260\n",
      "Avg Minibatch loss for epoch 1593: 99189.23740\n",
      "Avg Minibatch loss for epoch 1594: 99195.73482\n",
      "Avg Minibatch loss for epoch 1595: 99092.73149\n",
      "  ***   Training RMSE: 88.94252 ***\n",
      "  *** Validation RMSE: 3327.34089 ***\n",
      "Avg Minibatch loss for epoch 1596: 99132.53130\n",
      "Avg Minibatch loss for epoch 1597: 99120.42552\n",
      "Avg Minibatch loss for epoch 1598: 99058.23490\n",
      "Avg Minibatch loss for epoch 1599: 99108.33028\n",
      "Avg Minibatch loss for epoch 1600: 99145.77908\n",
      "  ***   Training RMSE: 88.30387 ***\n",
      "  *** Validation RMSE: 3327.34106 ***\n",
      "Avg Minibatch loss for epoch 1601: 99103.38339\n",
      "Avg Minibatch loss for epoch 1602: 99088.17706\n",
      "Avg Minibatch loss for epoch 1603: 99119.09609\n",
      "Avg Minibatch loss for epoch 1604: 99101.39867\n",
      "Avg Minibatch loss for epoch 1605: 99162.99514\n",
      "  ***   Training RMSE: 88.60367 ***\n",
      "  *** Validation RMSE: 3327.34089 ***\n",
      "Avg Minibatch loss for epoch 1606: 99077.13033\n",
      "Avg Minibatch loss for epoch 1607: 99112.76745\n",
      "Avg Minibatch loss for epoch 1608: 99106.53911\n",
      "Avg Minibatch loss for epoch 1609: 99137.84416\n",
      "Avg Minibatch loss for epoch 1610: 99045.80710\n",
      "  ***   Training RMSE: 88.80200 ***\n",
      "  *** Validation RMSE: 3327.34106 ***\n",
      "Avg Minibatch loss for epoch 1611: 99153.60418\n",
      "Avg Minibatch loss for epoch 1612: 99035.37268\n",
      "Avg Minibatch loss for epoch 1613: 98971.15568\n",
      "Avg Minibatch loss for epoch 1614: 99197.68922\n",
      "Avg Minibatch loss for epoch 1615: 99109.03943\n",
      "  ***   Training RMSE: 88.59247 ***\n",
      "  *** Validation RMSE: 3327.34156 ***\n",
      "Avg Minibatch loss for epoch 1616: 99120.38139\n",
      "Avg Minibatch loss for epoch 1617: 99151.05652\n",
      "Avg Minibatch loss for epoch 1618: 99084.50281\n",
      "Avg Minibatch loss for epoch 1619: 99008.42659\n",
      "Avg Minibatch loss for epoch 1620: 99159.66657\n",
      "  ***   Training RMSE: 88.84421 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 1621: 99115.94604\n",
      "Avg Minibatch loss for epoch 1622: 99122.83158\n",
      "Avg Minibatch loss for epoch 1623: 99025.85550\n",
      "Avg Minibatch loss for epoch 1624: 99146.61017\n",
      "Avg Minibatch loss for epoch 1625: 99009.76618\n",
      "  ***   Training RMSE: 88.64373 ***\n",
      "  *** Validation RMSE: 3327.34106 ***\n",
      "Avg Minibatch loss for epoch 1626: 99120.80622\n",
      "Avg Minibatch loss for epoch 1627: 99075.36155\n",
      "Avg Minibatch loss for epoch 1628: 99018.28603\n",
      "Avg Minibatch loss for epoch 1629: 99063.53640\n",
      "Avg Minibatch loss for epoch 1630: 99022.07628\n",
      "  ***   Training RMSE: 88.89049 ***\n",
      "  *** Validation RMSE: 3327.34106 ***\n",
      "Avg Minibatch loss for epoch 1631: 99067.10097\n",
      "Avg Minibatch loss for epoch 1632: 99057.61358\n",
      "Avg Minibatch loss for epoch 1633: 99139.96179\n",
      "Avg Minibatch loss for epoch 1634: 99099.14800\n",
      "Avg Minibatch loss for epoch 1635: 99003.73044\n",
      "  ***   Training RMSE: 88.81007 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 1636: 99156.03551\n",
      "Avg Minibatch loss for epoch 1637: 99201.45974\n",
      "Avg Minibatch loss for epoch 1638: 99106.69473\n",
      "Avg Minibatch loss for epoch 1639: 99065.11782\n",
      "Avg Minibatch loss for epoch 1640: 99112.00258\n",
      "  ***   Training RMSE: 88.57299 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 1641: 99114.03944\n",
      "Avg Minibatch loss for epoch 1642: 99047.43372\n",
      "Avg Minibatch loss for epoch 1643: 98975.45264\n",
      "Avg Minibatch loss for epoch 1644: 99168.33795\n",
      "Avg Minibatch loss for epoch 1645: 99066.20933\n",
      "  ***   Training RMSE: 88.88649 ***\n",
      "  *** Validation RMSE: 3327.34089 ***\n",
      "Avg Minibatch loss for epoch 1646: 99072.47704\n",
      "Avg Minibatch loss for epoch 1647: 99203.44061\n",
      "Avg Minibatch loss for epoch 1648: 99150.73391\n",
      "Avg Minibatch loss for epoch 1649: 99158.18215\n",
      "Avg Minibatch loss for epoch 1650: 99071.29905\n",
      "  ***   Training RMSE: 88.71504 ***\n",
      "  *** Validation RMSE: 3327.34089 ***\n",
      "Avg Minibatch loss for epoch 1651: 99038.40671\n",
      "Avg Minibatch loss for epoch 1652: 99171.53131\n",
      "Avg Minibatch loss for epoch 1653: 98969.67063\n",
      "Avg Minibatch loss for epoch 1654: 99093.15464\n",
      "Avg Minibatch loss for epoch 1655: 99167.99345\n",
      "  ***   Training RMSE: 88.61015 ***\n",
      "  *** Validation RMSE: 3327.34156 ***\n",
      "Avg Minibatch loss for epoch 1656: 99089.03091\n",
      "Avg Minibatch loss for epoch 1657: 99141.89022\n",
      "Avg Minibatch loss for epoch 1658: 99061.22792\n",
      "Avg Minibatch loss for epoch 1659: 99072.87734\n",
      "Avg Minibatch loss for epoch 1660: 99099.20576\n",
      "  ***   Training RMSE: 88.85678 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 1661: 99078.56558\n",
      "Avg Minibatch loss for epoch 1662: 99052.40827\n",
      "Avg Minibatch loss for epoch 1663: 99083.42413\n",
      "Avg Minibatch loss for epoch 1664: 99069.85416\n",
      "Avg Minibatch loss for epoch 1665: 99097.24761\n",
      "  ***   Training RMSE: 88.58271 ***\n",
      "  *** Validation RMSE: 3327.34172 ***\n",
      "Avg Minibatch loss for epoch 1666: 99131.26096\n",
      "Avg Minibatch loss for epoch 1667: 99104.82569\n",
      "Avg Minibatch loss for epoch 1668: 99100.46500\n",
      "Avg Minibatch loss for epoch 1669: 98885.07357\n",
      "Avg Minibatch loss for epoch 1670: 99047.42970\n",
      "  ***   Training RMSE: 88.62617 ***\n",
      "  *** Validation RMSE: 3327.34139 ***\n",
      "Avg Minibatch loss for epoch 1671: 99024.42560\n",
      "Avg Minibatch loss for epoch 1672: 99080.21098\n",
      "Avg Minibatch loss for epoch 1673: 99181.59144\n",
      "Avg Minibatch loss for epoch 1674: 99083.36307\n",
      "Avg Minibatch loss for epoch 1675: 99057.33952\n",
      "  ***   Training RMSE: 88.86249 ***\n",
      "  *** Validation RMSE: 3327.34156 ***\n",
      "Avg Minibatch loss for epoch 1676: 99138.97090\n",
      "Avg Minibatch loss for epoch 1677: 99188.27589\n",
      "Avg Minibatch loss for epoch 1678: 99060.13572\n",
      "Avg Minibatch loss for epoch 1679: 98975.61535\n",
      "Avg Minibatch loss for epoch 1680: 99115.07461\n",
      "  ***   Training RMSE: 89.15013 ***\n",
      "  *** Validation RMSE: 3327.34089 ***\n",
      "Avg Minibatch loss for epoch 1681: 99073.16179\n",
      "Avg Minibatch loss for epoch 1682: 99147.90541\n",
      "Avg Minibatch loss for epoch 1683: 99123.56672\n",
      "Avg Minibatch loss for epoch 1684: 99082.51060\n",
      "Avg Minibatch loss for epoch 1685: 99021.05667\n",
      "  ***   Training RMSE: 88.91888 ***\n",
      "  *** Validation RMSE: 3327.34089 ***\n",
      "Avg Minibatch loss for epoch 1686: 99125.44809\n",
      "Avg Minibatch loss for epoch 1687: 99269.86132\n",
      "Avg Minibatch loss for epoch 1688: 99096.64766\n",
      "Avg Minibatch loss for epoch 1689: 99079.52232\n",
      "Avg Minibatch loss for epoch 1690: 98988.55167\n",
      "  ***   Training RMSE: 88.78039 ***\n",
      "  *** Validation RMSE: 3327.34089 ***\n",
      "Avg Minibatch loss for epoch 1691: 99056.05985\n",
      "Avg Minibatch loss for epoch 1692: 99094.43369\n",
      "Avg Minibatch loss for epoch 1693: 99112.06237\n",
      "Avg Minibatch loss for epoch 1694: 99060.29528\n",
      "Avg Minibatch loss for epoch 1695: 99123.47006\n",
      "  ***   Training RMSE: 88.47193 ***\n",
      "  *** Validation RMSE: 3327.34189 ***\n",
      "Avg Minibatch loss for epoch 1696: 99055.75603\n",
      "Avg Minibatch loss for epoch 1697: 99101.48874\n",
      "Avg Minibatch loss for epoch 1698: 99075.32006\n",
      "Avg Minibatch loss for epoch 1699: 99019.51894\n",
      "Avg Minibatch loss for epoch 1700: 99133.53503\n",
      "  ***   Training RMSE: 88.68000 ***\n",
      "  *** Validation RMSE: 3327.34139 ***\n",
      "Avg Minibatch loss for epoch 1701: 99032.90382\n",
      "Avg Minibatch loss for epoch 1702: 99042.10339\n",
      "Avg Minibatch loss for epoch 1703: 99241.74503\n",
      "Avg Minibatch loss for epoch 1704: 99188.30184\n",
      "Avg Minibatch loss for epoch 1705: 99005.91008\n",
      "  ***   Training RMSE: 89.00836 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 1706: 98999.64492\n",
      "Avg Minibatch loss for epoch 1707: 99132.51770\n",
      "Avg Minibatch loss for epoch 1708: 99155.19729\n",
      "Avg Minibatch loss for epoch 1709: 99189.27958\n",
      "Avg Minibatch loss for epoch 1710: 99022.19520\n",
      "  ***   Training RMSE: 88.96262 ***\n",
      "  *** Validation RMSE: 3327.34139 ***\n",
      "Avg Minibatch loss for epoch 1711: 99113.36231\n",
      "Avg Minibatch loss for epoch 1712: 99189.04337\n",
      "Avg Minibatch loss for epoch 1713: 99121.55501\n",
      "Avg Minibatch loss for epoch 1714: 99104.36311\n",
      "Avg Minibatch loss for epoch 1715: 99094.20682\n",
      "  ***   Training RMSE: 88.97666 ***\n",
      "  *** Validation RMSE: 3327.34056 ***\n",
      "Avg Minibatch loss for epoch 1716: 99011.18669\n",
      "Avg Minibatch loss for epoch 1717: 99145.54348\n",
      "Avg Minibatch loss for epoch 1718: 99082.82162\n",
      "Avg Minibatch loss for epoch 1719: 99141.41653\n",
      "Avg Minibatch loss for epoch 1720: 99179.86425\n",
      "  ***   Training RMSE: 88.63316 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 1721: 99068.23211\n",
      "Avg Minibatch loss for epoch 1722: 99078.54987\n",
      "Avg Minibatch loss for epoch 1723: 99097.48171\n",
      "Avg Minibatch loss for epoch 1724: 99088.54723\n",
      "Avg Minibatch loss for epoch 1725: 99032.00338\n",
      "  ***   Training RMSE: 88.80374 ***\n",
      "  *** Validation RMSE: 3327.34139 ***\n",
      "Avg Minibatch loss for epoch 1726: 99112.21949\n",
      "Avg Minibatch loss for epoch 1727: 99076.33082\n",
      "Avg Minibatch loss for epoch 1728: 98957.43917\n",
      "Avg Minibatch loss for epoch 1729: 99020.98599\n",
      "Avg Minibatch loss for epoch 1730: 99193.97835\n",
      "  ***   Training RMSE: 88.15579 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 1731: 98977.78094\n",
      "Avg Minibatch loss for epoch 1732: 99050.87533\n",
      "Avg Minibatch loss for epoch 1733: 99174.99822\n",
      "Avg Minibatch loss for epoch 1734: 99033.06224\n",
      "Avg Minibatch loss for epoch 1735: 99021.94643\n",
      "  ***   Training RMSE: 88.57336 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 1736: 99082.87208\n",
      "Avg Minibatch loss for epoch 1737: 99083.81193\n",
      "Avg Minibatch loss for epoch 1738: 99019.62592\n",
      "Avg Minibatch loss for epoch 1739: 99075.11956\n",
      "Avg Minibatch loss for epoch 1740: 99110.99507\n",
      "  ***   Training RMSE: 88.43785 ***\n",
      "  *** Validation RMSE: 3327.34156 ***\n",
      "Avg Minibatch loss for epoch 1741: 99114.01549\n",
      "Avg Minibatch loss for epoch 1742: 99094.78895\n",
      "Avg Minibatch loss for epoch 1743: 99125.43912\n",
      "Avg Minibatch loss for epoch 1744: 99156.54546\n",
      "Avg Minibatch loss for epoch 1745: 99089.81374\n",
      "  ***   Training RMSE: 88.95626 ***\n",
      "  *** Validation RMSE: 3327.34089 ***\n",
      "Avg Minibatch loss for epoch 1746: 99051.32829\n",
      "Avg Minibatch loss for epoch 1747: 99169.78447\n",
      "Avg Minibatch loss for epoch 1748: 99101.53929\n",
      "Avg Minibatch loss for epoch 1749: 99108.20802\n",
      "Avg Minibatch loss for epoch 1750: 99038.49674\n",
      "  ***   Training RMSE: 88.76844 ***\n",
      "  *** Validation RMSE: 3327.34072 ***\n",
      "Avg Minibatch loss for epoch 1751: 99090.02517\n",
      "Avg Minibatch loss for epoch 1752: 99134.24829\n",
      "Avg Minibatch loss for epoch 1753: 99168.73549\n",
      "Avg Minibatch loss for epoch 1754: 99088.32427\n",
      "Avg Minibatch loss for epoch 1755: 99174.23084\n",
      "  ***   Training RMSE: 88.84605 ***\n",
      "  *** Validation RMSE: 3327.34072 ***\n",
      "Avg Minibatch loss for epoch 1756: 99125.69424\n",
      "Avg Minibatch loss for epoch 1757: 99098.84228\n",
      "Avg Minibatch loss for epoch 1758: 99067.02735\n",
      "Avg Minibatch loss for epoch 1759: 99101.73398\n",
      "Avg Minibatch loss for epoch 1760: 99080.86926\n",
      "  ***   Training RMSE: 88.64820 ***\n",
      "  *** Validation RMSE: 3327.34072 ***\n",
      "Avg Minibatch loss for epoch 1761: 99107.67828\n",
      "Avg Minibatch loss for epoch 1762: 99137.91769\n",
      "Avg Minibatch loss for epoch 1763: 99117.75137\n",
      "Avg Minibatch loss for epoch 1764: 99106.58171\n",
      "Avg Minibatch loss for epoch 1765: 99100.06992\n",
      "  ***   Training RMSE: 88.46473 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 1766: 99051.96920\n",
      "Avg Minibatch loss for epoch 1767: 99080.40382\n",
      "Avg Minibatch loss for epoch 1768: 99065.15965\n",
      "Avg Minibatch loss for epoch 1769: 99081.42028\n",
      "Avg Minibatch loss for epoch 1770: 99080.20176\n",
      "  ***   Training RMSE: 88.88665 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 1771: 99117.39103\n",
      "Avg Minibatch loss for epoch 1772: 99159.79094\n",
      "Avg Minibatch loss for epoch 1773: 99097.03347\n",
      "Avg Minibatch loss for epoch 1774: 99006.05529\n",
      "Avg Minibatch loss for epoch 1775: 98997.92735\n",
      "  ***   Training RMSE: 88.57870 ***\n",
      "  *** Validation RMSE: 3327.34139 ***\n",
      "Avg Minibatch loss for epoch 1776: 99048.81383\n",
      "Avg Minibatch loss for epoch 1777: 99129.08498\n",
      "Avg Minibatch loss for epoch 1778: 99068.63805\n",
      "Avg Minibatch loss for epoch 1779: 99114.89589\n",
      "Avg Minibatch loss for epoch 1780: 99064.17464\n",
      "  ***   Training RMSE: 88.80816 ***\n",
      "  *** Validation RMSE: 3327.34072 ***\n",
      "Avg Minibatch loss for epoch 1781: 99111.81057\n",
      "Avg Minibatch loss for epoch 1782: 99076.32370\n",
      "Avg Minibatch loss for epoch 1783: 99056.16022\n",
      "Avg Minibatch loss for epoch 1784: 99050.48123\n",
      "Avg Minibatch loss for epoch 1785: 99087.92380\n",
      "  ***   Training RMSE: 88.70793 ***\n",
      "  *** Validation RMSE: 3327.34089 ***\n",
      "Avg Minibatch loss for epoch 1786: 99068.65316\n",
      "Avg Minibatch loss for epoch 1787: 99065.32964\n",
      "Avg Minibatch loss for epoch 1788: 99152.00401\n",
      "Avg Minibatch loss for epoch 1789: 99090.31757\n",
      "Avg Minibatch loss for epoch 1790: 99086.26905\n",
      "  ***   Training RMSE: 88.37298 ***\n",
      "  *** Validation RMSE: 3327.34106 ***\n",
      "Avg Minibatch loss for epoch 1791: 99039.60586\n",
      "Avg Minibatch loss for epoch 1792: 99150.83577\n",
      "Avg Minibatch loss for epoch 1793: 99045.51738\n",
      "Avg Minibatch loss for epoch 1794: 99002.54661\n",
      "Avg Minibatch loss for epoch 1795: 99114.02774\n",
      "  ***   Training RMSE: 88.90563 ***\n",
      "  *** Validation RMSE: 3327.34106 ***\n",
      "Avg Minibatch loss for epoch 1796: 99275.80319\n",
      "Avg Minibatch loss for epoch 1797: 99091.02054\n",
      "Avg Minibatch loss for epoch 1798: 98953.63253\n",
      "Avg Minibatch loss for epoch 1799: 99128.85708\n",
      "Avg Minibatch loss for epoch 1800: 99057.87252\n",
      "  ***   Training RMSE: 88.75925 ***\n",
      "  *** Validation RMSE: 3327.34089 ***\n",
      "Avg Minibatch loss for epoch 1801: 99080.88478\n",
      "Avg Minibatch loss for epoch 1802: 99056.33724\n",
      "Avg Minibatch loss for epoch 1803: 99145.54899\n",
      "Avg Minibatch loss for epoch 1804: 99108.09762\n",
      "Avg Minibatch loss for epoch 1805: 99121.65734\n",
      "  ***   Training RMSE: 88.42845 ***\n",
      "  *** Validation RMSE: 3327.34106 ***\n",
      "Avg Minibatch loss for epoch 1806: 99096.20875\n",
      "Avg Minibatch loss for epoch 1807: 99116.67161\n",
      "Avg Minibatch loss for epoch 1808: 99013.20706\n",
      "Avg Minibatch loss for epoch 1809: 99087.18100\n",
      "Avg Minibatch loss for epoch 1810: 99081.64280\n",
      "  ***   Training RMSE: 88.70631 ***\n",
      "  *** Validation RMSE: 3327.34072 ***\n",
      "Avg Minibatch loss for epoch 1811: 99135.44937\n",
      "Avg Minibatch loss for epoch 1812: 99005.02198\n",
      "Avg Minibatch loss for epoch 1813: 99098.08576\n",
      "Avg Minibatch loss for epoch 1814: 99205.68114\n",
      "Avg Minibatch loss for epoch 1815: 99094.10617\n",
      "  ***   Training RMSE: 88.72241 ***\n",
      "  *** Validation RMSE: 3327.34089 ***\n",
      "Avg Minibatch loss for epoch 1816: 99049.02075\n",
      "Avg Minibatch loss for epoch 1817: 99095.51154\n",
      "Avg Minibatch loss for epoch 1818: 99041.61537\n",
      "Avg Minibatch loss for epoch 1819: 99075.81081\n",
      "Avg Minibatch loss for epoch 1820: 99114.84899\n",
      "  ***   Training RMSE: 88.40818 ***\n",
      "  *** Validation RMSE: 3327.34089 ***\n",
      "Avg Minibatch loss for epoch 1821: 99092.01860\n",
      "Avg Minibatch loss for epoch 1822: 99089.35647\n",
      "Avg Minibatch loss for epoch 1823: 99039.61195\n",
      "Avg Minibatch loss for epoch 1824: 98960.60533\n",
      "Avg Minibatch loss for epoch 1825: 99149.78248\n",
      "  ***   Training RMSE: 88.13405 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 1826: 99127.72440\n",
      "Avg Minibatch loss for epoch 1827: 99137.06769\n",
      "Avg Minibatch loss for epoch 1828: 99161.67469\n",
      "Avg Minibatch loss for epoch 1829: 99129.49547\n",
      "Avg Minibatch loss for epoch 1830: 99023.80229\n",
      "  ***   Training RMSE: 88.55665 ***\n",
      "  *** Validation RMSE: 3327.34106 ***\n",
      "Avg Minibatch loss for epoch 1831: 99097.92641\n",
      "Avg Minibatch loss for epoch 1832: 99112.54095\n",
      "Avg Minibatch loss for epoch 1833: 99082.39895\n",
      "Avg Minibatch loss for epoch 1834: 99154.24487\n",
      "Avg Minibatch loss for epoch 1835: 99099.15361\n",
      "  ***   Training RMSE: 88.71601 ***\n",
      "  *** Validation RMSE: 3327.34106 ***\n",
      "Avg Minibatch loss for epoch 1836: 99078.03541\n",
      "Avg Minibatch loss for epoch 1837: 99028.33361\n",
      "Avg Minibatch loss for epoch 1838: 99075.46865\n",
      "Avg Minibatch loss for epoch 1839: 99045.01624\n",
      "Avg Minibatch loss for epoch 1840: 99009.93922\n",
      "  ***   Training RMSE: 88.66794 ***\n",
      "  *** Validation RMSE: 3327.34106 ***\n",
      "Avg Minibatch loss for epoch 1841: 99024.68425\n",
      "Avg Minibatch loss for epoch 1842: 99066.78146\n",
      "Avg Minibatch loss for epoch 1843: 99087.60433\n",
      "Avg Minibatch loss for epoch 1844: 99039.64801\n",
      "Avg Minibatch loss for epoch 1845: 98999.49879\n",
      "  ***   Training RMSE: 88.56492 ***\n",
      "  *** Validation RMSE: 3327.34089 ***\n",
      "Avg Minibatch loss for epoch 1846: 99106.40790\n",
      "Avg Minibatch loss for epoch 1847: 99085.37885\n",
      "Avg Minibatch loss for epoch 1848: 99083.56910\n",
      "Avg Minibatch loss for epoch 1849: 99117.88779\n",
      "Avg Minibatch loss for epoch 1850: 99162.51581\n",
      "  ***   Training RMSE: 88.62213 ***\n",
      "  *** Validation RMSE: 3327.34139 ***\n",
      "Avg Minibatch loss for epoch 1851: 99157.52996\n",
      "Avg Minibatch loss for epoch 1852: 99215.71752\n",
      "Avg Minibatch loss for epoch 1853: 99149.85368\n",
      "Avg Minibatch loss for epoch 1854: 99013.06658\n",
      "Avg Minibatch loss for epoch 1855: 98971.02722\n",
      "  ***   Training RMSE: 88.93050 ***\n",
      "  *** Validation RMSE: 3327.34072 ***\n",
      "Avg Minibatch loss for epoch 1856: 99148.80884\n",
      "Avg Minibatch loss for epoch 1857: 99049.36745\n",
      "Avg Minibatch loss for epoch 1858: 99142.42985\n",
      "Avg Minibatch loss for epoch 1859: 99047.65760\n",
      "Avg Minibatch loss for epoch 1860: 99116.46327\n",
      "  ***   Training RMSE: 88.80007 ***\n",
      "  *** Validation RMSE: 3327.34072 ***\n",
      "Avg Minibatch loss for epoch 1861: 98968.30158\n",
      "Avg Minibatch loss for epoch 1862: 99076.65004\n",
      "Avg Minibatch loss for epoch 1863: 99022.63595\n",
      "Avg Minibatch loss for epoch 1864: 99054.83916\n",
      "Avg Minibatch loss for epoch 1865: 99080.61491\n",
      "  ***   Training RMSE: 88.24829 ***\n",
      "  *** Validation RMSE: 3327.34106 ***\n",
      "Avg Minibatch loss for epoch 1866: 99163.01212\n",
      "Avg Minibatch loss for epoch 1867: 99148.97434\n",
      "Avg Minibatch loss for epoch 1868: 99076.43851\n",
      "Avg Minibatch loss for epoch 1869: 99041.09047\n",
      "Avg Minibatch loss for epoch 1870: 99048.10789\n",
      "  ***   Training RMSE: 88.22693 ***\n",
      "  *** Validation RMSE: 3327.34106 ***\n",
      "Avg Minibatch loss for epoch 1871: 99093.12742\n",
      "Avg Minibatch loss for epoch 1872: 99090.55283\n",
      "Avg Minibatch loss for epoch 1873: 99085.90576\n",
      "Avg Minibatch loss for epoch 1874: 99006.25631\n",
      "Avg Minibatch loss for epoch 1875: 99115.52822\n",
      "  ***   Training RMSE: 88.49154 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 1876: 99097.67448\n",
      "Avg Minibatch loss for epoch 1877: 99145.20397\n",
      "Avg Minibatch loss for epoch 1878: 99139.32137\n",
      "Avg Minibatch loss for epoch 1879: 99010.79787\n",
      "Avg Minibatch loss for epoch 1880: 99094.27768\n",
      "  ***   Training RMSE: 88.90564 ***\n",
      "  *** Validation RMSE: 3327.34106 ***\n",
      "Avg Minibatch loss for epoch 1881: 99084.31945\n",
      "Avg Minibatch loss for epoch 1882: 99123.74895\n",
      "Avg Minibatch loss for epoch 1883: 99097.21278\n",
      "Avg Minibatch loss for epoch 1884: 99018.68086\n",
      "Avg Minibatch loss for epoch 1885: 99207.73573\n",
      "  ***   Training RMSE: 88.58866 ***\n",
      "  *** Validation RMSE: 3327.34072 ***\n",
      "Avg Minibatch loss for epoch 1886: 99002.06193\n",
      "Avg Minibatch loss for epoch 1887: 99175.74535\n",
      "Avg Minibatch loss for epoch 1888: 99142.01123\n",
      "Avg Minibatch loss for epoch 1889: 99049.65258\n",
      "Avg Minibatch loss for epoch 1890: 99083.77210\n",
      "  ***   Training RMSE: 88.75216 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 1891: 99022.70902\n",
      "Avg Minibatch loss for epoch 1892: 99014.29340\n",
      "Avg Minibatch loss for epoch 1893: 98998.40521\n",
      "Avg Minibatch loss for epoch 1894: 99178.36038\n",
      "Avg Minibatch loss for epoch 1895: 99117.39580\n",
      "  ***   Training RMSE: 88.76887 ***\n",
      "  *** Validation RMSE: 3327.34089 ***\n",
      "Avg Minibatch loss for epoch 1896: 99140.29342\n",
      "Avg Minibatch loss for epoch 1897: 99055.09801\n",
      "Avg Minibatch loss for epoch 1898: 99025.95124\n",
      "Avg Minibatch loss for epoch 1899: 99073.38538\n",
      "Avg Minibatch loss for epoch 1900: 99172.87162\n",
      "  ***   Training RMSE: 88.59123 ***\n",
      "  *** Validation RMSE: 3327.34072 ***\n",
      "Avg Minibatch loss for epoch 1901: 99140.31209\n",
      "Avg Minibatch loss for epoch 1902: 98962.30307\n",
      "Avg Minibatch loss for epoch 1903: 99109.27701\n",
      "Avg Minibatch loss for epoch 1904: 99015.56043\n",
      "Avg Minibatch loss for epoch 1905: 99099.16755\n",
      "  ***   Training RMSE: 88.87285 ***\n",
      "  *** Validation RMSE: 3327.34106 ***\n",
      "Avg Minibatch loss for epoch 1906: 99129.57168\n",
      "Avg Minibatch loss for epoch 1907: 99010.27324\n",
      "Avg Minibatch loss for epoch 1908: 99145.72904\n",
      "Avg Minibatch loss for epoch 1909: 99063.08043\n",
      "Avg Minibatch loss for epoch 1910: 99010.39235\n",
      "  ***   Training RMSE: 88.47104 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 1911: 99077.45060\n",
      "Avg Minibatch loss for epoch 1912: 99030.92859\n",
      "Avg Minibatch loss for epoch 1913: 99095.76749\n",
      "Avg Minibatch loss for epoch 1914: 99128.11244\n",
      "Avg Minibatch loss for epoch 1915: 99011.29288\n",
      "  ***   Training RMSE: 88.56423 ***\n",
      "  *** Validation RMSE: 3327.34106 ***\n",
      "Avg Minibatch loss for epoch 1916: 99135.19192\n",
      "Avg Minibatch loss for epoch 1917: 99056.37535\n",
      "Avg Minibatch loss for epoch 1918: 99034.95320\n",
      "Avg Minibatch loss for epoch 1919: 99019.10231\n",
      "Avg Minibatch loss for epoch 1920: 99083.37580\n",
      "  ***   Training RMSE: 88.51556 ***\n",
      "  *** Validation RMSE: 3327.34106 ***\n",
      "Avg Minibatch loss for epoch 1921: 99157.77957\n",
      "Avg Minibatch loss for epoch 1922: 99140.71874\n",
      "Avg Minibatch loss for epoch 1923: 99088.09823\n",
      "Avg Minibatch loss for epoch 1924: 99113.53709\n",
      "Avg Minibatch loss for epoch 1925: 99120.05621\n",
      "  ***   Training RMSE: 88.68294 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 1926: 99126.38684\n",
      "Avg Minibatch loss for epoch 1927: 99117.23280\n",
      "Avg Minibatch loss for epoch 1928: 99144.29014\n",
      "Avg Minibatch loss for epoch 1929: 99095.85112\n",
      "Avg Minibatch loss for epoch 1930: 99001.32170\n",
      "  ***   Training RMSE: 88.51870 ***\n",
      "  *** Validation RMSE: 3327.34122 ***\n",
      "Avg Minibatch loss for epoch 1931: 99052.05100\n",
      "Avg Minibatch loss for epoch 1932: 99213.93063\n",
      "Avg Minibatch loss for epoch 1933: 99053.11849\n",
      "Avg Minibatch loss for epoch 1934: 99108.95157\n",
      "Avg Minibatch loss for epoch 1935: 99117.43060\n",
      "  ***   Training RMSE: 88.54156 ***\n",
      "  *** Validation RMSE: 3327.34106 ***\n",
      "Avg Minibatch loss for epoch 1936: 99114.10764\n",
      "Avg Minibatch loss for epoch 1937: 99163.67869\n",
      "Avg Minibatch loss for epoch 1938: 99089.79183\n",
      "Avg Minibatch loss for epoch 1939: 99056.67904\n",
      "Avg Minibatch loss for epoch 1940: 99093.43504\n",
      "  ***   Training RMSE: 88.63468 ***\n",
      "  *** Validation RMSE: 3327.34089 ***\n",
      "Avg Minibatch loss for epoch 1941: 99163.62843\n",
      "Avg Minibatch loss for epoch 1942: 99032.33478\n",
      "Avg Minibatch loss for epoch 1943: 99105.95006\n",
      "Avg Minibatch loss for epoch 1944: 99158.15421\n",
      "Avg Minibatch loss for epoch 1945: 99071.15940\n",
      "  ***   Training RMSE: 88.85025 ***\n",
      "  *** Validation RMSE: 3327.34106 ***\n",
      "Avg Minibatch loss for epoch 1946: 99008.66064\n",
      "Avg Minibatch loss for epoch 1947: 99132.63043\n",
      "Avg Minibatch loss for epoch 1948: 99094.75675\n",
      "Avg Minibatch loss for epoch 1949: 98944.91707\n",
      "Avg Minibatch loss for epoch 1950: 98949.23667\n",
      "  ***   Training RMSE: 88.44622 ***\n",
      "  *** Validation RMSE: 3327.34156 ***\n",
      "Avg Minibatch loss for epoch 1951: 99035.37222\n",
      "Avg Minibatch loss for epoch 1952: 98982.94754\n",
      "Avg Minibatch loss for epoch 1953: 99019.72933\n",
      "Avg Minibatch loss for epoch 1954: 99037.35955\n",
      "Avg Minibatch loss for epoch 1955: 99143.07105\n",
      "  ***   Training RMSE: 88.35886 ***\n",
      "  *** Validation RMSE: 3327.34106 ***\n",
      "Avg Minibatch loss for epoch 1956: 99147.18889\n",
      "Avg Minibatch loss for epoch 1957: 99140.01076\n",
      "Avg Minibatch loss for epoch 1958: 99122.08491\n",
      "Avg Minibatch loss for epoch 1959: 99026.39942\n",
      "Avg Minibatch loss for epoch 1960: 99017.92445\n",
      "  ***   Training RMSE: 88.49215 ***\n",
      "  *** Validation RMSE: 3327.34089 ***\n",
      "Avg Minibatch loss for epoch 1961: 99007.41349\n",
      "Avg Minibatch loss for epoch 1962: 99072.17230\n",
      "Avg Minibatch loss for epoch 1963: 99145.27861\n",
      "Avg Minibatch loss for epoch 1964: 99125.95248\n",
      "Avg Minibatch loss for epoch 1965: 99099.50256\n",
      "  ***   Training RMSE: 88.73050 ***\n",
      "  *** Validation RMSE: 3327.34089 ***\n",
      "Avg Minibatch loss for epoch 1966: 99025.38493\n",
      "Avg Minibatch loss for epoch 1967: 99170.65288\n",
      "Avg Minibatch loss for epoch 1968: 99039.85564\n",
      "Avg Minibatch loss for epoch 1969: 99181.34954\n",
      "Avg Minibatch loss for epoch 1970: 99091.61306\n",
      "  ***   Training RMSE: 88.89576 ***\n",
      "  *** Validation RMSE: 3327.34089 ***\n",
      "Avg Minibatch loss for epoch 1971: 99061.07307\n",
      "Avg Minibatch loss for epoch 1972: 99038.49563\n",
      "Avg Minibatch loss for epoch 1973: 99065.75054\n",
      "Avg Minibatch loss for epoch 1974: 99030.73903\n",
      "Avg Minibatch loss for epoch 1975: 99001.02786\n",
      "  ***   Training RMSE: 88.46560 ***\n",
      "  *** Validation RMSE: 3327.34072 ***\n",
      "Avg Minibatch loss for epoch 1976: 99122.17853\n",
      "Avg Minibatch loss for epoch 1977: 99055.14600\n",
      "Avg Minibatch loss for epoch 1978: 99167.32219\n",
      "Avg Minibatch loss for epoch 1979: 99050.82336\n",
      "Avg Minibatch loss for epoch 1980: 99096.14930\n",
      "  ***   Training RMSE: 88.42256 ***\n",
      "  *** Validation RMSE: 3327.34089 ***\n",
      "Avg Minibatch loss for epoch 1981: 99052.42387\n",
      "Avg Minibatch loss for epoch 1982: 99023.80679\n",
      "Avg Minibatch loss for epoch 1983: 99072.63094\n",
      "Avg Minibatch loss for epoch 1984: 99041.94368\n",
      "Avg Minibatch loss for epoch 1985: 99081.81697\n",
      "  ***   Training RMSE: 88.44873 ***\n",
      "  *** Validation RMSE: 3327.34089 ***\n",
      "Avg Minibatch loss for epoch 1986: 99119.97180\n",
      "Avg Minibatch loss for epoch 1987: 98977.76805\n",
      "Avg Minibatch loss for epoch 1988: 99208.11371\n",
      "Avg Minibatch loss for epoch 1989: 99062.02989\n",
      "Avg Minibatch loss for epoch 1990: 99035.83676\n",
      "  ***   Training RMSE: 89.05657 ***\n",
      "  *** Validation RMSE: 3327.34089 ***\n",
      "Avg Minibatch loss for epoch 1991: 99075.19072\n",
      "Avg Minibatch loss for epoch 1992: 98992.38355\n",
      "Avg Minibatch loss for epoch 1993: 99068.40602\n",
      "Avg Minibatch loss for epoch 1994: 99053.04270\n",
      "Avg Minibatch loss for epoch 1995: 99103.77233\n",
      "  ***   Training RMSE: 88.78708 ***\n",
      "  *** Validation RMSE: 3327.34056 ***\n",
      "Avg Minibatch loss for epoch 1996: 99163.16601\n",
      "Avg Minibatch loss for epoch 1997: 99002.94401\n",
      "Avg Minibatch loss for epoch 1998: 99159.93109\n",
      "Avg Minibatch loss for epoch 1999: 99013.57381\n",
      "\n",
      "Final Training set RMSE: 88.53911\n",
      "Final Validation set RMSE: 3327.34106\n",
      "Test set RMSE: 1498.36856\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "h0 = 75\n",
    "h1 = len(users)\n",
    "lambda_ = 0.0005\n",
    "lr = .0005\n",
    "keep_prob = .75\n",
    "\n",
    "num_epochs = 2000\n",
    "\n",
    "train_dataset = train.transpose().toarray().astype(np.float32)\n",
    "test_dataset = test.transpose().toarray().astype(np.float32)\n",
    "valid_dataset = val.transpose().toarray().astype(np.float32)\n",
    "\n",
    "means = np.mean(train_dataset, axis=0)\n",
    "stds = np.std(train_dataset, axis=0)\n",
    "stds[stds == 0] = 1\n",
    "train_dataset = (train_dataset - means) / stds\n",
    "valid_dataset = (valid_dataset - means) / stds\n",
    "test_dataset = (test_dataset - means) /stds\n",
    "\n",
    "train_labels = train_dataset\n",
    "valid_labels = valid_dataset\n",
    "test_labels = test_dataset\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,shape=(batch_size, len(users)))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, len(users)))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    tf_tr_dataset = tf.constant(train_dataset)\n",
    "\n",
    "    # Variables.\n",
    "    w0 = tf.Variable(tf.truncated_normal([len(users), h0]))\n",
    "    b0 = tf.Variable(tf.zeros([h0]))\n",
    "\n",
    "    w1 = tf.Variable(tf.truncated_normal([h0, h1]))\n",
    "    b1 = tf.Variable(tf.zeros([h1]))\n",
    "    \n",
    "    # Training computation.\n",
    "    s0 = tf.nn.dropout(tf.nn.sigmoid(tf.matmul(tf_train_dataset, w0) + b0), keep_prob)\n",
    "    logits = tf.matmul(s0, w1) + b1\n",
    "\n",
    "    reg = tf.nn.l2_loss(w0) + tf.nn.l2_loss(w1)\n",
    "    # loss = tf.reduce_sum((logits - tf_train_dataset) ** 2)+ (lambda_ * reg)\n",
    "    loss = tf.nn.l2_loss(logits - tf_train_dataset) + (lambda_ * reg)\n",
    "\n",
    "    # Optimizer.\n",
    "    # optimizer = tf.train.GradientDescentOptimizer(lr).minimize(loss)\n",
    "    optimizer = tf.train.AdamOptimizer(lr).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = logits\n",
    "\n",
    "    v0 = tf.nn.sigmoid(tf.matmul(tf_valid_dataset, w0) + b0)\n",
    "    valid_prediction = tf.matmul(v0, w1) + b1\n",
    " \n",
    "\n",
    "    t0 = tf.nn.sigmoid(tf.matmul(tf_test_dataset, w0) + b0)\n",
    "    test_prediction = tf.matmul(t0, w1) + b1\n",
    "    \n",
    "    r0 = tf.nn.sigmoid(tf.matmul(tf_tr_dataset, w0) + b0)\n",
    "    tr_prediction = tf.matmul(r0, w1 + b1)\n",
    "\n",
    "batches = np.ceil(float(len(items)) / batch_size)\n",
    "num_steps = int(np.ceil(float(len(items)) / batch_size))\n",
    "# curves = []\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for epoch in range(num_epochs):\n",
    "        l_mean = 0\n",
    "        for step in range(num_steps):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            # Note: we could use better randomization across epochs.\n",
    "            # offset = (step * batch_size) % (int(batch_size * 2.5) - batch_size)\n",
    "            offset = (step * batch_size) % (len(items) - batch_size)\n",
    "            # Generate a minibatch.\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            # and the value is the numpy array to feed to it.\n",
    "            feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n",
    "\n",
    "            _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "            l_mean += l\n",
    "\n",
    "        print(\"Avg Minibatch loss for epoch %d: %.5f\" % (epoch, l_mean / num_steps))\n",
    "        # print(\" Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "        if (epoch % 5 == 0):\n",
    "            tra = accuracy(tr_prediction.eval(), train_labels)\n",
    "            va = accuracy(valid_prediction.eval(), valid_labels)\n",
    "            print(\"  ***   Training RMSE: %.5f ***\" % tra)\n",
    "            print(\"  *** Validation RMSE: %.5f ***\" % va)\n",
    "            # curves.append({'epoch': epoch, 'train': 1 - tra, 'val': 1- va})\n",
    "    print(\"\\nFinal Training set RMSE: %.5f\" % accuracy(tr_prediction.eval(), train_labels))\n",
    "    print(\"Final Validation set RMSE: %.5f\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test set RMSE: %.5f\" % accuracy(test_prediction.eval(), test_labels))\n",
    "    tr_pred = tr_prediction.eval()\n",
    "    va_pred = valid_prediction.eval()\n",
    "    te_pred = test_prediction.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x11836a0d0>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAEACAYAAABbMHZzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngr+IyBT0/wG2SRPyN4gDsAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11144ed90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.scatter(train_dataset[:500].reshape(-1,1), tr_pred[:500].reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [tensorflow]",
   "language": "python",
   "name": "Python [tensorflow]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
